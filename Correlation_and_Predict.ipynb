{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FTR</th>\n",
       "      <th>HTGS</th>\n",
       "      <th>ATGS</th>\n",
       "      <th>HTGC</th>\n",
       "      <th>ATGC</th>\n",
       "      <th>HTP</th>\n",
       "      <th>ATP</th>\n",
       "      <th>HM1</th>\n",
       "      <th>HM2</th>\n",
       "      <th>HM3</th>\n",
       "      <th>HM4</th>\n",
       "      <th>HM5</th>\n",
       "      <th>AM1</th>\n",
       "      <th>AM2</th>\n",
       "      <th>AM3</th>\n",
       "      <th>AM4</th>\n",
       "      <th>AM5</th>\n",
       "      <th>HTWinStreak3</th>\n",
       "      <th>HTWinStreak5</th>\n",
       "      <th>HTLossStreak3</th>\n",
       "      <th>HTLossStreak5</th>\n",
       "      <th>ATWinStreak3</th>\n",
       "      <th>ATWinStreak5</th>\n",
       "      <th>ATLossStreak3</th>\n",
       "      <th>ATLossStreak5</th>\n",
       "      <th>HTGD</th>\n",
       "      <th>ATGD</th>\n",
       "      <th>DiffPts</th>\n",
       "      <th>DiffFormPts</th>\n",
       "      <th>DiffLP</th>\n",
       "      <th>final1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-9</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-13</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2655</th>\n",
       "      <td>0</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.354167</td>\n",
       "      <td>0.590361</td>\n",
       "      <td>0.822785</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>-0.815789</td>\n",
       "      <td>-0.052632</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>-3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2656</th>\n",
       "      <td>1</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.635417</td>\n",
       "      <td>0.903614</td>\n",
       "      <td>0.443038</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.078947</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>-0.157895</td>\n",
       "      <td>14</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2657</th>\n",
       "      <td>0</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.354167</td>\n",
       "      <td>0.337349</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.421053</td>\n",
       "      <td>-0.263158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.131579</td>\n",
       "      <td>-17</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2658</th>\n",
       "      <td>1</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2659</th>\n",
       "      <td>0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.489583</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>0.683544</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.657895</td>\n",
       "      <td>-0.184211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.157895</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2660 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      FTR  HTGS      ATGS      HTGC      ATGC       HTP       ATP  HM1  HM2  \\\n",
       "0       1  0.00  0.000000  0.000000  0.000000  0.000000  0.000000    2    2   \n",
       "1       1  0.00  0.000000  0.000000  0.000000  0.000000  0.000000    2    2   \n",
       "2       1  0.00  0.000000  0.000000  0.000000  0.000000  0.000000    2    2   \n",
       "3       0  0.00  0.000000  0.000000  0.000000  0.000000  0.000000    2    2   \n",
       "4       0  0.00  0.000000  0.000000  0.000000  0.000000  0.000000    2    2   \n",
       "...   ...   ...       ...       ...       ...       ...       ...  ...  ...   \n",
       "2655    0  0.59  0.354167  0.590361  0.822785  0.026316  0.078947    0    3   \n",
       "2656    1  0.34  0.635417  0.903614  0.443038  0.026316  0.000000    0    1   \n",
       "2657    0  0.82  0.354167  0.337349  0.556962  0.078947  0.078947    3    3   \n",
       "2658    1  0.46  0.750000  0.783133  0.556962  0.000000  0.000000    1    3   \n",
       "2659    0  0.30  0.489583  0.662651  0.683544  0.000000  0.000000    1    0   \n",
       "\n",
       "      HM3  HM4  HM5  AM1  AM2  AM3  AM4  AM5  HTWinStreak3  HTWinStreak5  \\\n",
       "0       2    2    2    2    2    2    2    2             0             0   \n",
       "1       2    2    2    2    2    2    2    2             0             0   \n",
       "2       2    2    2    2    2    2    2    2             0             0   \n",
       "3       2    2    2    2    2    2    2    2             0             0   \n",
       "4       2    2    2    2    2    2    2    2             0             0   \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...           ...           ...   \n",
       "2655    3    0    0    3    0    1    0    0             0             0   \n",
       "2656    1    0    1    1    3    3    0    0             0             0   \n",
       "2657    3    0    1    3    0    1    1    0             0             0   \n",
       "2658    1    3    0    1    3    0    1    3             0             0   \n",
       "2659    0    0    1    1    3    3    1    3             0             0   \n",
       "\n",
       "      HTLossStreak3  HTLossStreak5  ATWinStreak3  ATWinStreak5  ATLossStreak3  \\\n",
       "0                 0              0             0             0              0   \n",
       "1                 0              0             0             0              0   \n",
       "2                 0              0             0             0              0   \n",
       "3                 0              0             0             0              0   \n",
       "4                 0              0             0             0              0   \n",
       "...             ...            ...           ...           ...            ...   \n",
       "2655              0              0             0             0              0   \n",
       "2656              0              0             0             0              0   \n",
       "2657              0              0             0             0              0   \n",
       "2658              0              0             0             0              0   \n",
       "2659              0              0             0             0              0   \n",
       "\n",
       "      ATLossStreak5      HTGD      ATGD   DiffPts  DiffFormPts  DiffLP  final1  \n",
       "0                 0  0.000000  0.000000  0.000000     0.000000       3     0.0  \n",
       "1                 0  0.000000  0.000000  0.000000     0.000000      -9     0.0  \n",
       "2                 0  0.000000  0.000000  0.000000     0.000000     -13     0.0  \n",
       "3                 0  0.000000  0.000000  0.000000     0.000000       4     1.0  \n",
       "4                 0  0.000000  0.000000  0.000000     0.000000       1     1.0  \n",
       "...             ...       ...       ...       ...          ...     ...     ...  \n",
       "2655              0  0.263158 -0.815789 -0.052632     0.078947      -3     1.0  \n",
       "2656              0 -1.078947  0.684211  0.026316    -0.157895      14     0.0  \n",
       "2657              0  1.421053 -0.263158  0.000000     0.131579     -17     1.0  \n",
       "2658              0 -0.500000  0.736842  0.000000     0.000000       1     0.0  \n",
       "2659              0 -0.657895 -0.184211  0.000000    -0.157895       3     1.0  \n",
       "\n",
       "[2660 rows x 31 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # ignore warnings message\n",
    "\n",
    "# making all pandas columns visable with display command\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "\n",
    "# df = pd.read_csv('datasets/EPL_sort.csv', index_col=0)\n",
    "df = pd.read_csv('datasets/epl_data_train_onehot.csv')\n",
    "df = df.drop(columns = ['final2'])\n",
    "# df = df.drop(columns = ['id', 'season', 'stage', 'date', 'match_api_id'])\n",
    "# df = df.drop(columns = ['id', 'season', 'stage', 'date', 'match_api_id', 'home_team_api_id', 'away_team_api_id'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing before get the correlation value\n",
    "# df.Result.replace(to_replace='H', value = 0, inplace = True)\n",
    "# df.Result.replace(to_replace='D', value = 1, inplace = True)\n",
    "# df.Result.replace(to_replace='A', value = 2, inplace = True)\n",
    "# df.isnull().sum()\n",
    "\n",
    "# df_dummies = pd.get_dummies(df)\n",
    "# df_dummies.head()\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Positive Correlations: \n",
      " FTR       1.000000\n",
      "DiffLP    0.312098\n",
      "ATGD      0.193031\n",
      "ATP       0.097125\n",
      "AM1       0.084886\n",
      "Name: FTR, dtype: float64\n",
      "\n",
      "Most Negative Correlations: \n",
      " HTGS          -0.156900\n",
      "DiffFormPts   -0.191770\n",
      "HTGD          -0.255272\n",
      "DiffPts       -0.268925\n",
      "final1        -1.000000\n",
      "Name: FTR, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#VISUALISATION\n",
    "import matplotlib.pyplot as plt\n",
    "#df_dum.corr()['is_promoted'].sort_values(ascending = False).plot(kind='bar')\n",
    "import matplotlib.cm as cm\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.style.use('ggplot')\n",
    "my_cmap = cm.get_cmap('Accent')\n",
    "\n",
    "correlations = df.corr()['FTR'].sort_values(ascending = False)\n",
    "correlations.plot(kind='bar', cmap=my_cmap)\n",
    "\n",
    "plt.title(' Correlation',fontsize=20)\n",
    "plt.xlabel('Variables', fontsize=20)\n",
    "plt.ylabel('Importance Rate',fontsize=20)\n",
    "\n",
    "plt.xticks(fontsize = 10) \n",
    "plt.yticks(fontsize = 20) \n",
    "plt.grid(True)\n",
    "plt.show() \n",
    "\n",
    "print('Most Positive Correlations: \\n', correlations.head(5))\n",
    "print('\\nMost Negative Correlations: \\n', correlations.tail(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data train an test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df[['B365H', 'B365D', 'B365A','HTP', 'ATP']]\n",
    "# X = df[['away_team_goal', 'B365H', 'away_possession','home_y_card']] # using top 5 highest features \n",
    "# X = df[['away_team_goal', 'B365H', 'away_possession','home_y_card','home_r_card', 'home_cross']]\n",
    "X = df.drop(columns = ['FTR'])\n",
    "y = df['FTR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (1995, 30)\n",
      "Train labels shape:  (1995,)\n",
      "Test data shape:  (665, 30)\n",
      "Test labels shape:  (665,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "columns = X_train.values.shape[1]\n",
    "columns\n",
    "\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DECISION TREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of DecisionTreeClassifier on test set: 1.00\n",
      "Logistic Regression Confusion Matrix \n",
      " [[299   0]\n",
      " [  0 366]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as DT\n",
    "from sklearn import metrics\n",
    "\n",
    "modelDT = DT()\n",
    "modelDT = modelDT.fit(X_train, y_train)\n",
    "preds_DT = modelDT.predict(X_test)\n",
    "print('Accuracy of DecisionTreeClassifier on test set: {:.2f}'.format(metrics.accuracy_score(y_test, preds_DT)))\n",
    "print('Logistic Regression Confusion Matrix \\n', metrics.confusion_matrix(y_test, preds_DT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression classifier on test set: 1.00\n",
      "Logistic Regression Confusion Matrix \n",
      " [[299   0]\n",
      " [  0 366]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn import metrics\n",
    "\n",
    "model_LR = LR(random_state=0)\n",
    "# model_LR = model_LR.fit(X_train, y_train)\n",
    "# pred_LR = model_LR.predict(X_test)\n",
    "\n",
    "# print('Accuracy of Logistic Regression classifier on test set: {:.2f}'.format(metrics.accuracy_score(y_test, pred_LR)))\n",
    "# print('Recall of Logistic Regression classifier on test set: {:.2f}'.format(metrics.recall_score(y_test, pred_LR)))\n",
    "# print('Precision of Logistic Regression classifier on test set: {:.2f}'.format(metrics.precision_score(y_test, pred_LR)))\n",
    "# print('F1-Score of Logistic Regression classifier on test set: {:.2f}'.format(metrics.f1_score(y_test, pred_LR)))\n",
    "# print('AUC Score of Logistic Regression classifier on test set: {:.2f}'.format(metrics.roc_auc_score(y_test, pred_LR)))\n",
    "# print('Logistic Regression Confusion Matrix = \\n', metrics.confusion_matrix(y_test, pred_LR))\n",
    "\n",
    "model_LR = model_LR.fit(X_train, y_train)\n",
    "pred_LR = model_LR.predict(X_test)\n",
    "\n",
    "print('Accuracy of Logistic Regression classifier on test set: {:.2f}'.format(metrics.accuracy_score(y_test, pred_LR)))\n",
    "print('Logistic Regression Confusion Matrix \\n', metrics.confusion_matrix(y_test, pred_LR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of XGBoost classifier on test set: 1.00\n",
      "XGBoost - Confusion Matrix \n",
      " [[299   0]\n",
      " [  0 366]]\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# get an instance from the clf\n",
    "model_XGB = XGBClassifier()\n",
    "\n",
    "# fit data\n",
    "model_XGB.fit(X_train, y_train)\n",
    "\n",
    "# predict unseen data\n",
    "pred_XGB = model_XGB.predict(X_test)\n",
    "\n",
    "print('Accuracy of XGBoost classifier on test set: {:.2f}'.format(metrics.accuracy_score(y_test, pred_XGB)))\n",
    "print('XGBoost - Confusion Matrix \\n', metrics.confusion_matrix(y_test, pred_XGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KERAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 41)                1271      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 75)                3150      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 75)                0         \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 75)                0         \n",
      "=================================================================\n",
      "Total params: 4,421\n",
      "Trainable params: 4,421\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Activation, Flatten, Dense, Dropout, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# 41-75-3\n",
    "# We first need to define the sequence of dependencies (the computational graph)\n",
    "# x = Input(shape=(20,32))\n",
    "x = Input(shape=(columns,))\n",
    "# h = Conv2D(32, (7, 7), strides=(2, 2))(x)\n",
    "# h = Activation('relu')(x)\n",
    "h = Flatten()(x)\n",
    "h = Dense(41)(h)\n",
    "h = Dense(75)(h)\n",
    "# for i in range(10):\n",
    "#     h = Dense(75)(h)\n",
    "h = Dropout(0.3)(h)\n",
    "p = Activation('softmax')(h)\n",
    "\n",
    "# Now that we have defined how to find p from x, we can create a \n",
    "# model simply by saying what is input and what is output\n",
    "model = Model(inputs=x, outputs=p)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(lr=0.00001), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1915 samples, validate on 80 samples\n",
      "Epoch 1/200\n",
      "1915/1915 [==============================] - 1s 404us/sample - loss: 5.4950 - acc: 0.0219 - val_loss: 4.8052 - val_acc: 0.0250\n",
      "Epoch 2/200\n",
      "1915/1915 [==============================] - 1s 290us/sample - loss: 5.2236 - acc: 0.0491 - val_loss: 4.5145 - val_acc: 0.0500\n",
      "Epoch 3/200\n",
      "1915/1915 [==============================] - 1s 315us/sample - loss: 4.9310 - acc: 0.0872 - val_loss: 4.2455 - val_acc: 0.0875\n",
      "Epoch 4/200\n",
      "1915/1915 [==============================] - 1s 273us/sample - loss: 4.6425 - acc: 0.1222 - val_loss: 3.9854 - val_acc: 0.1875\n",
      "Epoch 5/200\n",
      "1915/1915 [==============================] - 1s 311us/sample - loss: 4.3953 - acc: 0.1781 - val_loss: 3.7424 - val_acc: 0.2250\n",
      "Epoch 6/200\n",
      "1915/1915 [==============================] - 1s 301us/sample - loss: 4.1796 - acc: 0.1932 - val_loss: 3.5170 - val_acc: 0.2875\n",
      "Epoch 7/200\n",
      "1915/1915 [==============================] - 1s 265us/sample - loss: 3.9591 - acc: 0.2282 - val_loss: 3.3028 - val_acc: 0.3250\n",
      "Epoch 8/200\n",
      "1915/1915 [==============================] - 1s 312us/sample - loss: 3.6823 - acc: 0.2663 - val_loss: 3.0959 - val_acc: 0.4125\n",
      "Epoch 9/200\n",
      "1915/1915 [==============================] - 1s 335us/sample - loss: 3.5302 - acc: 0.2752 - val_loss: 2.9058 - val_acc: 0.4500\n",
      "Epoch 10/200\n",
      "1915/1915 [==============================] - 1s 282us/sample - loss: 3.3423 - acc: 0.3065 - val_loss: 2.7281 - val_acc: 0.4625\n",
      "Epoch 11/200\n",
      "1915/1915 [==============================] - 1s 385us/sample - loss: 3.2310 - acc: 0.3018 - val_loss: 2.5638 - val_acc: 0.4750\n",
      "Epoch 12/200\n",
      "1915/1915 [==============================] - 1s 321us/sample - loss: 2.9321 - acc: 0.3561 - val_loss: 2.4012 - val_acc: 0.4875\n",
      "Epoch 13/200\n",
      "1915/1915 [==============================] - 1s 275us/sample - loss: 2.8198 - acc: 0.3749 - val_loss: 2.2547 - val_acc: 0.5125\n",
      "Epoch 14/200\n",
      "1915/1915 [==============================] - 1s 284us/sample - loss: 2.7445 - acc: 0.3849 - val_loss: 2.1152 - val_acc: 0.5500\n",
      "Epoch 15/200\n",
      "1915/1915 [==============================] - 1s 308us/sample - loss: 2.6153 - acc: 0.4136 - val_loss: 1.9902 - val_acc: 0.5500\n",
      "Epoch 16/200\n",
      "1915/1915 [==============================] - 1s 321us/sample - loss: 2.5247 - acc: 0.4324 - val_loss: 1.8766 - val_acc: 0.5875\n",
      "Epoch 17/200\n",
      "1915/1915 [==============================] - 1s 280us/sample - loss: 2.4212 - acc: 0.4627 - val_loss: 1.7749 - val_acc: 0.6000\n",
      "Epoch 18/200\n",
      "1915/1915 [==============================] - 0s 258us/sample - loss: 2.2836 - acc: 0.4987 - val_loss: 1.6768 - val_acc: 0.6125\n",
      "Epoch 19/200\n",
      "1915/1915 [==============================] - 0s 259us/sample - loss: 2.1796 - acc: 0.5044 - val_loss: 1.5917 - val_acc: 0.6375\n",
      "Epoch 20/200\n",
      "1915/1915 [==============================] - 0s 258us/sample - loss: 2.1666 - acc: 0.5175 - val_loss: 1.5206 - val_acc: 0.6500\n",
      "Epoch 21/200\n",
      "1915/1915 [==============================] - 0s 256us/sample - loss: 2.1656 - acc: 0.5164 - val_loss: 1.4612 - val_acc: 0.6500\n",
      "Epoch 22/200\n",
      "1915/1915 [==============================] - 0s 251us/sample - loss: 2.1240 - acc: 0.5211 - val_loss: 1.4067 - val_acc: 0.6750\n",
      "Epoch 23/200\n",
      "1915/1915 [==============================] - 1s 263us/sample - loss: 2.1717 - acc: 0.4997 - val_loss: 1.3633 - val_acc: 0.6750\n",
      "Epoch 24/200\n",
      "1915/1915 [==============================] - 1s 264us/sample - loss: 2.0566 - acc: 0.5264 - val_loss: 1.3187 - val_acc: 0.6750\n",
      "Epoch 25/200\n",
      "1915/1915 [==============================] - 0s 250us/sample - loss: 2.0012 - acc: 0.5300 - val_loss: 1.2776 - val_acc: 0.6875\n",
      "Epoch 26/200\n",
      "1915/1915 [==============================] - 0s 256us/sample - loss: 1.9672 - acc: 0.5394 - val_loss: 1.2415 - val_acc: 0.6875\n",
      "Epoch 27/200\n",
      "1915/1915 [==============================] - 0s 260us/sample - loss: 1.9622 - acc: 0.5436 - val_loss: 1.2128 - val_acc: 0.6875\n",
      "Epoch 28/200\n",
      "1915/1915 [==============================] - 1s 303us/sample - loss: 1.9875 - acc: 0.5274 - val_loss: 1.1881 - val_acc: 0.6875\n",
      "Epoch 29/200\n",
      "1915/1915 [==============================] - 1s 265us/sample - loss: 1.9339 - acc: 0.5415 - val_loss: 1.1646 - val_acc: 0.6875\n",
      "Epoch 30/200\n",
      "1915/1915 [==============================] - 0s 257us/sample - loss: 1.8994 - acc: 0.5478 - val_loss: 1.1399 - val_acc: 0.6875\n",
      "Epoch 31/200\n",
      "1915/1915 [==============================] - 1s 296us/sample - loss: 1.8715 - acc: 0.5384 - val_loss: 1.1162 - val_acc: 0.7000\n",
      "Epoch 32/200\n",
      "1915/1915 [==============================] - 0s 253us/sample - loss: 1.8233 - acc: 0.5614 - val_loss: 1.0934 - val_acc: 0.7125\n",
      "Epoch 33/200\n",
      "1915/1915 [==============================] - 1s 280us/sample - loss: 1.7950 - acc: 0.5634 - val_loss: 1.0697 - val_acc: 0.7125\n",
      "Epoch 34/200\n",
      "1915/1915 [==============================] - 1s 284us/sample - loss: 1.8284 - acc: 0.5514 - val_loss: 1.0541 - val_acc: 0.7125\n",
      "Epoch 35/200\n",
      "1915/1915 [==============================] - 1s 318us/sample - loss: 1.8802 - acc: 0.5540 - val_loss: 1.0418 - val_acc: 0.7125\n",
      "Epoch 36/200\n",
      "1915/1915 [==============================] - 1s 322us/sample - loss: 1.8010 - acc: 0.5577 - val_loss: 1.0227 - val_acc: 0.7125\n",
      "Epoch 37/200\n",
      "1915/1915 [==============================] - 1s 325us/sample - loss: 1.8725 - acc: 0.5405 - val_loss: 1.0094 - val_acc: 0.7125\n",
      "Epoch 38/200\n",
      "1915/1915 [==============================] - 1s 334us/sample - loss: 1.8501 - acc: 0.5499 - val_loss: 0.9974 - val_acc: 0.7125\n",
      "Epoch 39/200\n",
      "1915/1915 [==============================] - 1s 337us/sample - loss: 1.8347 - acc: 0.5483 - val_loss: 0.9847 - val_acc: 0.7250\n",
      "Epoch 40/200\n",
      "1915/1915 [==============================] - 1s 303us/sample - loss: 1.7248 - acc: 0.5833 - val_loss: 0.9681 - val_acc: 0.7250\n",
      "Epoch 41/200\n",
      "1915/1915 [==============================] - 1s 326us/sample - loss: 1.7445 - acc: 0.5671 - val_loss: 0.9531 - val_acc: 0.7250\n",
      "Epoch 42/200\n",
      "1915/1915 [==============================] - 1s 275us/sample - loss: 1.6532 - acc: 0.5963 - val_loss: 0.9348 - val_acc: 0.7375\n",
      "Epoch 43/200\n",
      "1915/1915 [==============================] - 1s 344us/sample - loss: 1.7710 - acc: 0.5744 - val_loss: 0.9223 - val_acc: 0.7375\n",
      "Epoch 44/200\n",
      "1915/1915 [==============================] - 1s 279us/sample - loss: 1.7926 - acc: 0.5671 - val_loss: 0.9130 - val_acc: 0.7375\n",
      "Epoch 45/200\n",
      "1915/1915 [==============================] - 1s 311us/sample - loss: 1.7178 - acc: 0.5728 - val_loss: 0.8982 - val_acc: 0.7500\n",
      "Epoch 46/200\n",
      "1915/1915 [==============================] - 1s 272us/sample - loss: 1.6963 - acc: 0.5854 - val_loss: 0.8866 - val_acc: 0.7500\n",
      "Epoch 47/200\n",
      "1915/1915 [==============================] - 1s 268us/sample - loss: 1.6944 - acc: 0.5896 - val_loss: 0.8725 - val_acc: 0.7500\n",
      "Epoch 48/200\n",
      "1915/1915 [==============================] - 1s 336us/sample - loss: 1.7436 - acc: 0.5833 - val_loss: 0.8625 - val_acc: 0.7500\n",
      "Epoch 49/200\n",
      "1915/1915 [==============================] - 1s 324us/sample - loss: 1.6284 - acc: 0.5984 - val_loss: 0.8488 - val_acc: 0.7500\n",
      "Epoch 50/200\n",
      "1915/1915 [==============================] - 1s 300us/sample - loss: 1.6497 - acc: 0.6115 - val_loss: 0.8351 - val_acc: 0.7500\n",
      "Epoch 51/200\n",
      "1915/1915 [==============================] - 1s 291us/sample - loss: 1.6516 - acc: 0.6162 - val_loss: 0.8238 - val_acc: 0.7500\n",
      "Epoch 52/200\n",
      "1915/1915 [==============================] - 1s 413us/sample - loss: 1.6485 - acc: 0.6151 - val_loss: 0.8122 - val_acc: 0.7625\n",
      "Epoch 53/200\n",
      "1915/1915 [==============================] - 1s 326us/sample - loss: 1.6247 - acc: 0.6209 - val_loss: 0.8020 - val_acc: 0.7625\n",
      "Epoch 54/200\n",
      "1915/1915 [==============================] - 1s 285us/sample - loss: 1.5588 - acc: 0.6402 - val_loss: 0.7848 - val_acc: 0.7625\n",
      "Epoch 55/200\n",
      "1915/1915 [==============================] - 1s 283us/sample - loss: 1.6688 - acc: 0.6115 - val_loss: 0.7743 - val_acc: 0.7625\n",
      "Epoch 56/200\n",
      "1915/1915 [==============================] - 1s 269us/sample - loss: 1.6280 - acc: 0.6298 - val_loss: 0.7655 - val_acc: 0.7625\n",
      "Epoch 57/200\n",
      "1915/1915 [==============================] - 1s 301us/sample - loss: 1.5665 - acc: 0.6371 - val_loss: 0.7533 - val_acc: 0.7625\n",
      "Epoch 58/200\n",
      "1915/1915 [==============================] - 1s 311us/sample - loss: 1.5698 - acc: 0.6345 - val_loss: 0.7424 - val_acc: 0.7875\n",
      "Epoch 59/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1915/1915 [==============================] - 1s 305us/sample - loss: 1.5712 - acc: 0.6480 - val_loss: 0.7315 - val_acc: 0.7875\n",
      "Epoch 60/200\n",
      "1915/1915 [==============================] - 1s 282us/sample - loss: 1.6289 - acc: 0.6204 - val_loss: 0.7233 - val_acc: 0.8000\n",
      "Epoch 61/200\n",
      "1915/1915 [==============================] - 1s 296us/sample - loss: 1.5841 - acc: 0.6392 - val_loss: 0.7120 - val_acc: 0.8000\n",
      "Epoch 62/200\n",
      "1915/1915 [==============================] - 1s 305us/sample - loss: 1.5817 - acc: 0.6360 - val_loss: 0.7032 - val_acc: 0.8000\n",
      "Epoch 63/200\n",
      "1915/1915 [==============================] - 1s 308us/sample - loss: 1.5513 - acc: 0.6444 - val_loss: 0.6933 - val_acc: 0.8000\n",
      "Epoch 64/200\n",
      "1915/1915 [==============================] - 1s 313us/sample - loss: 1.4976 - acc: 0.6580 - val_loss: 0.6795 - val_acc: 0.8000\n",
      "Epoch 65/200\n",
      "1915/1915 [==============================] - 1s 311us/sample - loss: 1.5710 - acc: 0.6413 - val_loss: 0.6692 - val_acc: 0.8250\n",
      "Epoch 66/200\n",
      "1915/1915 [==============================] - 1s 271us/sample - loss: 1.5556 - acc: 0.6407 - val_loss: 0.6606 - val_acc: 0.8250\n",
      "Epoch 67/200\n",
      "1915/1915 [==============================] - 0s 258us/sample - loss: 1.5421 - acc: 0.6475 - val_loss: 0.6510 - val_acc: 0.8250\n",
      "Epoch 68/200\n",
      "1915/1915 [==============================] - 0s 260us/sample - loss: 1.5356 - acc: 0.6491 - val_loss: 0.6424 - val_acc: 0.8250\n",
      "Epoch 69/200\n",
      "1915/1915 [==============================] - 1s 308us/sample - loss: 1.4122 - acc: 0.6830 - val_loss: 0.6293 - val_acc: 0.8250\n",
      "Epoch 70/200\n",
      "1915/1915 [==============================] - 1s 279us/sample - loss: 1.5325 - acc: 0.6564 - val_loss: 0.6214 - val_acc: 0.8250\n",
      "Epoch 71/200\n",
      "1915/1915 [==============================] - 1s 328us/sample - loss: 1.5880 - acc: 0.6433 - val_loss: 0.6152 - val_acc: 0.8375\n",
      "Epoch 72/200\n",
      "1915/1915 [==============================] - 1s 323us/sample - loss: 1.4178 - acc: 0.6856 - val_loss: 0.6030 - val_acc: 0.8625\n",
      "Epoch 73/200\n",
      "1915/1915 [==============================] - 1s 437us/sample - loss: 1.5440 - acc: 0.6460 - val_loss: 0.5968 - val_acc: 0.8625\n",
      "Epoch 74/200\n",
      "1915/1915 [==============================] - 1s 334us/sample - loss: 1.4630 - acc: 0.6809 - val_loss: 0.5871 - val_acc: 0.8625\n",
      "Epoch 75/200\n",
      "1915/1915 [==============================] - 1s 357us/sample - loss: 1.4905 - acc: 0.6580 - val_loss: 0.5800 - val_acc: 0.8625\n",
      "Epoch 76/200\n",
      "1915/1915 [==============================] - 1s 309us/sample - loss: 1.5017 - acc: 0.6548 - val_loss: 0.5720 - val_acc: 0.8625\n",
      "Epoch 77/200\n",
      "1915/1915 [==============================] - 1s 279us/sample - loss: 1.4941 - acc: 0.6721 - val_loss: 0.5659 - val_acc: 0.8625\n",
      "Epoch 78/200\n",
      "1915/1915 [==============================] - 1s 265us/sample - loss: 1.4530 - acc: 0.6799 - val_loss: 0.5566 - val_acc: 0.8625\n",
      "Epoch 79/200\n",
      "1915/1915 [==============================] - 1s 290us/sample - loss: 1.4868 - acc: 0.6595 - val_loss: 0.5507 - val_acc: 0.8750\n",
      "Epoch 80/200\n",
      "1915/1915 [==============================] - 1s 341us/sample - loss: 1.4631 - acc: 0.6752 - val_loss: 0.5426 - val_acc: 0.8750\n",
      "Epoch 81/200\n",
      "1915/1915 [==============================] - 1s 289us/sample - loss: 1.4300 - acc: 0.6768 - val_loss: 0.5349 - val_acc: 0.8750\n",
      "Epoch 82/200\n",
      "1915/1915 [==============================] - 1s 317us/sample - loss: 1.5044 - acc: 0.6726 - val_loss: 0.5282 - val_acc: 0.8750\n",
      "Epoch 83/200\n",
      "1915/1915 [==============================] - 1s 322us/sample - loss: 1.4997 - acc: 0.6668 - val_loss: 0.5235 - val_acc: 0.8750\n",
      "Epoch 84/200\n",
      "1915/1915 [==============================] - 1s 358us/sample - loss: 1.4128 - acc: 0.6935 - val_loss: 0.5152 - val_acc: 0.8750\n",
      "Epoch 85/200\n",
      "1915/1915 [==============================] - 1s 379us/sample - loss: 1.4406 - acc: 0.6961 - val_loss: 0.5085 - val_acc: 0.8750\n",
      "Epoch 86/200\n",
      "1915/1915 [==============================] - 1s 430us/sample - loss: 1.4416 - acc: 0.6815 - val_loss: 0.5018 - val_acc: 0.8750\n",
      "Epoch 87/200\n",
      "1915/1915 [==============================] - 1s 315us/sample - loss: 1.4436 - acc: 0.6815 - val_loss: 0.4955 - val_acc: 0.8750\n",
      "Epoch 88/200\n",
      "1915/1915 [==============================] - 1s 351us/sample - loss: 1.3866 - acc: 0.6982 - val_loss: 0.4887 - val_acc: 0.8750\n",
      "Epoch 89/200\n",
      "1915/1915 [==============================] - 1s 302us/sample - loss: 1.4496 - acc: 0.6825 - val_loss: 0.4827 - val_acc: 0.8750\n",
      "Epoch 90/200\n",
      "1915/1915 [==============================] - 1s 307us/sample - loss: 1.4066 - acc: 0.6867 - val_loss: 0.4776 - val_acc: 0.8750\n",
      "Epoch 91/200\n",
      "1915/1915 [==============================] - 1s 286us/sample - loss: 1.4208 - acc: 0.7008 - val_loss: 0.4713 - val_acc: 0.8750\n",
      "Epoch 92/200\n",
      "1915/1915 [==============================] - 1s 291us/sample - loss: 1.4781 - acc: 0.6658 - val_loss: 0.4663 - val_acc: 0.8750\n",
      "Epoch 93/200\n",
      "1915/1915 [==============================] - 1s 274us/sample - loss: 1.4050 - acc: 0.6877 - val_loss: 0.4613 - val_acc: 0.8750\n",
      "Epoch 94/200\n",
      "1915/1915 [==============================] - 1s 325us/sample - loss: 1.4875 - acc: 0.6841 - val_loss: 0.4567 - val_acc: 0.8750\n",
      "Epoch 95/200\n",
      "1915/1915 [==============================] - 1s 341us/sample - loss: 1.3956 - acc: 0.7107 - val_loss: 0.4500 - val_acc: 0.8750\n",
      "Epoch 96/200\n",
      "1915/1915 [==============================] - 1s 319us/sample - loss: 1.3409 - acc: 0.7154 - val_loss: 0.4430 - val_acc: 0.8750\n",
      "Epoch 97/200\n",
      "1915/1915 [==============================] - 1s 339us/sample - loss: 1.3709 - acc: 0.7029 - val_loss: 0.4385 - val_acc: 0.8750\n",
      "Epoch 98/200\n",
      "1915/1915 [==============================] - 1s 297us/sample - loss: 1.3664 - acc: 0.7144 - val_loss: 0.4330 - val_acc: 0.8875\n",
      "Epoch 99/200\n",
      "1915/1915 [==============================] - 1s 359us/sample - loss: 1.3884 - acc: 0.7018 - val_loss: 0.4280 - val_acc: 0.9000\n",
      "Epoch 100/200\n",
      "1915/1915 [==============================] - 1s 331us/sample - loss: 1.3614 - acc: 0.7138 - val_loss: 0.4226 - val_acc: 0.9000\n",
      "Epoch 101/200\n",
      "1915/1915 [==============================] - 1s 330us/sample - loss: 1.4385 - acc: 0.6956 - val_loss: 0.4191 - val_acc: 0.9000\n",
      "Epoch 102/200\n",
      "1915/1915 [==============================] - 1s 312us/sample - loss: 1.3732 - acc: 0.7117 - val_loss: 0.4143 - val_acc: 0.9125\n",
      "Epoch 103/200\n",
      "1915/1915 [==============================] - 1s 283us/sample - loss: 1.3393 - acc: 0.7185 - val_loss: 0.4093 - val_acc: 0.9375\n",
      "Epoch 104/200\n",
      "1915/1915 [==============================] - 1s 291us/sample - loss: 1.3287 - acc: 0.7238 - val_loss: 0.4046 - val_acc: 0.9375\n",
      "Epoch 105/200\n",
      "1915/1915 [==============================] - 1s 285us/sample - loss: 1.3589 - acc: 0.7201 - val_loss: 0.4000 - val_acc: 0.9375\n",
      "Epoch 106/200\n",
      "1915/1915 [==============================] - 1s 278us/sample - loss: 1.3403 - acc: 0.7191 - val_loss: 0.3951 - val_acc: 0.9375\n",
      "Epoch 107/200\n",
      "1915/1915 [==============================] - 1s 339us/sample - loss: 1.3554 - acc: 0.7133 - val_loss: 0.3908 - val_acc: 0.9375\n",
      "Epoch 108/200\n",
      "1915/1915 [==============================] - 1s 281us/sample - loss: 1.3142 - acc: 0.7217 - val_loss: 0.3871 - val_acc: 0.9375\n",
      "Epoch 109/200\n",
      "1915/1915 [==============================] - 1s 288us/sample - loss: 1.3843 - acc: 0.7133 - val_loss: 0.3830 - val_acc: 0.9375\n",
      "Epoch 110/200\n",
      "1915/1915 [==============================] - 1s 268us/sample - loss: 1.3481 - acc: 0.7159 - val_loss: 0.3789 - val_acc: 0.9500\n",
      "Epoch 111/200\n",
      "1915/1915 [==============================] - 1s 324us/sample - loss: 1.3508 - acc: 0.7180 - val_loss: 0.3751 - val_acc: 0.9625\n",
      "Epoch 112/200\n",
      "1915/1915 [==============================] - 1s 329us/sample - loss: 1.2957 - acc: 0.7389 - val_loss: 0.3693 - val_acc: 0.9500\n",
      "Epoch 113/200\n",
      "1915/1915 [==============================] - 1s 297us/sample - loss: 1.4238 - acc: 0.7029 - val_loss: 0.3659 - val_acc: 0.9500\n",
      "Epoch 114/200\n",
      "1915/1915 [==============================] - 1s 294us/sample - loss: 1.3275 - acc: 0.7290 - val_loss: 0.3616 - val_acc: 0.9500\n",
      "Epoch 115/200\n",
      "1915/1915 [==============================] - 1s 323us/sample - loss: 1.3448 - acc: 0.7206 - val_loss: 0.3577 - val_acc: 0.9500\n",
      "Epoch 116/200\n",
      "1915/1915 [==============================] - 1s 315us/sample - loss: 1.3360 - acc: 0.7238 - val_loss: 0.3532 - val_acc: 0.9500\n",
      "Epoch 117/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1915/1915 [==============================] - 1s 301us/sample - loss: 1.3883 - acc: 0.7008 - val_loss: 0.3501 - val_acc: 0.9625\n",
      "Epoch 118/200\n",
      "1915/1915 [==============================] - 1s 287us/sample - loss: 1.3141 - acc: 0.7305 - val_loss: 0.3464 - val_acc: 0.9625\n",
      "Epoch 119/200\n",
      "1915/1915 [==============================] - 1s 262us/sample - loss: 1.2875 - acc: 0.7295 - val_loss: 0.3420 - val_acc: 0.9625\n",
      "Epoch 120/200\n",
      "1915/1915 [==============================] - 1s 269us/sample - loss: 1.2978 - acc: 0.7258 - val_loss: 0.3386 - val_acc: 0.9625\n",
      "Epoch 121/200\n",
      "1915/1915 [==============================] - 1s 285us/sample - loss: 1.2550 - acc: 0.7415 - val_loss: 0.3344 - val_acc: 0.9625\n",
      "Epoch 122/200\n",
      "1915/1915 [==============================] - 1s 358us/sample - loss: 1.3689 - acc: 0.7039 - val_loss: 0.3313 - val_acc: 0.9625\n",
      "Epoch 123/200\n",
      "1915/1915 [==============================] - 1s 304us/sample - loss: 1.3370 - acc: 0.7159 - val_loss: 0.3280 - val_acc: 0.9625\n",
      "Epoch 124/200\n",
      "1915/1915 [==============================] - 1s 354us/sample - loss: 1.3631 - acc: 0.7170 - val_loss: 0.3243 - val_acc: 0.9625\n",
      "Epoch 125/200\n",
      "1915/1915 [==============================] - 1s 332us/sample - loss: 1.3338 - acc: 0.7201 - val_loss: 0.3206 - val_acc: 0.9625\n",
      "Epoch 126/200\n",
      "1915/1915 [==============================] - 1s 304us/sample - loss: 1.3092 - acc: 0.7269 - val_loss: 0.3170 - val_acc: 0.9625\n",
      "Epoch 127/200\n",
      "1915/1915 [==============================] - 1s 294us/sample - loss: 1.2619 - acc: 0.7352 - val_loss: 0.3135 - val_acc: 0.9625\n",
      "Epoch 128/200\n",
      "1915/1915 [==============================] - 1s 501us/sample - loss: 1.3286 - acc: 0.7253 - val_loss: 0.3099 - val_acc: 0.9625\n",
      "Epoch 129/200\n",
      "1915/1915 [==============================] - 1s 395us/sample - loss: 1.2640 - acc: 0.7347 - val_loss: 0.3062 - val_acc: 0.9625\n",
      "Epoch 130/200\n",
      "1915/1915 [==============================] - 1s 296us/sample - loss: 1.3260 - acc: 0.7232 - val_loss: 0.3028 - val_acc: 0.9625\n",
      "Epoch 131/200\n",
      "1915/1915 [==============================] - 0s 256us/sample - loss: 1.2453 - acc: 0.7441 - val_loss: 0.2993 - val_acc: 0.9625\n",
      "Epoch 132/200\n",
      "1915/1915 [==============================] - 1s 492us/sample - loss: 1.2796 - acc: 0.7300 - val_loss: 0.2959 - val_acc: 0.9625\n",
      "Epoch 133/200\n",
      "1915/1915 [==============================] - 1s 331us/sample - loss: 1.2807 - acc: 0.7405 - val_loss: 0.2923 - val_acc: 0.9750\n",
      "Epoch 134/200\n",
      "1915/1915 [==============================] - 1s 278us/sample - loss: 1.2573 - acc: 0.7399 - val_loss: 0.2887 - val_acc: 0.9625\n",
      "Epoch 135/200\n",
      "1915/1915 [==============================] - 1s 279us/sample - loss: 1.3510 - acc: 0.7175 - val_loss: 0.2864 - val_acc: 0.9625\n",
      "Epoch 136/200\n",
      "1915/1915 [==============================] - 1s 266us/sample - loss: 1.2934 - acc: 0.7248 - val_loss: 0.2835 - val_acc: 0.9625\n",
      "Epoch 137/200\n",
      "1915/1915 [==============================] - 0s 259us/sample - loss: 1.3423 - acc: 0.7164 - val_loss: 0.2804 - val_acc: 0.9625\n",
      "Epoch 138/200\n",
      "1915/1915 [==============================] - 0s 258us/sample - loss: 1.2474 - acc: 0.7342 - val_loss: 0.2772 - val_acc: 0.9625\n",
      "Epoch 139/200\n",
      "1915/1915 [==============================] - 1s 452us/sample - loss: 1.2627 - acc: 0.7279 - val_loss: 0.2744 - val_acc: 0.9875\n",
      "Epoch 140/200\n",
      "1915/1915 [==============================] - 2s 789us/sample - loss: 1.2262 - acc: 0.7426 - val_loss: 0.2716 - val_acc: 0.9875\n",
      "Epoch 141/200\n",
      "1915/1915 [==============================] - 1s 777us/sample - loss: 1.2364 - acc: 0.7415 - val_loss: 0.2678 - val_acc: 0.9875loss: 1.\n",
      "Epoch 142/200\n",
      "1915/1915 [==============================] - 1s 404us/sample - loss: 1.3206 - acc: 0.7185 - val_loss: 0.2646 - val_acc: 0.9875\n",
      "Epoch 143/200\n",
      "1915/1915 [==============================] - 1s 282us/sample - loss: 1.2157 - acc: 0.7483 - val_loss: 0.2613 - val_acc: 0.9875\n",
      "Epoch 144/200\n",
      "1915/1915 [==============================] - 0s 258us/sample - loss: 1.2382 - acc: 0.7363 - val_loss: 0.2581 - val_acc: 0.9875\n",
      "Epoch 145/200\n",
      "1915/1915 [==============================] - 0s 258us/sample - loss: 1.2279 - acc: 0.7415 - val_loss: 0.2552 - val_acc: 0.9875\n",
      "Epoch 146/200\n",
      "1915/1915 [==============================] - 1s 300us/sample - loss: 1.2556 - acc: 0.7295 - val_loss: 0.2524 - val_acc: 0.9875\n",
      "Epoch 147/200\n",
      "1915/1915 [==============================] - 1s 313us/sample - loss: 1.2402 - acc: 0.7337 - val_loss: 0.2496 - val_acc: 0.9875\n",
      "Epoch 148/200\n",
      "1915/1915 [==============================] - 1s 317us/sample - loss: 1.2800 - acc: 0.7238 - val_loss: 0.2466 - val_acc: 0.9875\n",
      "Epoch 149/200\n",
      "1915/1915 [==============================] - 1s 285us/sample - loss: 1.2333 - acc: 0.7420 - val_loss: 0.2436 - val_acc: 0.9875\n",
      "Epoch 150/200\n",
      "1915/1915 [==============================] - 1s 343us/sample - loss: 1.2628 - acc: 0.7358 - val_loss: 0.2408 - val_acc: 0.9875\n",
      "Epoch 151/200\n",
      "1915/1915 [==============================] - 1s 326us/sample - loss: 1.2428 - acc: 0.7311 - val_loss: 0.2384 - val_acc: 0.9875\n",
      "Epoch 152/200\n",
      "1915/1915 [==============================] - 1s 312us/sample - loss: 1.2591 - acc: 0.7295 - val_loss: 0.2354 - val_acc: 0.9875\n",
      "Epoch 153/200\n",
      "1915/1915 [==============================] - 1s 325us/sample - loss: 1.2501 - acc: 0.7399 - val_loss: 0.2321 - val_acc: 0.9875\n",
      "Epoch 154/200\n",
      "1915/1915 [==============================] - 1s 348us/sample - loss: 1.2492 - acc: 0.7337 - val_loss: 0.2293 - val_acc: 0.9875\n",
      "Epoch 155/200\n",
      "1915/1915 [==============================] - 1s 339us/sample - loss: 1.2098 - acc: 0.7358 - val_loss: 0.2264 - val_acc: 0.9875\n",
      "Epoch 156/200\n",
      "1915/1915 [==============================] - 1s 281us/sample - loss: 1.2299 - acc: 0.7368 - val_loss: 0.2237 - val_acc: 0.9875\n",
      "Epoch 157/200\n",
      "1915/1915 [==============================] - 2s 787us/sample - loss: 1.2014 - acc: 0.7478 - val_loss: 0.2210 - val_acc: 0.9875\n",
      "Epoch 158/200\n",
      "1915/1915 [==============================] - 1s 560us/sample - loss: 1.2246 - acc: 0.7399 - val_loss: 0.2183 - val_acc: 0.9875\n",
      "Epoch 159/200\n",
      "1915/1915 [==============================] - 1s 454us/sample - loss: 1.2835 - acc: 0.7264 - val_loss: 0.2156 - val_acc: 0.9875\n",
      "Epoch 160/200\n",
      "1915/1915 [==============================] - 1s 327us/sample - loss: 1.2528 - acc: 0.7274 - val_loss: 0.2130 - val_acc: 0.9875\n",
      "Epoch 161/200\n",
      "1915/1915 [==============================] - 1s 580us/sample - loss: 1.2276 - acc: 0.7389 - val_loss: 0.2104 - val_acc: 0.9875\n",
      "Epoch 162/200\n",
      "1915/1915 [==============================] - 1s 374us/sample - loss: 1.2818 - acc: 0.7164 - val_loss: 0.2078 - val_acc: 0.9875\n",
      "Epoch 163/200\n",
      "1915/1915 [==============================] - 1s 337us/sample - loss: 1.1990 - acc: 0.7347 - val_loss: 0.2051 - val_acc: 0.9875\n",
      "Epoch 164/200\n",
      "1915/1915 [==============================] - 1s 372us/sample - loss: 1.1790 - acc: 0.7483 - val_loss: 0.2022 - val_acc: 0.9875\n",
      "Epoch 165/200\n",
      "1915/1915 [==============================] - 1s 337us/sample - loss: 1.2348 - acc: 0.7305 - val_loss: 0.1998 - val_acc: 0.9875\n",
      "Epoch 166/200\n",
      "1915/1915 [==============================] - 1s 326us/sample - loss: 1.2011 - acc: 0.7467 - val_loss: 0.1972 - val_acc: 0.9875\n",
      "Epoch 167/200\n",
      "1915/1915 [==============================] - 1s 368us/sample - loss: 1.2190 - acc: 0.7290 - val_loss: 0.1945 - val_acc: 0.9875\n",
      "Epoch 168/200\n",
      "1915/1915 [==============================] - 1s 406us/sample - loss: 1.1915 - acc: 0.7379 - val_loss: 0.1924 - val_acc: 0.9875\n",
      "Epoch 169/200\n",
      "1915/1915 [==============================] - 1s 413us/sample - loss: 1.1354 - acc: 0.7530 - val_loss: 0.1896 - val_acc: 0.9875\n",
      "Epoch 170/200\n",
      "1915/1915 [==============================] - 1s 329us/sample - loss: 1.1873 - acc: 0.7410 - val_loss: 0.1872 - val_acc: 0.9875\n",
      "Epoch 171/200\n",
      "1915/1915 [==============================] - 1s 329us/sample - loss: 1.1308 - acc: 0.7530 - val_loss: 0.1846 - val_acc: 0.9875\n",
      "Epoch 172/200\n",
      "1915/1915 [==============================] - 1s 299us/sample - loss: 1.1859 - acc: 0.7347 - val_loss: 0.1824 - val_acc: 0.9875\n",
      "Epoch 173/200\n",
      "1915/1915 [==============================] - 1s 291us/sample - loss: 1.0983 - acc: 0.7551 - val_loss: 0.1798 - val_acc: 0.9875\n",
      "Epoch 174/200\n",
      "1915/1915 [==============================] - 0s 260us/sample - loss: 1.1961 - acc: 0.7368 - val_loss: 0.1775 - val_acc: 0.9875\n",
      "Epoch 175/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1915/1915 [==============================] - 1s 270us/sample - loss: 1.1796 - acc: 0.7373 - val_loss: 0.1749 - val_acc: 0.9875\n",
      "Epoch 176/200\n",
      "1915/1915 [==============================] - 1s 311us/sample - loss: 1.1828 - acc: 0.7368 - val_loss: 0.1725 - val_acc: 0.9875\n",
      "Epoch 177/200\n",
      "1915/1915 [==============================] - 1s 326us/sample - loss: 1.1956 - acc: 0.7305 - val_loss: 0.1703 - val_acc: 0.9875\n",
      "Epoch 178/200\n",
      "1915/1915 [==============================] - 1s 314us/sample - loss: 1.1143 - acc: 0.7525 - val_loss: 0.1678 - val_acc: 0.9875\n",
      "Epoch 179/200\n",
      "1915/1915 [==============================] - 1s 283us/sample - loss: 1.1918 - acc: 0.7415 - val_loss: 0.1655 - val_acc: 0.9875\n",
      "Epoch 180/200\n",
      "1915/1915 [==============================] - 1s 289us/sample - loss: 1.1633 - acc: 0.7530 - val_loss: 0.1630 - val_acc: 0.9875\n",
      "Epoch 181/200\n",
      "1915/1915 [==============================] - 1s 373us/sample - loss: 1.1907 - acc: 0.7321 - val_loss: 0.1609 - val_acc: 0.9875\n",
      "Epoch 182/200\n",
      "1915/1915 [==============================] - 1s 439us/sample - loss: 1.1521 - acc: 0.7410 - val_loss: 0.1588 - val_acc: 0.9875\n",
      "Epoch 183/200\n",
      "1915/1915 [==============================] - 1s 443us/sample - loss: 1.1561 - acc: 0.7426 - val_loss: 0.1566 - val_acc: 0.9875\n",
      "Epoch 184/200\n",
      "1915/1915 [==============================] - 1s 342us/sample - loss: 1.1486 - acc: 0.7368 - val_loss: 0.1544 - val_acc: 0.9875\n",
      "Epoch 185/200\n",
      "1915/1915 [==============================] - 1s 428us/sample - loss: 1.1691 - acc: 0.7394 - val_loss: 0.1522 - val_acc: 0.9875\n",
      "Epoch 186/200\n",
      "1915/1915 [==============================] - 1s 350us/sample - loss: 1.1873 - acc: 0.7363 - val_loss: 0.1500 - val_acc: 0.9875\n",
      "Epoch 187/200\n",
      "1915/1915 [==============================] - 1s 483us/sample - loss: 1.1180 - acc: 0.7488 - val_loss: 0.1482 - val_acc: 0.9875\n",
      "Epoch 188/200\n",
      "1915/1915 [==============================] - 1s 414us/sample - loss: 1.1549 - acc: 0.7389 - val_loss: 0.1461 - val_acc: 0.9875\n",
      "Epoch 189/200\n",
      "1915/1915 [==============================] - 1s 341us/sample - loss: 1.1386 - acc: 0.7384 - val_loss: 0.1440 - val_acc: 0.9875\n",
      "Epoch 190/200\n",
      "1915/1915 [==============================] - 1s 278us/sample - loss: 1.1229 - acc: 0.7478 - val_loss: 0.1418 - val_acc: 0.9875\n",
      "Epoch 191/200\n",
      "1915/1915 [==============================] - 0s 254us/sample - loss: 1.1681 - acc: 0.7311 - val_loss: 0.1398 - val_acc: 0.9875\n",
      "Epoch 192/200\n",
      "1915/1915 [==============================] - 1s 328us/sample - loss: 1.1036 - acc: 0.7525 - val_loss: 0.1377 - val_acc: 0.9875\n",
      "Epoch 193/200\n",
      "1915/1915 [==============================] - 1s 316us/sample - loss: 1.1353 - acc: 0.7410 - val_loss: 0.1356 - val_acc: 0.9875\n",
      "Epoch 194/200\n",
      "1915/1915 [==============================] - 1s 301us/sample - loss: 1.1896 - acc: 0.7290 - val_loss: 0.1336 - val_acc: 0.9875\n",
      "Epoch 195/200\n",
      "1915/1915 [==============================] - 1s 300us/sample - loss: 1.1446 - acc: 0.7405 - val_loss: 0.1319 - val_acc: 1.0000\n",
      "Epoch 196/200\n",
      "1915/1915 [==============================] - 1s 294us/sample - loss: 1.0721 - acc: 0.7603 - val_loss: 0.1297 - val_acc: 1.0000\n",
      "Epoch 197/200\n",
      "1915/1915 [==============================] - 1s 321us/sample - loss: 1.0643 - acc: 0.7572 - val_loss: 0.1278 - val_acc: 1.0000\n",
      "Epoch 198/200\n",
      "1915/1915 [==============================] - 1s 293us/sample - loss: 1.0870 - acc: 0.7546 - val_loss: 0.1257 - val_acc: 1.0000\n",
      "Epoch 199/200\n",
      "1915/1915 [==============================] - 1s 262us/sample - loss: 1.1271 - acc: 0.7399 - val_loss: 0.1239 - val_acc: 1.0000\n",
      "Epoch 200/200\n",
      "1915/1915 [==============================] - 1s 290us/sample - loss: 1.0885 - acc: 0.7462 - val_loss: 0.1219 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=8, epochs=200, validation_split=0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
