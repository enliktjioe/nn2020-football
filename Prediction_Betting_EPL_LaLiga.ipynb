{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "# making all pandas columns visable with display command\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EPL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Train (Season 2009/2010 until Season 2015/2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>match_api_id</th>\n",
       "      <th>B365H</th>\n",
       "      <th>B365D</th>\n",
       "      <th>B365A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>658575</td>\n",
       "      <td>1.67</td>\n",
       "      <td>3.60</td>\n",
       "      <td>5.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>658578</td>\n",
       "      <td>3.60</td>\n",
       "      <td>3.25</td>\n",
       "      <td>2.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>658579</td>\n",
       "      <td>2.25</td>\n",
       "      <td>3.25</td>\n",
       "      <td>3.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>658582</td>\n",
       "      <td>1.17</td>\n",
       "      <td>6.50</td>\n",
       "      <td>21.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>658576</td>\n",
       "      <td>3.20</td>\n",
       "      <td>3.25</td>\n",
       "      <td>2.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     match_api_id  B365H  B365D  B365A\n",
       "382        658575   1.67   3.60   5.50\n",
       "385        658578   3.60   3.25   2.10\n",
       "386        658579   2.25   3.25   3.25\n",
       "389        658582   1.17   6.50  21.00\n",
       "383        658576   3.20   3.25   2.30"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv(\"datasets/EPL_sort.csv\", index_col=0)\n",
    "df1 = df1[df1['season'] != '2008/2009']\n",
    "df1 = df1[['match_api_id','B365H', 'B365D', 'B365A']]\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>match_api_id</th>\n",
       "      <th>FTR</th>\n",
       "      <th>HTGS</th>\n",
       "      <th>ATGS</th>\n",
       "      <th>HTGC</th>\n",
       "      <th>ATGC</th>\n",
       "      <th>HTP</th>\n",
       "      <th>ATP</th>\n",
       "      <th>HM1</th>\n",
       "      <th>HM2</th>\n",
       "      <th>HM3</th>\n",
       "      <th>HM4</th>\n",
       "      <th>HM5</th>\n",
       "      <th>AM1</th>\n",
       "      <th>AM2</th>\n",
       "      <th>AM3</th>\n",
       "      <th>AM4</th>\n",
       "      <th>AM5</th>\n",
       "      <th>HTWinStreak3</th>\n",
       "      <th>HTWinStreak5</th>\n",
       "      <th>HTLossStreak3</th>\n",
       "      <th>HTLossStreak5</th>\n",
       "      <th>ATWinStreak3</th>\n",
       "      <th>ATWinStreak5</th>\n",
       "      <th>ATLossStreak3</th>\n",
       "      <th>ATLossStreak5</th>\n",
       "      <th>HTGD</th>\n",
       "      <th>ATGD</th>\n",
       "      <th>DiffPts</th>\n",
       "      <th>DiffFormPts</th>\n",
       "      <th>DiffLP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1987033</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1987034</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1987035</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1987036</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1987037</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   match_api_id  FTR  HTGS  ATGS  HTGC  ATGC  HTP  ATP  HM1  HM2  HM3  HM4  \\\n",
       "0       1987033    0   0.0   0.0   0.0   0.0  0.0  0.0    2    2    2    2   \n",
       "1       1987034    1   0.0   0.0   0.0   0.0  0.0  0.0    2    2    2    2   \n",
       "2       1987035    1   0.0   0.0   0.0   0.0  0.0  0.0    2    2    2    2   \n",
       "3       1987036    2   0.0   0.0   0.0   0.0  0.0  0.0    2    2    2    2   \n",
       "4       1987037    2   0.0   0.0   0.0   0.0  0.0  0.0    2    2    2    2   \n",
       "\n",
       "   HM5  AM1  AM2  AM3  AM4  AM5  HTWinStreak3  HTWinStreak5  HTLossStreak3  \\\n",
       "0    2    2    2    2    2    2             0             0              0   \n",
       "1    2    2    2    2    2    2             0             0              0   \n",
       "2    2    2    2    2    2    2             0             0              0   \n",
       "3    2    2    2    2    2    2             0             0              0   \n",
       "4    2    2    2    2    2    2             0             0              0   \n",
       "\n",
       "   HTLossStreak5  ATWinStreak3  ATWinStreak5  ATLossStreak3  ATLossStreak5  \\\n",
       "0              0             0             0              0              0   \n",
       "1              0             0             0              0              0   \n",
       "2              0             0             0              0              0   \n",
       "3              0             0             0              0              0   \n",
       "4              0             0             0              0              0   \n",
       "\n",
       "   HTGD  ATGD  DiffPts  DiffFormPts  DiffLP  \n",
       "0   0.0   0.0      0.0          0.0       3  \n",
       "1   0.0   0.0      0.0          0.0      -9  \n",
       "2   0.0   0.0      0.0          0.0     -13  \n",
       "3   0.0   0.0      0.0          0.0       4  \n",
       "4   0.0   0.0      0.0          0.0       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv(\"datasets/epl_data_train_onehot_matchid.csv\")\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B365H</th>\n",
       "      <th>B365D</th>\n",
       "      <th>B365A</th>\n",
       "      <th>FTR</th>\n",
       "      <th>HTGS</th>\n",
       "      <th>ATGS</th>\n",
       "      <th>HTGC</th>\n",
       "      <th>ATGC</th>\n",
       "      <th>HTP</th>\n",
       "      <th>ATP</th>\n",
       "      <th>HM1</th>\n",
       "      <th>HM2</th>\n",
       "      <th>HM3</th>\n",
       "      <th>HM4</th>\n",
       "      <th>HM5</th>\n",
       "      <th>AM1</th>\n",
       "      <th>AM2</th>\n",
       "      <th>AM3</th>\n",
       "      <th>AM4</th>\n",
       "      <th>AM5</th>\n",
       "      <th>HTWinStreak3</th>\n",
       "      <th>HTWinStreak5</th>\n",
       "      <th>HTLossStreak3</th>\n",
       "      <th>HTLossStreak5</th>\n",
       "      <th>ATWinStreak3</th>\n",
       "      <th>ATWinStreak5</th>\n",
       "      <th>ATLossStreak3</th>\n",
       "      <th>ATLossStreak5</th>\n",
       "      <th>HTGD</th>\n",
       "      <th>ATGD</th>\n",
       "      <th>DiffPts</th>\n",
       "      <th>DiffFormPts</th>\n",
       "      <th>DiffLP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2650</th>\n",
       "      <td>1.17</td>\n",
       "      <td>9.00</td>\n",
       "      <td>17.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.911392</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>-1.184211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2651</th>\n",
       "      <td>2.30</td>\n",
       "      <td>3.75</td>\n",
       "      <td>3.10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.697917</td>\n",
       "      <td>0.626506</td>\n",
       "      <td>0.443038</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>-0.052632</td>\n",
       "      <td>-0.157895</td>\n",
       "      <td>-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2652</th>\n",
       "      <td>1.80</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.50</td>\n",
       "      <td>2</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>0.810127</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>-0.657895</td>\n",
       "      <td>-0.078947</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2653</th>\n",
       "      <td>4.50</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.80</td>\n",
       "      <td>2</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.771084</td>\n",
       "      <td>0.379747</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.657895</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2654</th>\n",
       "      <td>1.36</td>\n",
       "      <td>5.50</td>\n",
       "      <td>9.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.395833</td>\n",
       "      <td>0.481928</td>\n",
       "      <td>0.594937</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.394737</td>\n",
       "      <td>-0.236842</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2655</th>\n",
       "      <td>3.50</td>\n",
       "      <td>3.60</td>\n",
       "      <td>2.15</td>\n",
       "      <td>2</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.650602</td>\n",
       "      <td>0.620253</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.394737</td>\n",
       "      <td>0.394737</td>\n",
       "      <td>-0.078947</td>\n",
       "      <td>-0.236842</td>\n",
       "      <td>-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2656</th>\n",
       "      <td>6.00</td>\n",
       "      <td>4.75</td>\n",
       "      <td>1.53</td>\n",
       "      <td>1</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>0.614458</td>\n",
       "      <td>0.506329</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.263158</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2657</th>\n",
       "      <td>2.05</td>\n",
       "      <td>3.75</td>\n",
       "      <td>3.70</td>\n",
       "      <td>1</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>0.578313</td>\n",
       "      <td>0.759494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.263158</td>\n",
       "      <td>-0.368421</td>\n",
       "      <td>-0.078947</td>\n",
       "      <td>-0.131579</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2658</th>\n",
       "      <td>2.40</td>\n",
       "      <td>3.60</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.645833</td>\n",
       "      <td>0.566265</td>\n",
       "      <td>0.620253</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.368421</td>\n",
       "      <td>0.342105</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.157895</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2659</th>\n",
       "      <td>1.67</td>\n",
       "      <td>4.20</td>\n",
       "      <td>5.25</td>\n",
       "      <td>2</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.409639</td>\n",
       "      <td>0.810127</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>-0.526316</td>\n",
       "      <td>-0.026316</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>-11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      B365H  B365D  B365A  FTR  HTGS      ATGS      HTGC      ATGC       HTP  \\\n",
       "2650   1.17   9.00  17.00    2  0.61  0.281250  0.433735  0.911392  0.026316   \n",
       "2651   2.30   3.75   3.10    1  0.58  0.697917  0.626506  0.443038  0.026316   \n",
       "2652   1.80   4.00   4.50    2  0.56  0.406250  0.662651  0.810127  0.000000   \n",
       "2653   4.50   4.00   1.80    2  0.39  0.708333  0.771084  0.379747  0.026316   \n",
       "2654   1.36   5.50   9.00    2  0.55  0.395833  0.481928  0.594937  0.078947   \n",
       "2655   3.50   3.60   2.15    2  0.39  0.666667  0.650602  0.620253  0.000000   \n",
       "2656   6.00   4.75   1.53    1  0.41  0.729167  0.614458  0.506329  0.078947   \n",
       "2657   2.05   3.75   3.70    1  0.38  0.479167  0.578313  0.759494  0.000000   \n",
       "2658   2.40   3.60   3.00    1  0.33  0.645833  0.566265  0.620253  0.026316   \n",
       "2659   1.67   4.20   5.25    2  0.46  0.458333  0.409639  0.810127  0.000000   \n",
       "\n",
       "           ATP  HM1  HM2  HM3  HM4  HM5  AM1  AM2  AM3  AM4  AM5  \\\n",
       "2650  0.026316    0    3    0    3    0    0    1    1    1    1   \n",
       "2651  0.078947    0    1    0    3    1    3    0    3    0    3   \n",
       "2652  0.078947    1    1    3    1    0    3    1    1    1    1   \n",
       "2653  0.000000    0    3    0    0    3    1    0    0    3    3   \n",
       "2654  0.078947    3    3    3    0    3    3    1    1    0    0   \n",
       "2655  0.078947    1    0    1    1    1    3    1    3    3    0   \n",
       "2656  0.026316    3    3    1    1    3    0    1    3    0    3   \n",
       "2657  0.078947    1    1    3    1    3    3    3    0    0    3   \n",
       "2658  0.026316    0    1    0    1    1    0    3    1    0    3   \n",
       "2659  0.026316    1    3    0    3    3    0    1    1    1    3   \n",
       "\n",
       "      HTWinStreak3  HTWinStreak5  HTLossStreak3  HTLossStreak5  ATWinStreak3  \\\n",
       "2650             0             0              0              0             0   \n",
       "2651             0             0              0              0             0   \n",
       "2652             0             0              0              0             0   \n",
       "2653             0             0              0              0             0   \n",
       "2654             0             0              0              0             0   \n",
       "2655             0             0              1              0             0   \n",
       "2656             0             0              0              0             0   \n",
       "2657             0             0              0              0             0   \n",
       "2658             0             0              0              0             0   \n",
       "2659             0             0              0              0             0   \n",
       "\n",
       "      ATWinStreak5  ATLossStreak3  ATLossStreak5      HTGD      ATGD  \\\n",
       "2650             0              1              0  0.657895 -1.184211   \n",
       "2651             0              0              0  0.157895  0.842105   \n",
       "2652             0              1              0  0.026316 -0.657895   \n",
       "2653             0              0              0 -0.657895  1.000000   \n",
       "2654             0              0              0  0.394737 -0.236842   \n",
       "2655             0              0              0 -0.394737  0.394737   \n",
       "2656             0              0              0 -0.263158  0.789474   \n",
       "2657             0              0              0 -0.263158 -0.368421   \n",
       "2658             0              0              0 -0.368421  0.342105   \n",
       "2659             0              0              0  0.315789 -0.526316   \n",
       "\n",
       "       DiffPts  DiffFormPts  DiffLP  \n",
       "2650  0.000000     0.210526     -11  \n",
       "2651 -0.052632    -0.157895     -15  \n",
       "2652 -0.078947     0.026316     -13  \n",
       "2653  0.026316     0.026316       4  \n",
       "2654  0.000000     0.210526      -3  \n",
       "2655 -0.078947    -0.236842      -4  \n",
       "2656  0.052632     0.026316      11  \n",
       "2657 -0.078947    -0.131579       4  \n",
       "2658  0.000000    -0.157895      15  \n",
       "2659 -0.026316     0.157895     -11  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df1.set_index('match_api_id').join(df2.set_index('match_api_id'))\n",
    "df_train = df_train.reset_index()\n",
    "df_train = df_train.drop(columns = ['match_api_id'])\n",
    "df_train.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Test (Season 2008/2009)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>match_api_id</th>\n",
       "      <th>B365H</th>\n",
       "      <th>B365D</th>\n",
       "      <th>B365A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>489043</td>\n",
       "      <td>1.20</td>\n",
       "      <td>6.5</td>\n",
       "      <td>15.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>489049</td>\n",
       "      <td>1.83</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>489047</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.3</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>489050</td>\n",
       "      <td>2.60</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>489048</td>\n",
       "      <td>3.20</td>\n",
       "      <td>3.4</td>\n",
       "      <td>2.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   match_api_id  B365H  B365D  B365A\n",
       "1        489043   1.20    6.5  15.00\n",
       "7        489049   1.83    3.5   4.50\n",
       "5        489047   2.00    3.3   4.00\n",
       "8        489050   2.60    3.2   2.80\n",
       "6        489048   3.20    3.4   2.25"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = pd.read_csv(\"datasets/EPL_sort.csv\", index_col=0)\n",
    "df3 = df3[df3['season'] == '2008/2009']\n",
    "df3 = df3[['match_api_id','B365H', 'B365D', 'B365A']]\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>match_api_id</th>\n",
       "      <th>FTR</th>\n",
       "      <th>HTGS</th>\n",
       "      <th>ATGS</th>\n",
       "      <th>HTGC</th>\n",
       "      <th>ATGC</th>\n",
       "      <th>HTP</th>\n",
       "      <th>ATP</th>\n",
       "      <th>HM1</th>\n",
       "      <th>HM2</th>\n",
       "      <th>HM3</th>\n",
       "      <th>HM4</th>\n",
       "      <th>HM5</th>\n",
       "      <th>AM1</th>\n",
       "      <th>AM2</th>\n",
       "      <th>AM3</th>\n",
       "      <th>AM4</th>\n",
       "      <th>AM5</th>\n",
       "      <th>HTWinStreak3</th>\n",
       "      <th>HTWinStreak5</th>\n",
       "      <th>HTLossStreak3</th>\n",
       "      <th>HTLossStreak5</th>\n",
       "      <th>ATWinStreak3</th>\n",
       "      <th>ATWinStreak5</th>\n",
       "      <th>ATLossStreak3</th>\n",
       "      <th>ATLossStreak5</th>\n",
       "      <th>HTGD</th>\n",
       "      <th>ATGD</th>\n",
       "      <th>DiffPts</th>\n",
       "      <th>DiffFormPts</th>\n",
       "      <th>DiffLP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>489043</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>489049</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>489047</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>489050</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>489048</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   match_api_id  FTR  HTGS  ATGS  HTGC  ATGC  HTP  ATP  HM1  HM2  HM3  HM4  \\\n",
       "0        489043    2   0.0   0.0   0.0   0.0  0.0  0.0    2    2    2    2   \n",
       "1        489049    2   0.0   0.0   0.0   0.0  0.0  0.0    2    2    2    2   \n",
       "2        489047    0   0.0   0.0   0.0   0.0  0.0  0.0    2    2    2    2   \n",
       "3        489050    2   0.0   0.0   0.0   0.0  0.0  0.0    2    2    2    2   \n",
       "4        489048    2   0.0   0.0   0.0   0.0  0.0  0.0    2    2    2    2   \n",
       "\n",
       "   HM5  AM1  AM2  AM3  AM4  AM5  HTWinStreak3  HTWinStreak5  HTLossStreak3  \\\n",
       "0    2    2    2    2    2    2             0             0              0   \n",
       "1    2    2    2    2    2    2             0             0              0   \n",
       "2    2    2    2    2    2    2             0             0              0   \n",
       "3    2    2    2    2    2    2             0             0              0   \n",
       "4    2    2    2    2    2    2             0             0              0   \n",
       "\n",
       "   HTLossStreak5  ATWinStreak3  ATWinStreak5  ATLossStreak3  ATLossStreak5  \\\n",
       "0              0             0             0              0              0   \n",
       "1              0             0             0              0              0   \n",
       "2              0             0             0              0              0   \n",
       "3              0             0             0              0              0   \n",
       "4              0             0             0              0              0   \n",
       "\n",
       "   HTGD  ATGD  DiffPts  DiffFormPts  DiffLP  \n",
       "0   0.0   0.0      0.0          0.0     -14  \n",
       "1   0.0   0.0      0.0          0.0     -11  \n",
       "2   0.0   0.0      0.0          0.0      -4  \n",
       "3   0.0   0.0      0.0          0.0       0  \n",
       "4   0.0   0.0      0.0          0.0       7  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4 = pd.read_csv(\"datasets/epl_data_test_onehot_matchid.csv\")\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B365H</th>\n",
       "      <th>B365D</th>\n",
       "      <th>B365A</th>\n",
       "      <th>FTR</th>\n",
       "      <th>HTGS</th>\n",
       "      <th>ATGS</th>\n",
       "      <th>HTGC</th>\n",
       "      <th>ATGC</th>\n",
       "      <th>HTP</th>\n",
       "      <th>ATP</th>\n",
       "      <th>HM1</th>\n",
       "      <th>HM2</th>\n",
       "      <th>HM3</th>\n",
       "      <th>HM4</th>\n",
       "      <th>HM5</th>\n",
       "      <th>AM1</th>\n",
       "      <th>AM2</th>\n",
       "      <th>AM3</th>\n",
       "      <th>AM4</th>\n",
       "      <th>AM5</th>\n",
       "      <th>HTWinStreak3</th>\n",
       "      <th>HTWinStreak5</th>\n",
       "      <th>HTLossStreak3</th>\n",
       "      <th>HTLossStreak5</th>\n",
       "      <th>ATWinStreak3</th>\n",
       "      <th>ATWinStreak5</th>\n",
       "      <th>ATLossStreak3</th>\n",
       "      <th>ATLossStreak5</th>\n",
       "      <th>HTGD</th>\n",
       "      <th>ATGD</th>\n",
       "      <th>DiffPts</th>\n",
       "      <th>DiffFormPts</th>\n",
       "      <th>DiffLP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>1.33</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.385417</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.645570</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>-0.368421</td>\n",
       "      <td>-0.052632</td>\n",
       "      <td>-0.026316</td>\n",
       "      <td>-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>2.30</td>\n",
       "      <td>3.4</td>\n",
       "      <td>3.10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.578313</td>\n",
       "      <td>0.734177</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.131579</td>\n",
       "      <td>-0.473684</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>1.83</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.50</td>\n",
       "      <td>1</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.722892</td>\n",
       "      <td>0.848101</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.526316</td>\n",
       "      <td>-0.815789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>2.10</td>\n",
       "      <td>3.3</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.552083</td>\n",
       "      <td>0.385542</td>\n",
       "      <td>0.468354</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.184211</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>3.00</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.697917</td>\n",
       "      <td>0.759036</td>\n",
       "      <td>0.303797</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.631579</td>\n",
       "      <td>1.131579</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.315789</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>1.53</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.50</td>\n",
       "      <td>2</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.313253</td>\n",
       "      <td>0.531646</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.263158</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>1.73</td>\n",
       "      <td>3.6</td>\n",
       "      <td>5.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.427083</td>\n",
       "      <td>0.602410</td>\n",
       "      <td>0.658228</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.184211</td>\n",
       "      <td>-0.289474</td>\n",
       "      <td>-0.026316</td>\n",
       "      <td>0.131579</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>4.00</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.91</td>\n",
       "      <td>0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.677083</td>\n",
       "      <td>0.614458</td>\n",
       "      <td>0.278481</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>1.131579</td>\n",
       "      <td>-0.078947</td>\n",
       "      <td>-0.236842</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>2.25</td>\n",
       "      <td>3.4</td>\n",
       "      <td>3.20</td>\n",
       "      <td>2</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>0.530120</td>\n",
       "      <td>0.696203</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.105263</td>\n",
       "      <td>-0.736842</td>\n",
       "      <td>-0.026316</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>2.10</td>\n",
       "      <td>3.4</td>\n",
       "      <td>3.50</td>\n",
       "      <td>2</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.395833</td>\n",
       "      <td>0.542169</td>\n",
       "      <td>0.708861</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.315789</td>\n",
       "      <td>-0.473684</td>\n",
       "      <td>-0.078947</td>\n",
       "      <td>-0.078947</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     B365H  B365D  B365A  FTR  HTGS      ATGS      HTGC      ATGC       HTP  \\\n",
       "370   1.33    5.0  10.00    2  0.64  0.385417  0.433735  0.645570  0.026316   \n",
       "371   2.30    3.4   3.10    2  0.53  0.416667  0.578313  0.734177  0.026316   \n",
       "372   1.83    3.5   4.50    1  0.40  0.375000  0.722892  0.848101  0.000000   \n",
       "373   2.10    3.3   3.60    0  0.39  0.552083  0.385542  0.468354  0.078947   \n",
       "374   3.00    3.5   2.30    0  0.39  0.697917  0.759036  0.303797  0.026316   \n",
       "375   1.53    4.0   6.50    2  0.74  0.458333  0.313253  0.531646  0.078947   \n",
       "376   1.73    3.6   5.00    2  0.57  0.427083  0.602410  0.658228  0.000000   \n",
       "377   4.00    3.6   1.91    0  0.32  0.677083  0.614458  0.278481  0.000000   \n",
       "378   2.25    3.4   3.20    2  0.40  0.281250  0.530120  0.696203  0.000000   \n",
       "379   2.10    3.4   3.50    2  0.33  0.395833  0.542169  0.708861  0.000000   \n",
       "\n",
       "          ATP  HM1  HM2  HM3  HM4  HM5  AM1  AM2  AM3  AM4  AM5  HTWinStreak3  \\\n",
       "370  0.078947    0    1    3    3    0    3    3    1    1    3             0   \n",
       "371  0.000000    0    1    3    0    0    1    3    1    0    1             0   \n",
       "372  0.000000    1    3    1    3    1    1    3    1    3    1             0   \n",
       "373  0.078947    3    3    1    3    0    3    0    3    1    0             0   \n",
       "374  0.026316    0    1    1    1    1    0    3    3    3    3             0   \n",
       "375  0.078947    3    3    3    3    0    3    0    3    1    3             0   \n",
       "376  0.026316    1    1    3    3    3    0    0    0    0    1             1   \n",
       "377  0.078947    1    0    1    1    3    3    3    3    3    0             0   \n",
       "378  0.026316    1    1    3    1    0    0    1    1    1    0             0   \n",
       "379  0.078947    1    1    1    0    1    3    1    1    0    1             0   \n",
       "\n",
       "     HTWinStreak5  HTLossStreak3  HTLossStreak5  ATWinStreak3  ATWinStreak5  \\\n",
       "370             0              0              0             0             0   \n",
       "371             0              0              0             0             0   \n",
       "372             0              0              0             0             0   \n",
       "373             0              0              0             0             0   \n",
       "374             0              1              0             1             0   \n",
       "375             0              0              0             0             0   \n",
       "376             0              0              0             0             0   \n",
       "377             0              0              0             0             0   \n",
       "378             0              0              0             0             0   \n",
       "379             0              0              0             0             0   \n",
       "\n",
       "     ATLossStreak3  ATLossStreak5      HTGD      ATGD   DiffPts  DiffFormPts  \\\n",
       "370              0              0  0.736842 -0.368421 -0.052632    -0.026316   \n",
       "371              0              0  0.131579 -0.473684  0.026316     0.052632   \n",
       "372              0              0 -0.526316 -0.815789  0.000000     0.000000   \n",
       "373              0              0  0.184211  0.421053  0.000000     0.052632   \n",
       "374              0              0 -0.631579  1.131579  0.000000    -0.315789   \n",
       "375              0              0  1.263158  0.052632  0.000000     0.078947   \n",
       "376              0              0  0.184211 -0.289474 -0.026316     0.131579   \n",
       "377              0              0 -0.500000  1.131579 -0.078947    -0.236842   \n",
       "378              0              0 -0.105263 -0.736842 -0.026316     0.052632   \n",
       "379              0              0 -0.315789 -0.473684 -0.078947    -0.078947   \n",
       "\n",
       "     DiffLP  \n",
       "370     -14  \n",
       "371      -2  \n",
       "372      -8  \n",
       "373      12  \n",
       "374      17  \n",
       "375      -2  \n",
       "376       7  \n",
       "377      16  \n",
       "378       3  \n",
       "379       9  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = df3.set_index('match_api_id').join(df4.set_index('match_api_id'))\n",
    "df_test = df_test.reset_index()\n",
    "df_test = df_test.drop(columns = ['match_api_id'])\n",
    "df_test.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2660, 32)\n",
      "(2660,)\n",
      "(380, 32)\n",
      "(380,)\n",
      "(2660, 3)\n",
      "(380, 3)\n"
     ]
    }
   ],
   "source": [
    "x_train = df_train.drop(columns = ['FTR'])\n",
    "y_train = df_train['FTR']\n",
    "x_test = df_test.drop(columns = ['FTR'])\n",
    "y_test = df_test['FTR']\n",
    "bet_train = df_train[['B365H', 'B365D', 'B365A']]\n",
    "bet_test = df_test[['B365H', 'B365D', 'B365A']]\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "print(bet_train.shape)\n",
    "print(bet_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = x_train.values.shape[1]\n",
    "columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First neural model start here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 32)]              0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 41)                1353      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 75)                3150      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 228       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 4,731\n",
      "Trainable params: 4,731\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Activation, Flatten, Dense, Dropout, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# 41-75-3\n",
    "# We first need to define the sequence of dependencies (the computational graph)\n",
    "# x = Input(shape=(20,32))\n",
    "x = Input(shape=(columns,))\n",
    "# h = Conv2D(32, (7, 7), strides=(2, 2))(x)\n",
    "# h = Activation('relu')(x)\n",
    "h = Flatten()(x)\n",
    "h = Dense(41)(h)\n",
    "h = Dense(75)(h)\n",
    "h = Dense(3)(h)\n",
    "# for i in range(10):\n",
    "#     h = Dense(75)(h)\n",
    "h = Dropout(0.3)(h)\n",
    "p = Activation('softmax')(h)\n",
    "\n",
    "# Now that we have defined how to find p from x, we can create a \n",
    "# model simply by saying what is input and what is output\n",
    "model = Model(inputs=x, outputs=p)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(lr=0.00001), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.5258 - accuracy: 0.3455 - val_loss: 1.3820 - val_accuracy: 0.2710\n",
      "Epoch 2/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 1.2511 - accuracy: 0.4003 - val_loss: 1.2695 - val_accuracy: 0.3458\n",
      "Epoch 3/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 1.1811 - accuracy: 0.4219 - val_loss: 1.2266 - val_accuracy: 0.3551\n",
      "Epoch 4/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 1.1395 - accuracy: 0.4266 - val_loss: 1.2011 - val_accuracy: 0.3645\n",
      "Epoch 5/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1322 - accuracy: 0.4219 - val_loss: 1.1834 - val_accuracy: 0.3832\n",
      "Epoch 6/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 1.1184 - accuracy: 0.4269 - val_loss: 1.1691 - val_accuracy: 0.3832\n",
      "Epoch 7/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0976 - accuracy: 0.4352 - val_loss: 1.1588 - val_accuracy: 0.3738\n",
      "Epoch 8/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0995 - accuracy: 0.4363 - val_loss: 1.1455 - val_accuracy: 0.4019\n",
      "Epoch 9/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0995 - accuracy: 0.4297 - val_loss: 1.1320 - val_accuracy: 0.4112\n",
      "Epoch 10/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0986 - accuracy: 0.4348 - val_loss: 1.1230 - val_accuracy: 0.4112\n",
      "Epoch 11/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0777 - accuracy: 0.4505 - val_loss: 1.1125 - val_accuracy: 0.4299\n",
      "Epoch 12/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 1.0771 - accuracy: 0.4403 - val_loss: 1.1025 - val_accuracy: 0.4486\n",
      "Epoch 13/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0604 - accuracy: 0.4485 - val_loss: 1.0957 - val_accuracy: 0.4486\n",
      "Epoch 14/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0627 - accuracy: 0.4446 - val_loss: 1.0877 - val_accuracy: 0.4486\n",
      "Epoch 15/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0691 - accuracy: 0.4375 - val_loss: 1.0804 - val_accuracy: 0.4766\n",
      "Epoch 16/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0622 - accuracy: 0.4501 - val_loss: 1.0742 - val_accuracy: 0.4766\n",
      "Epoch 17/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0633 - accuracy: 0.4524 - val_loss: 1.0688 - val_accuracy: 0.4673\n",
      "Epoch 18/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0429 - accuracy: 0.4563 - val_loss: 1.0625 - val_accuracy: 0.4953\n",
      "Epoch 19/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0474 - accuracy: 0.4634 - val_loss: 1.0565 - val_accuracy: 0.4860\n",
      "Epoch 20/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0464 - accuracy: 0.4524 - val_loss: 1.0517 - val_accuracy: 0.4766\n",
      "Epoch 21/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0430 - accuracy: 0.4575 - val_loss: 1.0470 - val_accuracy: 0.4766\n",
      "Epoch 22/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0309 - accuracy: 0.4673 - val_loss: 1.0422 - val_accuracy: 0.4860\n",
      "Epoch 23/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0340 - accuracy: 0.4669 - val_loss: 1.0386 - val_accuracy: 0.4860\n",
      "Epoch 24/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0388 - accuracy: 0.4583 - val_loss: 1.0366 - val_accuracy: 0.4860\n",
      "Epoch 25/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0305 - accuracy: 0.4732 - val_loss: 1.0322 - val_accuracy: 0.4953\n",
      "Epoch 26/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0411 - accuracy: 0.4559 - val_loss: 1.0297 - val_accuracy: 0.4953\n",
      "Epoch 27/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0368 - accuracy: 0.4614 - val_loss: 1.0259 - val_accuracy: 0.4860\n",
      "Epoch 28/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0284 - accuracy: 0.4653 - val_loss: 1.0232 - val_accuracy: 0.4953\n",
      "Epoch 29/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0313 - accuracy: 0.4540 - val_loss: 1.0209 - val_accuracy: 0.4860\n",
      "Epoch 30/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0149 - accuracy: 0.4712 - val_loss: 1.0188 - val_accuracy: 0.4860\n",
      "Epoch 31/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 1.0206 - accuracy: 0.4743 - val_loss: 1.0153 - val_accuracy: 0.4766\n",
      "Epoch 32/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 1.0215 - accuracy: 0.4610 - val_loss: 1.0152 - val_accuracy: 0.4673\n",
      "Epoch 33/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0284 - accuracy: 0.4563 - val_loss: 1.0135 - val_accuracy: 0.4766\n",
      "Epoch 34/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0200 - accuracy: 0.4653 - val_loss: 1.0116 - val_accuracy: 0.4860\n",
      "Epoch 35/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0187 - accuracy: 0.4696 - val_loss: 1.0114 - val_accuracy: 0.5140\n",
      "Epoch 36/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 1.0214 - accuracy: 0.4728 - val_loss: 1.0087 - val_accuracy: 0.4673\n",
      "Epoch 37/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0137 - accuracy: 0.4732 - val_loss: 1.0073 - val_accuracy: 0.4953\n",
      "Epoch 38/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0166 - accuracy: 0.4732 - val_loss: 1.0064 - val_accuracy: 0.4860\n",
      "Epoch 39/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0181 - accuracy: 0.4689 - val_loss: 1.0049 - val_accuracy: 0.4953\n",
      "Epoch 40/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0089 - accuracy: 0.4790 - val_loss: 1.0042 - val_accuracy: 0.4860\n",
      "Epoch 41/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0185 - accuracy: 0.4685 - val_loss: 1.0036 - val_accuracy: 0.5047\n",
      "Epoch 42/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0070 - accuracy: 0.4888 - val_loss: 1.0025 - val_accuracy: 0.4953\n",
      "Epoch 43/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0134 - accuracy: 0.4763 - val_loss: 1.0017 - val_accuracy: 0.4953\n",
      "Epoch 44/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0157 - accuracy: 0.4716 - val_loss: 1.0009 - val_accuracy: 0.4766\n",
      "Epoch 45/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0161 - accuracy: 0.4677 - val_loss: 1.0000 - val_accuracy: 0.4860\n",
      "Epoch 46/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0045 - accuracy: 0.4834 - val_loss: 0.9986 - val_accuracy: 0.4860\n",
      "Epoch 47/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0106 - accuracy: 0.4724 - val_loss: 0.9982 - val_accuracy: 0.4673\n",
      "Epoch 48/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0040 - accuracy: 0.4810 - val_loss: 0.9980 - val_accuracy: 0.4860\n",
      "Epoch 49/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0121 - accuracy: 0.4689 - val_loss: 0.9970 - val_accuracy: 0.4953\n",
      "Epoch 50/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0141 - accuracy: 0.4771 - val_loss: 0.9954 - val_accuracy: 0.4766\n",
      "Epoch 51/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0104 - accuracy: 0.4720 - val_loss: 0.9954 - val_accuracy: 0.4953\n",
      "Epoch 52/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0146 - accuracy: 0.4743 - val_loss: 0.9950 - val_accuracy: 0.4766\n",
      "Epoch 53/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9952 - accuracy: 0.4975 - val_loss: 0.9946 - val_accuracy: 0.4953\n",
      "Epoch 54/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0098 - accuracy: 0.4716 - val_loss: 0.9942 - val_accuracy: 0.5047\n",
      "Epoch 55/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0035 - accuracy: 0.4732 - val_loss: 0.9918 - val_accuracy: 0.4860\n",
      "Epoch 56/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9986 - accuracy: 0.4822 - val_loss: 0.9915 - val_accuracy: 0.4766\n",
      "Epoch 57/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9991 - accuracy: 0.4787 - val_loss: 0.9918 - val_accuracy: 0.4953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9985 - accuracy: 0.4755 - val_loss: 0.9949 - val_accuracy: 0.4766\n",
      "Epoch 59/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9989 - accuracy: 0.4775 - val_loss: 0.9928 - val_accuracy: 0.4953\n",
      "Epoch 60/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0046 - accuracy: 0.4755 - val_loss: 0.9905 - val_accuracy: 0.4860\n",
      "Epoch 61/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9948 - accuracy: 0.4822 - val_loss: 0.9924 - val_accuracy: 0.4860\n",
      "Epoch 62/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0078 - accuracy: 0.4775 - val_loss: 0.9930 - val_accuracy: 0.4860\n",
      "Epoch 63/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0106 - accuracy: 0.4693 - val_loss: 0.9927 - val_accuracy: 0.4953\n",
      "Epoch 64/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9955 - accuracy: 0.4771 - val_loss: 0.9923 - val_accuracy: 0.5047\n",
      "Epoch 65/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0090 - accuracy: 0.4794 - val_loss: 0.9898 - val_accuracy: 0.4766\n",
      "Epoch 66/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9995 - accuracy: 0.4865 - val_loss: 0.9902 - val_accuracy: 0.4860\n",
      "Epoch 67/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0048 - accuracy: 0.4845 - val_loss: 0.9896 - val_accuracy: 0.4860\n",
      "Epoch 68/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9950 - accuracy: 0.4943 - val_loss: 0.9885 - val_accuracy: 0.4673\n",
      "Epoch 69/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0056 - accuracy: 0.4767 - val_loss: 0.9915 - val_accuracy: 0.5047\n",
      "Epoch 70/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0022 - accuracy: 0.4826 - val_loss: 0.9892 - val_accuracy: 0.4860\n",
      "Epoch 71/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9987 - accuracy: 0.4818 - val_loss: 0.9891 - val_accuracy: 0.4860\n",
      "Epoch 72/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9992 - accuracy: 0.4853 - val_loss: 0.9890 - val_accuracy: 0.4953\n",
      "Epoch 73/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0042 - accuracy: 0.4845 - val_loss: 0.9887 - val_accuracy: 0.4766\n",
      "Epoch 74/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0075 - accuracy: 0.4853 - val_loss: 0.9892 - val_accuracy: 0.4953\n",
      "Epoch 75/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9975 - accuracy: 0.4849 - val_loss: 0.9880 - val_accuracy: 0.4860\n",
      "Epoch 76/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0010 - accuracy: 0.4908 - val_loss: 0.9879 - val_accuracy: 0.4766\n",
      "Epoch 77/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9910 - accuracy: 0.5104 - val_loss: 0.9874 - val_accuracy: 0.4766\n",
      "Epoch 78/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9986 - accuracy: 0.4873 - val_loss: 0.9864 - val_accuracy: 0.4766\n",
      "Epoch 79/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0053 - accuracy: 0.4783 - val_loss: 0.9872 - val_accuracy: 0.4860\n",
      "Epoch 80/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0003 - accuracy: 0.4904 - val_loss: 0.9876 - val_accuracy: 0.4860\n",
      "Epoch 81/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9998 - accuracy: 0.4834 - val_loss: 0.9870 - val_accuracy: 0.4766\n",
      "Epoch 82/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9933 - accuracy: 0.4881 - val_loss: 0.9876 - val_accuracy: 0.4766\n",
      "Epoch 83/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9962 - accuracy: 0.4947 - val_loss: 0.9877 - val_accuracy: 0.4860\n",
      "Epoch 84/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9976 - accuracy: 0.4873 - val_loss: 0.9895 - val_accuracy: 0.4766\n",
      "Epoch 85/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9980 - accuracy: 0.4814 - val_loss: 0.9877 - val_accuracy: 0.4766\n",
      "Epoch 86/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9925 - accuracy: 0.4912 - val_loss: 0.9883 - val_accuracy: 0.4766\n",
      "Epoch 87/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0007 - accuracy: 0.4877 - val_loss: 0.9881 - val_accuracy: 0.4860\n",
      "Epoch 88/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9974 - accuracy: 0.4884 - val_loss: 0.9889 - val_accuracy: 0.4766\n",
      "Epoch 89/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9947 - accuracy: 0.4873 - val_loss: 0.9878 - val_accuracy: 0.4860\n",
      "Epoch 90/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9981 - accuracy: 0.4908 - val_loss: 0.9889 - val_accuracy: 0.5047\n",
      "Epoch 91/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9983 - accuracy: 0.4912 - val_loss: 0.9879 - val_accuracy: 0.4860\n",
      "Epoch 92/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9965 - accuracy: 0.4928 - val_loss: 0.9890 - val_accuracy: 0.4766\n",
      "Epoch 93/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9919 - accuracy: 0.4920 - val_loss: 0.9903 - val_accuracy: 0.4860\n",
      "Epoch 94/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0039 - accuracy: 0.4818 - val_loss: 0.9909 - val_accuracy: 0.5047\n",
      "Epoch 95/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9941 - accuracy: 0.4978 - val_loss: 0.9896 - val_accuracy: 0.4860\n",
      "Epoch 96/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9976 - accuracy: 0.4826 - val_loss: 0.9898 - val_accuracy: 0.5047\n",
      "Epoch 97/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0034 - accuracy: 0.4802 - val_loss: 0.9886 - val_accuracy: 0.5047\n",
      "Epoch 98/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9966 - accuracy: 0.4892 - val_loss: 0.9884 - val_accuracy: 0.5047\n",
      "Epoch 99/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9971 - accuracy: 0.4892 - val_loss: 0.9903 - val_accuracy: 0.4953\n",
      "Epoch 100/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0018 - accuracy: 0.4751 - val_loss: 0.9888 - val_accuracy: 0.5047\n",
      "Epoch 101/200\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9990 - accuracy: 0.4873 - val_loss: 0.9890 - val_accuracy: 0.4860\n",
      "Epoch 102/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9959 - accuracy: 0.4881 - val_loss: 0.9887 - val_accuracy: 0.4953\n",
      "Epoch 103/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9937 - accuracy: 0.4971 - val_loss: 0.9898 - val_accuracy: 0.4953\n",
      "Epoch 104/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9901 - accuracy: 0.4943 - val_loss: 0.9879 - val_accuracy: 0.4860\n",
      "Epoch 105/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9968 - accuracy: 0.4869 - val_loss: 0.9887 - val_accuracy: 0.4953\n",
      "Epoch 106/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9989 - accuracy: 0.4967 - val_loss: 0.9883 - val_accuracy: 0.4953\n",
      "Epoch 107/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0010 - accuracy: 0.5014 - val_loss: 0.9894 - val_accuracy: 0.4953\n",
      "Epoch 108/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0019 - accuracy: 0.4806 - val_loss: 0.9888 - val_accuracy: 0.4860\n",
      "Epoch 109/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 1.0003 - accuracy: 0.4884 - val_loss: 0.9899 - val_accuracy: 0.4860\n",
      "Epoch 110/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9978 - accuracy: 0.4790 - val_loss: 0.9894 - val_accuracy: 0.4860\n",
      "Epoch 111/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 1.0003 - accuracy: 0.4790 - val_loss: 0.9883 - val_accuracy: 0.4953\n",
      "Epoch 112/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9963 - accuracy: 0.4955 - val_loss: 0.9876 - val_accuracy: 0.4953\n",
      "Epoch 113/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9929 - accuracy: 0.4959 - val_loss: 0.9870 - val_accuracy: 0.4953\n",
      "Epoch 114/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9970 - accuracy: 0.4802 - val_loss: 0.9886 - val_accuracy: 0.4860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9922 - accuracy: 0.4845 - val_loss: 0.9884 - val_accuracy: 0.4860\n",
      "Epoch 116/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9905 - accuracy: 0.4928 - val_loss: 0.9874 - val_accuracy: 0.4860\n",
      "Epoch 117/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9921 - accuracy: 0.4947 - val_loss: 0.9890 - val_accuracy: 0.5047\n",
      "Epoch 118/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9976 - accuracy: 0.4884 - val_loss: 0.9897 - val_accuracy: 0.5047\n",
      "Epoch 119/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9910 - accuracy: 0.4916 - val_loss: 0.9899 - val_accuracy: 0.5047\n",
      "Epoch 120/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9920 - accuracy: 0.4955 - val_loss: 0.9907 - val_accuracy: 0.5140\n",
      "Epoch 121/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9978 - accuracy: 0.4834 - val_loss: 0.9889 - val_accuracy: 0.5047\n",
      "Epoch 122/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9937 - accuracy: 0.4845 - val_loss: 0.9884 - val_accuracy: 0.5047\n",
      "Epoch 123/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9989 - accuracy: 0.4920 - val_loss: 0.9880 - val_accuracy: 0.5047\n",
      "Epoch 124/200\n",
      "160/160 [==============================] - 1s 3ms/step - loss: 0.9903 - accuracy: 0.4928 - val_loss: 0.9881 - val_accuracy: 0.5047\n",
      "Epoch 125/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9973 - accuracy: 0.4853 - val_loss: 0.9855 - val_accuracy: 0.4860\n",
      "Epoch 126/200\n",
      "160/160 [==============================] - 1s 3ms/step - loss: 0.9940 - accuracy: 0.4865 - val_loss: 0.9854 - val_accuracy: 0.4953\n",
      "Epoch 127/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9861 - accuracy: 0.5037 - val_loss: 0.9859 - val_accuracy: 0.4953\n",
      "Epoch 128/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9922 - accuracy: 0.5006 - val_loss: 0.9868 - val_accuracy: 0.5140\n",
      "Epoch 129/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9888 - accuracy: 0.4920 - val_loss: 0.9875 - val_accuracy: 0.4860\n",
      "Epoch 130/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9898 - accuracy: 0.4853 - val_loss: 0.9863 - val_accuracy: 0.4953\n",
      "Epoch 131/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9959 - accuracy: 0.4853 - val_loss: 0.9867 - val_accuracy: 0.5047\n",
      "Epoch 132/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9917 - accuracy: 0.4982 - val_loss: 0.9876 - val_accuracy: 0.5234\n",
      "Epoch 133/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9881 - accuracy: 0.5104 - val_loss: 0.9852 - val_accuracy: 0.4953\n",
      "Epoch 134/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9949 - accuracy: 0.4873 - val_loss: 0.9858 - val_accuracy: 0.5047\n",
      "Epoch 135/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9973 - accuracy: 0.4963 - val_loss: 0.9863 - val_accuracy: 0.5047\n",
      "Epoch 136/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0071 - accuracy: 0.4763 - val_loss: 0.9870 - val_accuracy: 0.5047\n",
      "Epoch 137/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9974 - accuracy: 0.4900 - val_loss: 0.9866 - val_accuracy: 0.5234\n",
      "Epoch 138/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9938 - accuracy: 0.4935 - val_loss: 0.9857 - val_accuracy: 0.5047\n",
      "Epoch 139/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9892 - accuracy: 0.5010 - val_loss: 0.9867 - val_accuracy: 0.5047\n",
      "Epoch 140/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9830 - accuracy: 0.4931 - val_loss: 0.9865 - val_accuracy: 0.4953\n",
      "Epoch 141/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9838 - accuracy: 0.5022 - val_loss: 0.9850 - val_accuracy: 0.4860\n",
      "Epoch 142/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9899 - accuracy: 0.4924 - val_loss: 0.9869 - val_accuracy: 0.5140\n",
      "Epoch 143/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9974 - accuracy: 0.4924 - val_loss: 0.9856 - val_accuracy: 0.5140\n",
      "Epoch 144/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9954 - accuracy: 0.4888 - val_loss: 0.9859 - val_accuracy: 0.5140\n",
      "Epoch 145/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9959 - accuracy: 0.4873 - val_loss: 0.9854 - val_accuracy: 0.5047\n",
      "Epoch 146/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9890 - accuracy: 0.4994 - val_loss: 0.9869 - val_accuracy: 0.5047\n",
      "Epoch 147/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9883 - accuracy: 0.4908 - val_loss: 0.9858 - val_accuracy: 0.5140\n",
      "Epoch 148/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9881 - accuracy: 0.5037 - val_loss: 0.9854 - val_accuracy: 0.4860\n",
      "Epoch 149/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9912 - accuracy: 0.4978 - val_loss: 0.9854 - val_accuracy: 0.4860\n",
      "Epoch 150/200\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9939 - accuracy: 0.4900 - val_loss: 0.9871 - val_accuracy: 0.4860\n",
      "Epoch 151/200\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9913 - accuracy: 0.4982 - val_loss: 0.9869 - val_accuracy: 0.5047\n",
      "Epoch 152/200\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9866 - accuracy: 0.5041 - val_loss: 0.9861 - val_accuracy: 0.5047\n",
      "Epoch 153/200\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.9890 - accuracy: 0.5033 - val_loss: 0.9870 - val_accuracy: 0.5047\n",
      "Epoch 154/200\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9956 - accuracy: 0.4861 - val_loss: 0.9870 - val_accuracy: 0.5140\n",
      "Epoch 155/200\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9884 - accuracy: 0.5002 - val_loss: 0.9865 - val_accuracy: 0.5140\n",
      "Epoch 156/200\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.9908 - accuracy: 0.4928 - val_loss: 0.9863 - val_accuracy: 0.5140\n",
      "Epoch 157/200\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9948 - accuracy: 0.4967 - val_loss: 0.9867 - val_accuracy: 0.5140\n",
      "Epoch 158/200\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9949 - accuracy: 0.4971 - val_loss: 0.9886 - val_accuracy: 0.5140\n",
      "Epoch 159/200\n",
      "160/160 [==============================] - 1s 7ms/step - loss: 0.9920 - accuracy: 0.4900 - val_loss: 0.9865 - val_accuracy: 0.5140\n",
      "Epoch 160/200\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.0023 - accuracy: 0.4767 - val_loss: 0.9859 - val_accuracy: 0.5234\n",
      "Epoch 161/200\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.9916 - accuracy: 0.4935 - val_loss: 0.9868 - val_accuracy: 0.5047\n",
      "Epoch 162/200\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9986 - accuracy: 0.4955 - val_loss: 0.9864 - val_accuracy: 0.5047\n",
      "Epoch 163/200\n",
      "160/160 [==============================] - 1s 6ms/step - loss: 0.9914 - accuracy: 0.4963 - val_loss: 0.9892 - val_accuracy: 0.5234\n",
      "Epoch 164/200\n",
      "160/160 [==============================] - 1s 3ms/step - loss: 0.9993 - accuracy: 0.4841 - val_loss: 0.9889 - val_accuracy: 0.5047\n",
      "Epoch 165/200\n",
      "160/160 [==============================] - 1s 3ms/step - loss: 0.9917 - accuracy: 0.4896 - val_loss: 0.9882 - val_accuracy: 0.5234\n",
      "Epoch 166/200\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.9883 - accuracy: 0.4982 - val_loss: 0.9881 - val_accuracy: 0.5047\n",
      "Epoch 167/200\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.9949 - accuracy: 0.5014 - val_loss: 0.9880 - val_accuracy: 0.5047\n",
      "Epoch 168/200\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.9818 - accuracy: 0.5018 - val_loss: 0.9887 - val_accuracy: 0.5047\n",
      "Epoch 169/200\n",
      "160/160 [==============================] - 1s 7ms/step - loss: 0.9944 - accuracy: 0.4967 - val_loss: 0.9873 - val_accuracy: 0.5234\n",
      "Epoch 170/200\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9930 - accuracy: 0.4963 - val_loss: 0.9882 - val_accuracy: 0.5234\n",
      "Epoch 171/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9887 - accuracy: 0.4849 - val_loss: 0.9906 - val_accuracy: 0.5140\n",
      "Epoch 172/200\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9936 - accuracy: 0.4939 - val_loss: 0.9891 - val_accuracy: 0.5234\n",
      "Epoch 173/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9930 - accuracy: 0.5037 - val_loss: 0.9878 - val_accuracy: 0.5140\n",
      "Epoch 174/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9955 - accuracy: 0.4865 - val_loss: 0.9875 - val_accuracy: 0.5234\n",
      "Epoch 175/200\n",
      "160/160 [==============================] - 1s 3ms/step - loss: 0.9933 - accuracy: 0.4924 - val_loss: 0.9866 - val_accuracy: 0.5047\n",
      "Epoch 176/200\n",
      "160/160 [==============================] - 1s 3ms/step - loss: 0.9941 - accuracy: 0.4928 - val_loss: 0.9890 - val_accuracy: 0.5140\n",
      "Epoch 177/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9955 - accuracy: 0.4943 - val_loss: 0.9882 - val_accuracy: 0.5140\n",
      "Epoch 178/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9847 - accuracy: 0.5053 - val_loss: 0.9903 - val_accuracy: 0.5047\n",
      "Epoch 179/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9862 - accuracy: 0.5022 - val_loss: 0.9871 - val_accuracy: 0.5047\n",
      "Epoch 180/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9845 - accuracy: 0.5057 - val_loss: 0.9872 - val_accuracy: 0.5047\n",
      "Epoch 181/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9867 - accuracy: 0.5069 - val_loss: 0.9880 - val_accuracy: 0.5047\n",
      "Epoch 182/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9894 - accuracy: 0.4963 - val_loss: 0.9867 - val_accuracy: 0.4953\n",
      "Epoch 183/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9886 - accuracy: 0.4892 - val_loss: 0.9863 - val_accuracy: 0.4953\n",
      "Epoch 184/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9915 - accuracy: 0.4928 - val_loss: 0.9883 - val_accuracy: 0.5140\n",
      "Epoch 185/200\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9891 - accuracy: 0.5045 - val_loss: 0.9880 - val_accuracy: 0.5140\n",
      "Epoch 186/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9917 - accuracy: 0.4967 - val_loss: 0.9874 - val_accuracy: 0.5140\n",
      "Epoch 187/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9899 - accuracy: 0.4857 - val_loss: 0.9870 - val_accuracy: 0.5140\n",
      "Epoch 188/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9845 - accuracy: 0.4982 - val_loss: 0.9887 - val_accuracy: 0.5047\n",
      "Epoch 189/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9850 - accuracy: 0.5076 - val_loss: 0.9865 - val_accuracy: 0.4953\n",
      "Epoch 190/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9900 - accuracy: 0.4943 - val_loss: 0.9872 - val_accuracy: 0.5047\n",
      "Epoch 191/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9805 - accuracy: 0.5022 - val_loss: 0.9877 - val_accuracy: 0.5047\n",
      "Epoch 192/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9829 - accuracy: 0.5053 - val_loss: 0.9887 - val_accuracy: 0.5234\n",
      "Epoch 193/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9937 - accuracy: 0.5025 - val_loss: 0.9888 - val_accuracy: 0.5234\n",
      "Epoch 194/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9832 - accuracy: 0.4994 - val_loss: 0.9887 - val_accuracy: 0.5140\n",
      "Epoch 195/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9815 - accuracy: 0.5041 - val_loss: 0.9879 - val_accuracy: 0.5140\n",
      "Epoch 196/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9956 - accuracy: 0.4908 - val_loss: 0.9879 - val_accuracy: 0.5327\n",
      "Epoch 197/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9856 - accuracy: 0.5006 - val_loss: 0.9881 - val_accuracy: 0.5140\n",
      "Epoch 198/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9927 - accuracy: 0.4990 - val_loss: 0.9873 - val_accuracy: 0.5047\n",
      "Epoch 199/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9890 - accuracy: 0.5002 - val_loss: 0.9880 - val_accuracy: 0.5140\n",
      "Epoch 200/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9954 - accuracy: 0.4853 - val_loss: 0.9877 - val_accuracy: 0.5140\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, batch_size=16, epochs=200, validation_split=0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'], label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.531578947368421\n"
     ]
    }
   ],
   "source": [
    "predictions = np.argmax(model.predict(x_test), axis=1)\n",
    "accuracy = (predictions == y_test.values).sum() / y_test.values.shape[0]\n",
    "print(\"Test set accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Betting time..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outcome when bet on home team\n",
    "Just a sanity check. Should not see high/any yields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agencies = ['B365', 'BW', 'IW', 'LB', 'WH', 'SJ', 'VC', 'GB', 'BS']\n",
    "agencies = ['B365']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set size (380,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Test set size\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B365H</th>\n",
       "      <th>B365D</th>\n",
       "      <th>B365A</th>\n",
       "      <th>HTGS</th>\n",
       "      <th>ATGS</th>\n",
       "      <th>HTGC</th>\n",
       "      <th>ATGC</th>\n",
       "      <th>HTP</th>\n",
       "      <th>ATP</th>\n",
       "      <th>HM1</th>\n",
       "      <th>HM2</th>\n",
       "      <th>HM3</th>\n",
       "      <th>HM4</th>\n",
       "      <th>HM5</th>\n",
       "      <th>AM1</th>\n",
       "      <th>AM2</th>\n",
       "      <th>AM3</th>\n",
       "      <th>AM4</th>\n",
       "      <th>AM5</th>\n",
       "      <th>HTWinStreak3</th>\n",
       "      <th>HTWinStreak5</th>\n",
       "      <th>HTLossStreak3</th>\n",
       "      <th>HTLossStreak5</th>\n",
       "      <th>ATWinStreak3</th>\n",
       "      <th>ATWinStreak5</th>\n",
       "      <th>ATLossStreak3</th>\n",
       "      <th>ATLossStreak5</th>\n",
       "      <th>HTGD</th>\n",
       "      <th>ATGD</th>\n",
       "      <th>DiffPts</th>\n",
       "      <th>DiffFormPts</th>\n",
       "      <th>DiffLP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.67</td>\n",
       "      <td>3.60</td>\n",
       "      <td>5.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.60</td>\n",
       "      <td>3.25</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.25</td>\n",
       "      <td>3.25</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.17</td>\n",
       "      <td>6.50</td>\n",
       "      <td>21.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.20</td>\n",
       "      <td>3.25</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2655</th>\n",
       "      <td>3.50</td>\n",
       "      <td>3.60</td>\n",
       "      <td>2.15</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.650602</td>\n",
       "      <td>0.620253</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.394737</td>\n",
       "      <td>0.394737</td>\n",
       "      <td>-0.078947</td>\n",
       "      <td>-0.236842</td>\n",
       "      <td>-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2656</th>\n",
       "      <td>6.00</td>\n",
       "      <td>4.75</td>\n",
       "      <td>1.53</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>0.614458</td>\n",
       "      <td>0.506329</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.263158</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2657</th>\n",
       "      <td>2.05</td>\n",
       "      <td>3.75</td>\n",
       "      <td>3.70</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>0.578313</td>\n",
       "      <td>0.759494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.263158</td>\n",
       "      <td>-0.368421</td>\n",
       "      <td>-0.078947</td>\n",
       "      <td>-0.131579</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2658</th>\n",
       "      <td>2.40</td>\n",
       "      <td>3.60</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.645833</td>\n",
       "      <td>0.566265</td>\n",
       "      <td>0.620253</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.368421</td>\n",
       "      <td>0.342105</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.157895</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2659</th>\n",
       "      <td>1.67</td>\n",
       "      <td>4.20</td>\n",
       "      <td>5.25</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.409639</td>\n",
       "      <td>0.810127</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>-0.526316</td>\n",
       "      <td>-0.026316</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>-11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2660 rows  32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      B365H  B365D  B365A  HTGS      ATGS      HTGC      ATGC       HTP  \\\n",
       "0      1.67   3.60   5.50  0.00  0.000000  0.000000  0.000000  0.000000   \n",
       "1      3.60   3.25   2.10  0.00  0.000000  0.000000  0.000000  0.000000   \n",
       "2      2.25   3.25   3.25  0.00  0.000000  0.000000  0.000000  0.000000   \n",
       "3      1.17   6.50  21.00  0.00  0.000000  0.000000  0.000000  0.000000   \n",
       "4      3.20   3.25   2.30  0.00  0.000000  0.000000  0.000000  0.000000   \n",
       "...     ...    ...    ...   ...       ...       ...       ...       ...   \n",
       "2655   3.50   3.60   2.15  0.39  0.666667  0.650602  0.620253  0.000000   \n",
       "2656   6.00   4.75   1.53  0.41  0.729167  0.614458  0.506329  0.078947   \n",
       "2657   2.05   3.75   3.70  0.38  0.479167  0.578313  0.759494  0.000000   \n",
       "2658   2.40   3.60   3.00  0.33  0.645833  0.566265  0.620253  0.026316   \n",
       "2659   1.67   4.20   5.25  0.46  0.458333  0.409639  0.810127  0.000000   \n",
       "\n",
       "           ATP  HM1  HM2  HM3  HM4  HM5  AM1  AM2  AM3  AM4  AM5  \\\n",
       "0     0.000000    2    2    2    2    2    2    2    2    2    2   \n",
       "1     0.000000    2    2    2    2    2    2    2    2    2    2   \n",
       "2     0.000000    2    2    2    2    2    2    2    2    2    2   \n",
       "3     0.000000    2    2    2    2    2    2    2    2    2    2   \n",
       "4     0.000000    2    2    2    2    2    2    2    2    2    2   \n",
       "...        ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "2655  0.078947    1    0    1    1    1    3    1    3    3    0   \n",
       "2656  0.026316    3    3    1    1    3    0    1    3    0    3   \n",
       "2657  0.078947    1    1    3    1    3    3    3    0    0    3   \n",
       "2658  0.026316    0    1    0    1    1    0    3    1    0    3   \n",
       "2659  0.026316    1    3    0    3    3    0    1    1    1    3   \n",
       "\n",
       "      HTWinStreak3  HTWinStreak5  HTLossStreak3  HTLossStreak5  ATWinStreak3  \\\n",
       "0                0             0              0              0             0   \n",
       "1                0             0              0              0             0   \n",
       "2                0             0              0              0             0   \n",
       "3                0             0              0              0             0   \n",
       "4                0             0              0              0             0   \n",
       "...            ...           ...            ...            ...           ...   \n",
       "2655             0             0              1              0             0   \n",
       "2656             0             0              0              0             0   \n",
       "2657             0             0              0              0             0   \n",
       "2658             0             0              0              0             0   \n",
       "2659             0             0              0              0             0   \n",
       "\n",
       "      ATWinStreak5  ATLossStreak3  ATLossStreak5      HTGD      ATGD  \\\n",
       "0                0              0              0  0.000000  0.000000   \n",
       "1                0              0              0  0.000000  0.000000   \n",
       "2                0              0              0  0.000000  0.000000   \n",
       "3                0              0              0  0.000000  0.000000   \n",
       "4                0              0              0  0.000000  0.000000   \n",
       "...            ...            ...            ...       ...       ...   \n",
       "2655             0              0              0 -0.394737  0.394737   \n",
       "2656             0              0              0 -0.263158  0.789474   \n",
       "2657             0              0              0 -0.263158 -0.368421   \n",
       "2658             0              0              0 -0.368421  0.342105   \n",
       "2659             0              0              0  0.315789 -0.526316   \n",
       "\n",
       "       DiffPts  DiffFormPts  DiffLP  \n",
       "0     0.000000     0.000000      -8  \n",
       "1     0.000000     0.000000      -2  \n",
       "2     0.000000     0.000000       1  \n",
       "3     0.000000     0.000000     -16  \n",
       "4     0.000000     0.000000       2  \n",
       "...        ...          ...     ...  \n",
       "2655 -0.078947    -0.236842      -4  \n",
       "2656  0.052632     0.026316      11  \n",
       "2657 -0.078947    -0.131579       4  \n",
       "2658  0.000000    -0.157895      15  \n",
       "2659 -0.026316     0.157895     -11  \n",
       "\n",
       "[2660 rows x 32 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bet_train\n",
    "bet_test\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Reference: https://github.com/raedovj/NN19_Project_Football/blob/master/ModelTester.py\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "def predict_always_on_one_thing_benefit(labels, betting_odds, predictable_value):\n",
    "    predictable_indices = np.zeros((labels.shape[0], 3))\n",
    "    predictable_indices[:, predictable_value] = 1\n",
    "    \n",
    "    print(predictable_value)\n",
    "    \n",
    "    agency = \"B365\"\n",
    "    odds = pd.concat([betting_odds[agency+'H'], betting_odds[agency+'D'], betting_odds[agency+'A']], axis=1)\n",
    "    # r holds betting results. 0 indicates loss, value otherwise shows the win amount\n",
    "    r = odds * predictable_indices * to_categorical(labels)\n",
    "    # Take max value of win, draw, other win. \n",
    "    r = r.values.max(axis=1)\n",
    "    # Let's say we bet 1. then our profit(or loss) would be = earnings - 1 per bet\n",
    "    r -= len(predictable_value)\n",
    "\n",
    "    print(\"Agency %s, \\twin amount: %.2f\" % (agency, r.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "Agency B365, \twin amount: 33.66\n",
      "[0]\n",
      "Agency B365, \twin amount: 50.57\n"
     ]
    }
   ],
   "source": [
    "## Profit for Away Wins Only \n",
    "predict_always_on_one_thing_benefit(y_train, bet_train, [0]) #Data Train\n",
    "predict_always_on_one_thing_benefit(y_test, bet_test, [0]) #Data Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "Agency B365, \twin amount: -96.47\n",
      "[1]\n",
      "Agency B365, \twin amount: -25.46\n"
     ]
    }
   ],
   "source": [
    "## Profit for Draw Only \n",
    "predict_always_on_one_thing_benefit(y_train, bet_train, [1]) #Data Train\n",
    "predict_always_on_one_thing_benefit(y_test, bet_test, [1]) #Data Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n",
      "Agency B365, \twin amount: 5112.06\n",
      "[2]\n",
      "Agency B365, \twin amount: 685.15\n"
     ]
    }
   ],
   "source": [
    "## Profit for Home Wins Only \n",
    "predict_always_on_one_thing_benefit(y_train, bet_train, [2]) #Data Train\n",
    "predict_always_on_one_thing_benefit(y_test, bet_test, [2]) #Data Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2]\n",
      "Agency B365, \twin amount: 5145.72\n",
      "[0, 2]\n",
      "Agency B365, \twin amount: 735.72\n"
     ]
    }
   ],
   "source": [
    "## Profit for Home or Away \n",
    "predict_always_on_one_thing_benefit(y_train, bet_train, [0, 2]) #Data Train\n",
    "predict_always_on_one_thing_benefit(y_test, bet_test, [0, 2]) #Data Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2]\n",
      "Agency B365, \twin amount: 5049.25\n",
      "[0, 1, 2]\n",
      "Agency B365, \twin amount: 710.26\n"
     ]
    }
   ],
   "source": [
    "## Profit for All Possibilities\n",
    "predict_always_on_one_thing_benefit(y_train, bet_train, [0, 1, 2]) #Data Train\n",
    "predict_always_on_one_thing_benefit(y_test, bet_test, [0, 1, 2]) #Data Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bet on every match\n",
    "Always bet on the predicted winner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions_3x1 = model.predict(x_train)\n",
    "test_predictions_3x1 = model.predict(x_test)\n",
    "train_predictions = np.argmax(train_predictions_3x1 , axis=1)\n",
    "test_predictions = np.argmax(test_predictions_3x1 , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2660, 3)\n",
      "(380, 3)\n"
     ]
    }
   ],
   "source": [
    "# train_predictions\n",
    "# test_predictions\n",
    "print(train_predictions_3x1.shape)\n",
    "print(test_predictions_3x1.shape)\n",
    "# x_train.shape\n",
    "# x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def always_bet_predicted_winner_profit(predictions, labels, betting_odds):      \n",
    "    predictions_categorical = to_categorical(predictions)\n",
    "    \n",
    "    agency = \"B365\"\n",
    "    odds = pd.concat([betting_odds[agency+'H'], betting_odds[agency+'D'], betting_odds[agency+'A']], axis=1)\n",
    "    # r holds betting results. 0 indicates loss, value otherwise shows the win amount\n",
    "    r = odds * predictions_categorical * to_categorical(labels)\n",
    "    # Take max value of win, draw, other win. \n",
    "    r = r.values.max(axis=1)\n",
    "    # Let's say we bet 1. then our profit(or loss) would be = earnings - 1\n",
    "    r -= 1\n",
    "    \n",
    "    print(\"Agency %s, \\twin amount: %.2f\" % (agency, r.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agency B365, \twin amount: 6619.85\n",
      "Agency B365, \twin amount: 925.40\n"
     ]
    }
   ],
   "source": [
    "always_bet_predicted_winner_profit(train_predictions, y_train, bet_train)\n",
    "always_bet_predicted_winner_profit(test_predictions, y_test, bet_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bet when expected return high enough\n",
    "First calculate the expected return of the team expected to win. If yield is high enough, then bet. \n",
    "* yield = prediction probability * odds. \n",
    "* Bet if yield > threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bet_predicted_winner_with_threshold_profit(predictions_3x1, predictions, labels, \n",
    "                                                          betting_odds, threshold):\n",
    "    predictions_categorical = to_categorical(predictions)\n",
    "    \n",
    "    agency = \"B365\"\n",
    "    odds = pd.concat([betting_odds[agency+'H'], betting_odds[agency+'D'], betting_odds[agency+'A']], axis=1)\n",
    "    # r holds betting results. 0 indicates loss, value otherwise shows the win amount\n",
    "    bet = odds * predictions_categorical * predictions_3x1\n",
    "    bet = bet > threshold\n",
    "    r = odds * predictions_categorical * to_categorical(labels)\n",
    "    r -= 1\n",
    "    # Set win/lose amount to 0 on matched it didn't bet\n",
    "    r[np.invert(bet)] = 0\n",
    "    # Take max value of win, draw, other win. \n",
    "    r = r.values.max(axis=1)\n",
    "\n",
    "    skip_percentage = (r==0).sum() / r.shape[0] * 100   \n",
    "    print(\"Agency %s, \\twin amount: %.2f. Didn't bet on %.2f%% of matches\" % (agency, r.sum(), skip_percentage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agency B365, \twin amount: 7755.69. Didn't bet on 49.17% of matches\n",
      "Agency B365, \twin amount: 1086.53. Didn't bet on 50.00% of matches\n"
     ]
    }
   ],
   "source": [
    "bet_predicted_winner_with_threshold_profit(train_predictions_3x1, train_predictions, y_train, bet_train, threshold=1)\n",
    "bet_predicted_winner_with_threshold_profit(test_predictions_3x1, test_predictions, y_test, bet_test, threshold=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bet on highest return\n",
    "Multiplies the neural network match predictions with betting odds and from these multiplications chooses from home win, draw, away win the highest expected return value. A threshhold can be set to choose if the yield is high enough to bet.  [prediction probability * odds > threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_highest_return(predictions_3x1, labels, betting_odds, threshold):\n",
    "        agency = \"B365\"\n",
    "        odds = pd.concat([betting_odds[agency+'H'], betting_odds[agency+'D'], betting_odds[agency+'A']], axis=1)\n",
    "        # Expected earning value. Basically expects that our NN predicts real match outcomes\n",
    "        expected = (odds * predictions_3x1).values\n",
    "\n",
    "        # Threshold matches, when we'd actually would make a bet. If expected yield is too low, it'll pass\n",
    "        bet = np.max(expected > threshold, axis=1)\n",
    "\n",
    "        # Take the highest yield of [home win, draw, other win]\n",
    "        r = np.argmax(expected, axis=1) \n",
    "\n",
    "        # Calculate wins/losses according to real match results\n",
    "        r = to_categorical(r) * to_categorical(labels)\n",
    "        r -= 1 # subtract our input bet\n",
    "\n",
    "        # Calculate earnings\n",
    "        r = r.max(axis=1) # Take max value of win, draw, other win. \n",
    "        r[np.invert(bet)] = 0 # Set win/lose amount to 0 on matched it didn't bet\n",
    "\n",
    "        skip_percentage = (bet==0).sum() / bet.shape[0] * 100   \n",
    "        print(\"Agency %s, \\twin amount: %.2f. Didn't bet on %.2f%% of matches\" % (agency, r.sum(), skip_percentage))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agency B365, \twin amount: -304.00. Didn't bet on 63.05% of matches\n",
      "Agency B365, \twin amount: -40.00. Didn't bet on 63.42% of matches\n"
     ]
    }
   ],
   "source": [
    "predict_on_highest_return(train_predictions_3x1, y_train, bet_train, threshold=2.5)\n",
    "predict_on_highest_return(test_predictions_3x1, y_test, bet_test, threshold=2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La Liga"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Train (Season 2009/2010 until Season 2015/2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>match_api_id</th>\n",
       "      <th>B365H</th>\n",
       "      <th>B365D</th>\n",
       "      <th>B365A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>684585</td>\n",
       "      <td>1.25</td>\n",
       "      <td>5.5</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>684589</td>\n",
       "      <td>1.91</td>\n",
       "      <td>3.4</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>684586</td>\n",
       "      <td>2.10</td>\n",
       "      <td>3.3</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>684588</td>\n",
       "      <td>2.88</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>684594</td>\n",
       "      <td>3.40</td>\n",
       "      <td>3.4</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     match_api_id  B365H  B365D  B365A\n",
       "380        684585   1.25    5.5   13.0\n",
       "384        684589   1.91    3.4    4.2\n",
       "381        684586   2.10    3.3    3.5\n",
       "383        684588   2.88    3.2    2.5\n",
       "389        684594   3.40    3.4    2.1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv(\"datasets/LaLiga_sort.csv\", index_col=0)\n",
    "df1 = df1[df1['season'] != '2008/2009']\n",
    "df1 = df1[['match_api_id','B365H', 'B365D', 'B365A']]\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>match_api_id</th>\n",
       "      <th>FTR</th>\n",
       "      <th>HTGS</th>\n",
       "      <th>ATGS</th>\n",
       "      <th>HTGC</th>\n",
       "      <th>ATGC</th>\n",
       "      <th>HTP</th>\n",
       "      <th>ATP</th>\n",
       "      <th>HM1</th>\n",
       "      <th>HM2</th>\n",
       "      <th>HM3</th>\n",
       "      <th>HM4</th>\n",
       "      <th>HM5</th>\n",
       "      <th>AM1</th>\n",
       "      <th>AM2</th>\n",
       "      <th>AM3</th>\n",
       "      <th>AM4</th>\n",
       "      <th>AM5</th>\n",
       "      <th>HTWinStreak3</th>\n",
       "      <th>HTWinStreak5</th>\n",
       "      <th>HTLossStreak3</th>\n",
       "      <th>HTLossStreak5</th>\n",
       "      <th>ATWinStreak3</th>\n",
       "      <th>ATWinStreak5</th>\n",
       "      <th>ATLossStreak3</th>\n",
       "      <th>ATLossStreak5</th>\n",
       "      <th>HTGD</th>\n",
       "      <th>ATGD</th>\n",
       "      <th>DiffPts</th>\n",
       "      <th>DiffFormPts</th>\n",
       "      <th>DiffLP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2030084</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2030083</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2030090</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2030087</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2030091</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   match_api_id  FTR  HTGS  ATGS  HTGC  ATGC  HTP  ATP  HM1  HM2  HM3  HM4  \\\n",
       "0       2030084    1   0.0   0.0   0.0   0.0  0.0  0.0    2    2    2    2   \n",
       "1       2030083    2   0.0   0.0   0.0   0.0  0.0  0.0    2    2    2    2   \n",
       "2       2030090    1   0.0   0.0   0.0   0.0  0.0  0.0    2    2    2    2   \n",
       "3       2030087    2   0.0   0.0   0.0   0.0  0.0  0.0    2    2    2    2   \n",
       "4       2030091    1   0.0   0.0   0.0   0.0  0.0  0.0    2    2    2    2   \n",
       "\n",
       "   HM5  AM1  AM2  AM3  AM4  AM5  HTWinStreak3  HTWinStreak5  HTLossStreak3  \\\n",
       "0    2    2    2    2    2    2             0             0              0   \n",
       "1    2    2    2    2    2    2             0             0              0   \n",
       "2    2    2    2    2    2    2             0             0              0   \n",
       "3    2    2    2    2    2    2             0             0              0   \n",
       "4    2    2    2    2    2    2             0             0              0   \n",
       "\n",
       "   HTLossStreak5  ATWinStreak3  ATWinStreak5  ATLossStreak3  ATLossStreak5  \\\n",
       "0              0             0             0              0              0   \n",
       "1              0             0             0              0              0   \n",
       "2              0             0             0              0              0   \n",
       "3              0             0             0              0              0   \n",
       "4              0             0             0              0              0   \n",
       "\n",
       "   HTGD  ATGD  DiffPts  DiffFormPts  DiffLP  \n",
       "0   0.0   0.0      0.0          0.0       4  \n",
       "1   0.0   0.0      0.0          0.0     -15  \n",
       "2   0.0   0.0      0.0          0.0       6  \n",
       "3   0.0   0.0      0.0          0.0      -5  \n",
       "4   0.0   0.0      0.0          0.0       7  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv(\"datasets/laliga_data_train_onehot_matchid.csv\")\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B365H</th>\n",
       "      <th>B365D</th>\n",
       "      <th>B365A</th>\n",
       "      <th>FTR</th>\n",
       "      <th>HTGS</th>\n",
       "      <th>ATGS</th>\n",
       "      <th>HTGC</th>\n",
       "      <th>ATGC</th>\n",
       "      <th>HTP</th>\n",
       "      <th>ATP</th>\n",
       "      <th>HM1</th>\n",
       "      <th>HM2</th>\n",
       "      <th>HM3</th>\n",
       "      <th>HM4</th>\n",
       "      <th>HM5</th>\n",
       "      <th>AM1</th>\n",
       "      <th>AM2</th>\n",
       "      <th>AM3</th>\n",
       "      <th>AM4</th>\n",
       "      <th>AM5</th>\n",
       "      <th>HTWinStreak3</th>\n",
       "      <th>HTWinStreak5</th>\n",
       "      <th>HTLossStreak3</th>\n",
       "      <th>HTLossStreak5</th>\n",
       "      <th>ATWinStreak3</th>\n",
       "      <th>ATWinStreak5</th>\n",
       "      <th>ATLossStreak3</th>\n",
       "      <th>ATLossStreak5</th>\n",
       "      <th>HTGD</th>\n",
       "      <th>ATGD</th>\n",
       "      <th>DiffPts</th>\n",
       "      <th>DiffFormPts</th>\n",
       "      <th>DiffLP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2650</th>\n",
       "      <td>1.95</td>\n",
       "      <td>3.75</td>\n",
       "      <td>3.75</td>\n",
       "      <td>0</td>\n",
       "      <td>0.393162</td>\n",
       "      <td>0.382609</td>\n",
       "      <td>0.602564</td>\n",
       "      <td>0.623377</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.026316</td>\n",
       "      <td>-0.105263</td>\n",
       "      <td>-0.078947</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2651</th>\n",
       "      <td>1.33</td>\n",
       "      <td>5.75</td>\n",
       "      <td>8.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.470085</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.564103</td>\n",
       "      <td>0.610390</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.289474</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2652</th>\n",
       "      <td>1.75</td>\n",
       "      <td>3.75</td>\n",
       "      <td>4.50</td>\n",
       "      <td>2</td>\n",
       "      <td>0.521368</td>\n",
       "      <td>0.443478</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.740260</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.131579</td>\n",
       "      <td>-0.157895</td>\n",
       "      <td>-0.078947</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2653</th>\n",
       "      <td>26.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0</td>\n",
       "      <td>0.393162</td>\n",
       "      <td>0.947826</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.376623</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.526316</td>\n",
       "      <td>2.105263</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.078947</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2654</th>\n",
       "      <td>13.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.939130</td>\n",
       "      <td>0.756410</td>\n",
       "      <td>0.441558</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.368421</td>\n",
       "      <td>1.947368</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.263158</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2655</th>\n",
       "      <td>1.80</td>\n",
       "      <td>3.75</td>\n",
       "      <td>4.50</td>\n",
       "      <td>2</td>\n",
       "      <td>0.290598</td>\n",
       "      <td>0.382609</td>\n",
       "      <td>0.435897</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.131579</td>\n",
       "      <td>-0.026316</td>\n",
       "      <td>-0.026316</td>\n",
       "      <td>-9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2656</th>\n",
       "      <td>2.00</td>\n",
       "      <td>3.60</td>\n",
       "      <td>3.70</td>\n",
       "      <td>2</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.408696</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.740260</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.947368</td>\n",
       "      <td>-0.263158</td>\n",
       "      <td>-0.026316</td>\n",
       "      <td>-0.026316</td>\n",
       "      <td>-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2657</th>\n",
       "      <td>1.33</td>\n",
       "      <td>5.25</td>\n",
       "      <td>9.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.418803</td>\n",
       "      <td>0.313043</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.870130</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.605263</td>\n",
       "      <td>-0.815789</td>\n",
       "      <td>-0.078947</td>\n",
       "      <td>-0.078947</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2658</th>\n",
       "      <td>5.00</td>\n",
       "      <td>3.80</td>\n",
       "      <td>1.70</td>\n",
       "      <td>2</td>\n",
       "      <td>0.273504</td>\n",
       "      <td>0.313043</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.844156</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.763158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.078947</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2659</th>\n",
       "      <td>1.44</td>\n",
       "      <td>4.50</td>\n",
       "      <td>7.50</td>\n",
       "      <td>2</td>\n",
       "      <td>0.324786</td>\n",
       "      <td>0.382609</td>\n",
       "      <td>0.794872</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.631579</td>\n",
       "      <td>0.289474</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      B365H  B365D  B365A  FTR      HTGS      ATGS      HTGC      ATGC  \\\n",
       "2650   1.95   3.75   3.75    0  0.393162  0.382609  0.602564  0.623377   \n",
       "2651   1.33   5.75   8.00    2  0.470085  0.434783  0.564103  0.610390   \n",
       "2652   1.75   3.75   4.50    2  0.521368  0.443478  0.230769  0.740260   \n",
       "2653  26.00  11.00   1.08    0  0.393162  0.947826  0.846154  0.376623   \n",
       "2654  13.00   8.00   1.18    0  0.384615  0.939130  0.756410  0.441558   \n",
       "2655   1.80   3.75   4.50    2  0.290598  0.382609  0.435897  0.636364   \n",
       "2656   2.00   3.60   3.70    2  0.307692  0.408696  0.923077  0.740260   \n",
       "2657   1.33   5.25   9.00    2  0.418803  0.313043  0.923077  0.870130   \n",
       "2658   5.00   3.80   1.70    2  0.273504  0.313043  0.653846  0.844156   \n",
       "2659   1.44   4.50   7.50    2  0.324786  0.382609  0.794872  0.428571   \n",
       "\n",
       "           HTP       ATP  HM1  HM2  HM3  HM4  HM5  AM1  AM2  AM3  AM4  AM5  \\\n",
       "2650  0.000000  0.078947    1    1    0    3    3    3    1    0    1    1   \n",
       "2651  0.026316  0.000000    0    3    0    1    3    1    1    3    1    0   \n",
       "2652  0.000000  0.078947    1    3    3    3    3    3    1    3    0    0   \n",
       "2653  0.078947  0.078947    3    3    1    3    1    3    3    3    3    1   \n",
       "2654  0.078947  0.078947    3    1    0    1    0    3    3    3    3    3   \n",
       "2655  0.000000  0.026316    1    3    1    0    1    0    1    3    1    0   \n",
       "2656  0.000000  0.026316    1    3    1    0    1    0    1    0    1    3   \n",
       "2657  0.000000  0.078947    1    1    1    0    3    3    1    0    1    3   \n",
       "2658  0.026316  0.026316    0    1    1    3    0    0    3    0    3    1   \n",
       "2659  0.026316  0.000000    0    3    1    3    0    1    3    0    1    1   \n",
       "\n",
       "      HTWinStreak3  HTWinStreak5  HTLossStreak3  HTLossStreak5  ATWinStreak3  \\\n",
       "2650             0             0              0              0             0   \n",
       "2651             0             0              0              0             0   \n",
       "2652             1             0              0              0             0   \n",
       "2653             0             0              0              0             0   \n",
       "2654             0             0              0              0             1   \n",
       "2655             0             0              0              0             0   \n",
       "2656             0             0              0              0             0   \n",
       "2657             0             0              0              0             0   \n",
       "2658             0             0              0              0             0   \n",
       "2659             0             0              0              0             0   \n",
       "\n",
       "      ATWinStreak5  ATLossStreak3  ATLossStreak5      HTGD      ATGD  \\\n",
       "2650             0              0              0 -0.026316 -0.105263   \n",
       "2651             0              0              0  0.289474  0.078947   \n",
       "2652             0              0              0  1.131579 -0.157895   \n",
       "2653             0              0              0 -0.526316  2.105263   \n",
       "2654             1              0              0 -0.368421  1.947368   \n",
       "2655             0              0              0  0.000000 -0.131579   \n",
       "2656             0              0              0 -0.947368 -0.263158   \n",
       "2657             0              0              0 -0.605263 -0.815789   \n",
       "2658             0              0              0 -0.500000 -0.763158   \n",
       "2659             0              0              0 -0.631579  0.289474   \n",
       "\n",
       "       DiffPts  DiffFormPts  DiffLP  \n",
       "2650 -0.078947     0.078947      -8  \n",
       "2651  0.026316     0.105263       2  \n",
       "2652 -0.078947     0.105263      -5  \n",
       "2653  0.000000    -0.078947      16  \n",
       "2654  0.000000    -0.263158      16  \n",
       "2655 -0.026316    -0.026316      -9  \n",
       "2656 -0.026316    -0.026316      -8  \n",
       "2657 -0.078947    -0.078947      -3  \n",
       "2658  0.000000    -0.078947       3  \n",
       "2659  0.026316     0.105263      12  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df1.set_index('match_api_id').join(df2.set_index('match_api_id'))\n",
    "df_train = df_train.reset_index()\n",
    "df_train = df_train.drop(columns = ['match_api_id'])\n",
    "df_train.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Test (Season 2008/2009)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>match_api_id</th>\n",
       "      <th>B365H</th>\n",
       "      <th>B365D</th>\n",
       "      <th>B365A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>530090</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.3</td>\n",
       "      <td>3.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>530023</td>\n",
       "      <td>1.70</td>\n",
       "      <td>3.6</td>\n",
       "      <td>5.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>530091</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.3</td>\n",
       "      <td>3.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>530092</td>\n",
       "      <td>1.44</td>\n",
       "      <td>4.2</td>\n",
       "      <td>7.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>530084</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.3</td>\n",
       "      <td>2.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   match_api_id  B365H  B365D  B365A\n",
       "7        530090   2.00    3.3   3.80\n",
       "0        530023   1.70    3.6   5.25\n",
       "8        530091   2.00    3.3   3.80\n",
       "9        530092   1.44    4.2   7.50\n",
       "1        530084   2.80    3.3   2.50"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = pd.read_csv(\"datasets/LaLiga_sort.csv\", index_col=0)\n",
    "df3 = df3[df3['season'] == '2008/2009']\n",
    "df3 = df3[['match_api_id','B365H', 'B365D', 'B365A']]\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>match_api_id</th>\n",
       "      <th>FTR</th>\n",
       "      <th>HTGS</th>\n",
       "      <th>ATGS</th>\n",
       "      <th>HTGC</th>\n",
       "      <th>ATGC</th>\n",
       "      <th>HTP</th>\n",
       "      <th>ATP</th>\n",
       "      <th>HM1</th>\n",
       "      <th>HM2</th>\n",
       "      <th>HM3</th>\n",
       "      <th>HM4</th>\n",
       "      <th>HM5</th>\n",
       "      <th>AM1</th>\n",
       "      <th>AM2</th>\n",
       "      <th>AM3</th>\n",
       "      <th>AM4</th>\n",
       "      <th>AM5</th>\n",
       "      <th>HTWinStreak3</th>\n",
       "      <th>HTWinStreak5</th>\n",
       "      <th>HTLossStreak3</th>\n",
       "      <th>HTLossStreak5</th>\n",
       "      <th>ATWinStreak3</th>\n",
       "      <th>ATWinStreak5</th>\n",
       "      <th>ATLossStreak3</th>\n",
       "      <th>ATLossStreak5</th>\n",
       "      <th>HTGD</th>\n",
       "      <th>ATGD</th>\n",
       "      <th>DiffPts</th>\n",
       "      <th>DiffFormPts</th>\n",
       "      <th>DiffLP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>530090</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>530023</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>530091</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>530092</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>530084</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   match_api_id  FTR  HTGS  ATGS  HTGC  ATGC  HTP  ATP  HM1  HM2  HM3  HM4  \\\n",
       "0        530090    2   0.0   0.0   0.0   0.0  0.0  0.0    2    2    2    2   \n",
       "1        530023    2   0.0   0.0   0.0   0.0  0.0  0.0    2    2    2    2   \n",
       "2        530091    0   0.0   0.0   0.0   0.0  0.0  0.0    2    2    2    2   \n",
       "3        530092    2   0.0   0.0   0.0   0.0  0.0  0.0    2    2    2    2   \n",
       "4        530084    1   0.0   0.0   0.0   0.0  0.0  0.0    2    2    2    2   \n",
       "\n",
       "   HM5  AM1  AM2  AM3  AM4  AM5  HTWinStreak3  HTWinStreak5  HTLossStreak3  \\\n",
       "0    2    2    2    2    2    2             0             0              0   \n",
       "1    2    2    2    2    2    2             0             0              0   \n",
       "2    2    2    2    2    2    2             0             0              0   \n",
       "3    2    2    2    2    2    2             0             0              0   \n",
       "4    2    2    2    2    2    2             0             0              0   \n",
       "\n",
       "   HTLossStreak5  ATWinStreak3  ATWinStreak5  ATLossStreak3  ATLossStreak5  \\\n",
       "0              0             0             0              0              0   \n",
       "1              0             0             0              0              0   \n",
       "2              0             0             0              0              0   \n",
       "3              0             0             0              0              0   \n",
       "4              0             0             0              0              0   \n",
       "\n",
       "   HTGD  ATGD  DiffPts  DiffFormPts  DiffLP  \n",
       "0   0.0   0.0      0.0          0.0      -3  \n",
       "1   0.0   0.0      0.0          0.0       3  \n",
       "2   0.0   0.0      0.0          0.0       3  \n",
       "3   0.0   0.0      0.0          0.0     -14  \n",
       "4   0.0   0.0      0.0          0.0      15  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4 = pd.read_csv(\"datasets/laliga_data_test_onehot_matchid.csv\")\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B365H</th>\n",
       "      <th>B365D</th>\n",
       "      <th>B365A</th>\n",
       "      <th>FTR</th>\n",
       "      <th>HTGS</th>\n",
       "      <th>ATGS</th>\n",
       "      <th>HTGC</th>\n",
       "      <th>ATGC</th>\n",
       "      <th>HTP</th>\n",
       "      <th>ATP</th>\n",
       "      <th>HM1</th>\n",
       "      <th>HM2</th>\n",
       "      <th>HM3</th>\n",
       "      <th>HM4</th>\n",
       "      <th>HM5</th>\n",
       "      <th>AM1</th>\n",
       "      <th>AM2</th>\n",
       "      <th>AM3</th>\n",
       "      <th>AM4</th>\n",
       "      <th>AM5</th>\n",
       "      <th>HTWinStreak3</th>\n",
       "      <th>HTWinStreak5</th>\n",
       "      <th>HTLossStreak3</th>\n",
       "      <th>HTLossStreak5</th>\n",
       "      <th>ATWinStreak3</th>\n",
       "      <th>ATWinStreak5</th>\n",
       "      <th>ATLossStreak3</th>\n",
       "      <th>ATLossStreak5</th>\n",
       "      <th>HTGD</th>\n",
       "      <th>ATGD</th>\n",
       "      <th>DiffPts</th>\n",
       "      <th>DiffFormPts</th>\n",
       "      <th>DiffLP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>1.25</td>\n",
       "      <td>5.25</td>\n",
       "      <td>13.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.658120</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.753247</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>-0.342105</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>2.00</td>\n",
       "      <td>3.60</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1</td>\n",
       "      <td>0.401709</td>\n",
       "      <td>0.904348</td>\n",
       "      <td>0.589744</td>\n",
       "      <td>0.441558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>1.842105</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>3.40</td>\n",
       "      <td>3.30</td>\n",
       "      <td>2.15</td>\n",
       "      <td>0</td>\n",
       "      <td>0.435897</td>\n",
       "      <td>0.504348</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.675325</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.157895</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>1.36</td>\n",
       "      <td>4.75</td>\n",
       "      <td>8.50</td>\n",
       "      <td>2</td>\n",
       "      <td>0.564103</td>\n",
       "      <td>0.408696</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.779221</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>-0.342105</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.078947</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>1.70</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.33</td>\n",
       "      <td>2</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.713043</td>\n",
       "      <td>0.589744</td>\n",
       "      <td>0.649351</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.184211</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>4.20</td>\n",
       "      <td>3.60</td>\n",
       "      <td>1.83</td>\n",
       "      <td>0</td>\n",
       "      <td>0.324786</td>\n",
       "      <td>0.452174</td>\n",
       "      <td>0.858974</td>\n",
       "      <td>0.506494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.763158</td>\n",
       "      <td>0.342105</td>\n",
       "      <td>-0.078947</td>\n",
       "      <td>-0.078947</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>1.91</td>\n",
       "      <td>3.60</td>\n",
       "      <td>3.80</td>\n",
       "      <td>2</td>\n",
       "      <td>0.367521</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.628205</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.157895</td>\n",
       "      <td>-0.026316</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.184211</td>\n",
       "      <td>-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>3.30</td>\n",
       "      <td>3.30</td>\n",
       "      <td>2.15</td>\n",
       "      <td>1</td>\n",
       "      <td>0.410256</td>\n",
       "      <td>0.426087</td>\n",
       "      <td>0.602564</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>-0.157895</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.026316</td>\n",
       "      <td>-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>1.83</td>\n",
       "      <td>3.60</td>\n",
       "      <td>4.20</td>\n",
       "      <td>1</td>\n",
       "      <td>0.427350</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.740260</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.184211</td>\n",
       "      <td>-0.315789</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>1.33</td>\n",
       "      <td>5.00</td>\n",
       "      <td>8.50</td>\n",
       "      <td>2</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.286957</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.868421</td>\n",
       "      <td>-0.578947</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     B365H  B365D  B365A  FTR      HTGS      ATGS      HTGC      ATGC  \\\n",
       "370   1.25   5.25  13.00    2  0.658120  0.391304  0.730769  0.753247   \n",
       "371   2.00   3.60   3.40    1  0.401709  0.904348  0.589744  0.441558   \n",
       "372   3.40   3.30   2.15    0  0.435897  0.504348  0.730769  0.675325   \n",
       "373   1.36   4.75   8.50    2  0.564103  0.408696  0.692308  0.779221   \n",
       "374   1.70   4.00   4.33    2  0.333333  0.713043  0.589744  0.649351   \n",
       "375   4.20   3.60   1.83    0  0.324786  0.452174  0.858974  0.506494   \n",
       "376   1.91   3.60   3.80    2  0.367521  0.478261  0.628205  0.727273   \n",
       "377   3.30   3.30   2.15    1  0.410256  0.426087  0.602564  0.714286   \n",
       "378   1.83   3.60   4.20    1  0.427350  0.391304  0.730769  0.740260   \n",
       "379   1.33   5.00   8.50    2  0.384615  0.286957  1.000000  0.714286   \n",
       "\n",
       "          HTP       ATP  HM1  HM2  HM3  HM4  HM5  AM1  AM2  AM3  AM4  AM5  \\\n",
       "370  0.078947  0.000000    3    3    3    3    3    1    1    3    3    3   \n",
       "371  0.000000  0.000000    1    0    3    3    0    1    1    0    3    0   \n",
       "372  0.078947  0.078947    3    3    1    3    3    3    3    0    1    3   \n",
       "373  0.000000  0.000000    1    1    3    1    0    1    1    3    0    3   \n",
       "374  0.078947  0.000000    3    0    1    1    0    1    1    1    1    3   \n",
       "375  0.000000  0.078947    1    3    0    3    1    3    0    3    3    1   \n",
       "376  0.078947  0.026316    3    3    1    3    3    0    1    3    1    0   \n",
       "377  0.078947  0.078947    3    3    1    1    1    3    0    3    1    1   \n",
       "378  0.026316  0.000000    0    3    1    1    1    1    1    0    1    0   \n",
       "379  0.078947  0.000000    3    3    1    0    1    1    1    1    3    1   \n",
       "\n",
       "     HTWinStreak3  HTWinStreak5  HTLossStreak3  HTLossStreak5  ATWinStreak3  \\\n",
       "370             1             1              0              0             1   \n",
       "371             0             0              0              0             0   \n",
       "372             0             0              0              0             0   \n",
       "373             0             0              0              0             0   \n",
       "374             0             0              0              0             0   \n",
       "375             0             0              0              0             0   \n",
       "376             0             0              0              0             0   \n",
       "377             0             0              1              0             0   \n",
       "378             0             0              1              0             0   \n",
       "379             0             0              0              0             0   \n",
       "\n",
       "     ATWinStreak5  ATLossStreak3  ATLossStreak5      HTGD      ATGD   DiffPts  \\\n",
       "370             0              0              0  0.526316 -0.342105  0.078947   \n",
       "371             0              0              0  0.026316  1.842105  0.000000   \n",
       "372             0              0              0 -0.157895  0.157895  0.000000   \n",
       "373             0              0              0  0.315789 -0.342105  0.000000   \n",
       "374             0              0              0 -0.184211  0.842105  0.078947   \n",
       "375             0              0              0 -0.763158  0.342105 -0.078947   \n",
       "376             0              0              0 -0.157895 -0.026316  0.052632   \n",
       "377             0              0              0  0.026316 -0.157895  0.000000   \n",
       "378             0              0              0 -0.184211 -0.315789  0.026316   \n",
       "379             0              0              0 -0.868421 -0.578947  0.078947   \n",
       "\n",
       "     DiffFormPts  DiffLP  \n",
       "370     0.157895      -4  \n",
       "371     0.078947       6  \n",
       "372     0.052632       5  \n",
       "373    -0.078947      -1  \n",
       "374     0.052632      16  \n",
       "375    -0.078947      13  \n",
       "376     0.184211      -6  \n",
       "377    -0.026316      -8  \n",
       "378     0.052632      -2  \n",
       "379     0.105263       0  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = df3.set_index('match_api_id').join(df4.set_index('match_api_id'))\n",
    "df_test = df_test.reset_index()\n",
    "df_test = df_test.drop(columns = ['match_api_id'])\n",
    "df_test.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2660, 32)\n",
      "(2660,)\n",
      "(380, 32)\n",
      "(380,)\n",
      "(2660, 3)\n",
      "(380, 3)\n"
     ]
    }
   ],
   "source": [
    "x_train = df_train.drop(columns = ['FTR'])\n",
    "y_train = df_train['FTR']\n",
    "x_test = df_test.drop(columns = ['FTR'])\n",
    "y_test = df_test['FTR']\n",
    "bet_train = df_train[['B365H', 'B365D', 'B365A']]\n",
    "bet_test = df_test[['B365H', 'B365D', 'B365A']]\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "print(bet_train.shape)\n",
    "print(bet_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = x_train.values.shape[1]\n",
    "columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First neural model start here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 32)]              0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 41)                1353      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 75)                3150      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 228       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 4,731\n",
      "Trainable params: 4,731\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Activation, Flatten, Dense, Dropout, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# 41-75-3\n",
    "# We first need to define the sequence of dependencies (the computational graph)\n",
    "# x = Input(shape=(20,32))\n",
    "x = Input(shape=(columns,))\n",
    "# h = Conv2D(32, (7, 7), strides=(2, 2))(x)\n",
    "# h = Activation('relu')(x)\n",
    "h = Flatten()(x)\n",
    "h = Dense(41)(h)\n",
    "h = Dense(75)(h)\n",
    "h = Dense(3)(h)\n",
    "# for i in range(10):\n",
    "#     h = Dense(75)(h)\n",
    "h = Dropout(0.3)(h)\n",
    "p = Activation('softmax')(h)\n",
    "\n",
    "# Now that we have defined how to find p from x, we can create a \n",
    "# model simply by saying what is input and what is output\n",
    "model = Model(inputs=x, outputs=p)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(lr=0.00001), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 1.9718 - accuracy: 0.2996 - val_loss: 1.4509 - val_accuracy: 0.3271\n",
      "Epoch 2/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6336 - accuracy: 0.3443 - val_loss: 1.1854 - val_accuracy: 0.4206\n",
      "Epoch 3/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.4344 - accuracy: 0.3705 - val_loss: 1.0634 - val_accuracy: 0.5421\n",
      "Epoch 4/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.3323 - accuracy: 0.3901 - val_loss: 1.0201 - val_accuracy: 0.5794\n",
      "Epoch 5/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.2652 - accuracy: 0.4042 - val_loss: 1.0021 - val_accuracy: 0.5981\n",
      "Epoch 6/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.2258 - accuracy: 0.4222 - val_loss: 0.9924 - val_accuracy: 0.5514\n",
      "Epoch 7/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 1.1740 - accuracy: 0.4336 - val_loss: 0.9883 - val_accuracy: 0.5514\n",
      "Epoch 8/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 1.1557 - accuracy: 0.4395 - val_loss: 0.9790 - val_accuracy: 0.5607\n",
      "Epoch 9/200\n",
      "160/160 [==============================] - 1s 3ms/step - loss: 1.1326 - accuracy: 0.4450 - val_loss: 0.9772 - val_accuracy: 0.5607\n",
      "Epoch 10/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1193 - accuracy: 0.4434 - val_loss: 0.9737 - val_accuracy: 0.5701\n",
      "Epoch 11/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1086 - accuracy: 0.4544 - val_loss: 0.9706 - val_accuracy: 0.5607\n",
      "Epoch 12/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1097 - accuracy: 0.4477 - val_loss: 0.9692 - val_accuracy: 0.5514\n",
      "Epoch 13/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 1.0929 - accuracy: 0.4512 - val_loss: 0.9634 - val_accuracy: 0.5514\n",
      "Epoch 14/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0682 - accuracy: 0.4743 - val_loss: 0.9626 - val_accuracy: 0.5607\n",
      "Epoch 15/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 1.0767 - accuracy: 0.4505 - val_loss: 0.9597 - val_accuracy: 0.5607\n",
      "Epoch 16/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0578 - accuracy: 0.4775 - val_loss: 0.9536 - val_accuracy: 0.5701\n",
      "Epoch 17/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0498 - accuracy: 0.4696 - val_loss: 0.9508 - val_accuracy: 0.5607\n",
      "Epoch 18/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0599 - accuracy: 0.4685 - val_loss: 0.9489 - val_accuracy: 0.5701\n",
      "Epoch 19/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0424 - accuracy: 0.4736 - val_loss: 0.9511 - val_accuracy: 0.5701\n",
      "Epoch 20/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 1.0467 - accuracy: 0.4708 - val_loss: 0.9486 - val_accuracy: 0.5701\n",
      "Epoch 21/200\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.0553 - accuracy: 0.4798 - val_loss: 0.9473 - val_accuracy: 0.5514\n",
      "Epoch 22/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 1.0317 - accuracy: 0.4841 - val_loss: 0.9483 - val_accuracy: 0.5607\n",
      "Epoch 23/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0428 - accuracy: 0.4794 - val_loss: 0.9465 - val_accuracy: 0.5514\n",
      "Epoch 24/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 1.0230 - accuracy: 0.4830 - val_loss: 0.9464 - val_accuracy: 0.5514\n",
      "Epoch 25/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0237 - accuracy: 0.4841 - val_loss: 0.9442 - val_accuracy: 0.5607\n",
      "Epoch 26/200\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.0252 - accuracy: 0.4724 - val_loss: 0.9368 - val_accuracy: 0.5794\n",
      "Epoch 27/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 1.0132 - accuracy: 0.4975 - val_loss: 0.9368 - val_accuracy: 0.5701\n",
      "Epoch 28/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 1.0208 - accuracy: 0.4884 - val_loss: 0.9373 - val_accuracy: 0.5794\n",
      "Epoch 29/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0076 - accuracy: 0.4873 - val_loss: 0.9329 - val_accuracy: 0.5981\n",
      "Epoch 30/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0071 - accuracy: 0.4951 - val_loss: 0.9351 - val_accuracy: 0.5981\n",
      "Epoch 31/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0110 - accuracy: 0.4873 - val_loss: 0.9326 - val_accuracy: 0.6075\n",
      "Epoch 32/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0138 - accuracy: 0.4810 - val_loss: 0.9367 - val_accuracy: 0.5794\n",
      "Epoch 33/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 1.0027 - accuracy: 0.4939 - val_loss: 0.9316 - val_accuracy: 0.6075\n",
      "Epoch 34/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9985 - accuracy: 0.4896 - val_loss: 0.9378 - val_accuracy: 0.5701\n",
      "Epoch 35/200\n",
      "160/160 [==============================] - 1s 3ms/step - loss: 1.0010 - accuracy: 0.4857 - val_loss: 0.9334 - val_accuracy: 0.5981\n",
      "Epoch 36/200\n",
      "160/160 [==============================] - 1s 3ms/step - loss: 1.0032 - accuracy: 0.4814 - val_loss: 0.9365 - val_accuracy: 0.5888\n",
      "Epoch 37/200\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.0073 - accuracy: 0.4853 - val_loss: 0.9363 - val_accuracy: 0.5888\n",
      "Epoch 38/200\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.0016 - accuracy: 0.4920 - val_loss: 0.9348 - val_accuracy: 0.5981\n",
      "Epoch 39/200\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.0052 - accuracy: 0.4775 - val_loss: 0.9400 - val_accuracy: 0.5888\n",
      "Epoch 40/200\n",
      "160/160 [==============================] - 1s 3ms/step - loss: 0.9960 - accuracy: 0.4865 - val_loss: 0.9370 - val_accuracy: 0.5888\n",
      "Epoch 41/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9847 - accuracy: 0.4916 - val_loss: 0.9359 - val_accuracy: 0.6075\n",
      "Epoch 42/200\n",
      "160/160 [==============================] - 1s 3ms/step - loss: 0.9900 - accuracy: 0.4990 - val_loss: 0.9374 - val_accuracy: 0.5981\n",
      "Epoch 43/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9891 - accuracy: 0.4982 - val_loss: 0.9340 - val_accuracy: 0.5981\n",
      "Epoch 44/200\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.9840 - accuracy: 0.5014 - val_loss: 0.9350 - val_accuracy: 0.6168\n",
      "Epoch 45/200\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9847 - accuracy: 0.4920 - val_loss: 0.9381 - val_accuracy: 0.6168\n",
      "Epoch 46/200\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 1.0007 - accuracy: 0.4826 - val_loss: 0.9412 - val_accuracy: 0.5981\n",
      "Epoch 47/200\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9790 - accuracy: 0.4928 - val_loss: 0.9359 - val_accuracy: 0.6262\n",
      "Epoch 48/200\n",
      "160/160 [==============================] - 1s 3ms/step - loss: 0.9846 - accuracy: 0.4935 - val_loss: 0.9354 - val_accuracy: 0.6262\n",
      "Epoch 49/200\n",
      "160/160 [==============================] - 1s 3ms/step - loss: 0.9857 - accuracy: 0.4990 - val_loss: 0.9394 - val_accuracy: 0.6075\n",
      "Epoch 50/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9827 - accuracy: 0.4951 - val_loss: 0.9363 - val_accuracy: 0.6262\n",
      "Epoch 51/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9823 - accuracy: 0.5018 - val_loss: 0.9360 - val_accuracy: 0.6262\n",
      "Epoch 52/200\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9744 - accuracy: 0.5002 - val_loss: 0.9383 - val_accuracy: 0.6262\n",
      "Epoch 53/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9708 - accuracy: 0.4998 - val_loss: 0.9358 - val_accuracy: 0.6168\n",
      "Epoch 54/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9804 - accuracy: 0.5018 - val_loss: 0.9381 - val_accuracy: 0.6168\n",
      "Epoch 55/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9668 - accuracy: 0.5029 - val_loss: 0.9392 - val_accuracy: 0.6168\n",
      "Epoch 56/200\n",
      "160/160 [==============================] - 1s 3ms/step - loss: 0.9821 - accuracy: 0.4869 - val_loss: 0.9355 - val_accuracy: 0.6168\n",
      "Epoch 57/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9753 - accuracy: 0.5053 - val_loss: 0.9388 - val_accuracy: 0.6168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9709 - accuracy: 0.5025 - val_loss: 0.9391 - val_accuracy: 0.6168\n",
      "Epoch 59/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9755 - accuracy: 0.5112 - val_loss: 0.9431 - val_accuracy: 0.6168\n",
      "Epoch 60/200\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9746 - accuracy: 0.4955 - val_loss: 0.9396 - val_accuracy: 0.6168\n",
      "Epoch 61/200\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9706 - accuracy: 0.5096 - val_loss: 0.9400 - val_accuracy: 0.6168\n",
      "Epoch 62/200\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9764 - accuracy: 0.4990 - val_loss: 0.9454 - val_accuracy: 0.6168\n",
      "Epoch 63/200\n",
      "160/160 [==============================] - 1s 3ms/step - loss: 0.9748 - accuracy: 0.4978 - val_loss: 0.9369 - val_accuracy: 0.5981\n",
      "Epoch 64/200\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9776 - accuracy: 0.5018 - val_loss: 0.9413 - val_accuracy: 0.6168\n",
      "Epoch 65/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9787 - accuracy: 0.4986 - val_loss: 0.9422 - val_accuracy: 0.6168\n",
      "Epoch 66/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9728 - accuracy: 0.4982 - val_loss: 0.9451 - val_accuracy: 0.6168\n",
      "Epoch 67/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9659 - accuracy: 0.5065 - val_loss: 0.9436 - val_accuracy: 0.6168\n",
      "Epoch 68/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9732 - accuracy: 0.5041 - val_loss: 0.9421 - val_accuracy: 0.5981\n",
      "Epoch 69/200\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9735 - accuracy: 0.5006 - val_loss: 0.9399 - val_accuracy: 0.5888\n",
      "Epoch 70/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9634 - accuracy: 0.5221 - val_loss: 0.9441 - val_accuracy: 0.6075\n",
      "Epoch 71/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9729 - accuracy: 0.5104 - val_loss: 0.9427 - val_accuracy: 0.5888\n",
      "Epoch 72/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9676 - accuracy: 0.5076 - val_loss: 0.9406 - val_accuracy: 0.5888\n",
      "Epoch 73/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9815 - accuracy: 0.4967 - val_loss: 0.9418 - val_accuracy: 0.5981\n",
      "Epoch 74/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9634 - accuracy: 0.5037 - val_loss: 0.9396 - val_accuracy: 0.5888\n",
      "Epoch 75/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9720 - accuracy: 0.5108 - val_loss: 0.9426 - val_accuracy: 0.5888\n",
      "Epoch 76/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9596 - accuracy: 0.5037 - val_loss: 0.9436 - val_accuracy: 0.5888\n",
      "Epoch 77/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9611 - accuracy: 0.5065 - val_loss: 0.9450 - val_accuracy: 0.5888\n",
      "Epoch 78/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9665 - accuracy: 0.5088 - val_loss: 0.9467 - val_accuracy: 0.5981\n",
      "Epoch 79/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9677 - accuracy: 0.5147 - val_loss: 0.9436 - val_accuracy: 0.5888\n",
      "Epoch 80/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9648 - accuracy: 0.5186 - val_loss: 0.9420 - val_accuracy: 0.5888\n",
      "Epoch 81/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9679 - accuracy: 0.5202 - val_loss: 0.9428 - val_accuracy: 0.5888\n",
      "Epoch 82/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9704 - accuracy: 0.5112 - val_loss: 0.9430 - val_accuracy: 0.5888\n",
      "Epoch 83/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9580 - accuracy: 0.5170 - val_loss: 0.9431 - val_accuracy: 0.5888\n",
      "Epoch 84/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9622 - accuracy: 0.5194 - val_loss: 0.9423 - val_accuracy: 0.5794\n",
      "Epoch 85/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9667 - accuracy: 0.5080 - val_loss: 0.9432 - val_accuracy: 0.5794\n",
      "Epoch 86/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9643 - accuracy: 0.5198 - val_loss: 0.9450 - val_accuracy: 0.5888\n",
      "Epoch 87/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9741 - accuracy: 0.4959 - val_loss: 0.9438 - val_accuracy: 0.5888\n",
      "Epoch 88/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9646 - accuracy: 0.5022 - val_loss: 0.9428 - val_accuracy: 0.5701\n",
      "Epoch 89/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9681 - accuracy: 0.5135 - val_loss: 0.9483 - val_accuracy: 0.5794\n",
      "Epoch 90/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9601 - accuracy: 0.5135 - val_loss: 0.9451 - val_accuracy: 0.5794\n",
      "Epoch 91/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9644 - accuracy: 0.5025 - val_loss: 0.9476 - val_accuracy: 0.5888\n",
      "Epoch 92/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9738 - accuracy: 0.5025 - val_loss: 0.9485 - val_accuracy: 0.5794\n",
      "Epoch 93/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9680 - accuracy: 0.5092 - val_loss: 0.9478 - val_accuracy: 0.5794\n",
      "Epoch 94/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9598 - accuracy: 0.5225 - val_loss: 0.9448 - val_accuracy: 0.5794\n",
      "Epoch 95/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9632 - accuracy: 0.5108 - val_loss: 0.9481 - val_accuracy: 0.5701\n",
      "Epoch 96/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9742 - accuracy: 0.5033 - val_loss: 0.9467 - val_accuracy: 0.5794\n",
      "Epoch 97/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9687 - accuracy: 0.5272 - val_loss: 0.9454 - val_accuracy: 0.5794\n",
      "Epoch 98/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9565 - accuracy: 0.5198 - val_loss: 0.9447 - val_accuracy: 0.5701\n",
      "Epoch 99/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9586 - accuracy: 0.5237 - val_loss: 0.9441 - val_accuracy: 0.5888\n",
      "Epoch 100/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9639 - accuracy: 0.5151 - val_loss: 0.9439 - val_accuracy: 0.5888\n",
      "Epoch 101/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9593 - accuracy: 0.5182 - val_loss: 0.9462 - val_accuracy: 0.5794\n",
      "Epoch 102/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9643 - accuracy: 0.5100 - val_loss: 0.9525 - val_accuracy: 0.5607\n",
      "Epoch 103/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9627 - accuracy: 0.5174 - val_loss: 0.9471 - val_accuracy: 0.5701\n",
      "Epoch 104/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9564 - accuracy: 0.5119 - val_loss: 0.9449 - val_accuracy: 0.5888\n",
      "Epoch 105/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9641 - accuracy: 0.5076 - val_loss: 0.9493 - val_accuracy: 0.5607\n",
      "Epoch 106/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9599 - accuracy: 0.5253 - val_loss: 0.9449 - val_accuracy: 0.5888\n",
      "Epoch 107/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9624 - accuracy: 0.5245 - val_loss: 0.9471 - val_accuracy: 0.5701\n",
      "Epoch 108/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9635 - accuracy: 0.5174 - val_loss: 0.9480 - val_accuracy: 0.5701\n",
      "Epoch 109/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9660 - accuracy: 0.5151 - val_loss: 0.9472 - val_accuracy: 0.5794\n",
      "Epoch 110/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9612 - accuracy: 0.5135 - val_loss: 0.9483 - val_accuracy: 0.5794\n",
      "Epoch 111/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9612 - accuracy: 0.5127 - val_loss: 0.9436 - val_accuracy: 0.5794\n",
      "Epoch 112/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9557 - accuracy: 0.5245 - val_loss: 0.9461 - val_accuracy: 0.5794\n",
      "Epoch 113/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9577 - accuracy: 0.5253 - val_loss: 0.9467 - val_accuracy: 0.5888\n",
      "Epoch 114/200\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9589 - accuracy: 0.5288 - val_loss: 0.9447 - val_accuracy: 0.5794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9571 - accuracy: 0.5288 - val_loss: 0.9465 - val_accuracy: 0.5794\n",
      "Epoch 116/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9590 - accuracy: 0.5257 - val_loss: 0.9469 - val_accuracy: 0.5701\n",
      "Epoch 117/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9649 - accuracy: 0.5088 - val_loss: 0.9504 - val_accuracy: 0.5701\n",
      "Epoch 118/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9572 - accuracy: 0.5170 - val_loss: 0.9485 - val_accuracy: 0.5701\n",
      "Epoch 119/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9574 - accuracy: 0.5237 - val_loss: 0.9477 - val_accuracy: 0.5794\n",
      "Epoch 120/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9630 - accuracy: 0.5202 - val_loss: 0.9485 - val_accuracy: 0.5794\n",
      "Epoch 121/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9641 - accuracy: 0.5163 - val_loss: 0.9481 - val_accuracy: 0.5794\n",
      "Epoch 122/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9668 - accuracy: 0.5123 - val_loss: 0.9461 - val_accuracy: 0.5794\n",
      "Epoch 123/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9605 - accuracy: 0.5096 - val_loss: 0.9464 - val_accuracy: 0.5794\n",
      "Epoch 124/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9573 - accuracy: 0.5147 - val_loss: 0.9487 - val_accuracy: 0.5888\n",
      "Epoch 125/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9618 - accuracy: 0.5260 - val_loss: 0.9467 - val_accuracy: 0.5794\n",
      "Epoch 126/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9639 - accuracy: 0.5061 - val_loss: 0.9471 - val_accuracy: 0.5794\n",
      "Epoch 127/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9599 - accuracy: 0.5225 - val_loss: 0.9470 - val_accuracy: 0.5794\n",
      "Epoch 128/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9606 - accuracy: 0.5182 - val_loss: 0.9458 - val_accuracy: 0.5794\n",
      "Epoch 129/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9700 - accuracy: 0.5123 - val_loss: 0.9487 - val_accuracy: 0.5888\n",
      "Epoch 130/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9575 - accuracy: 0.5296 - val_loss: 0.9479 - val_accuracy: 0.5794\n",
      "Epoch 131/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9672 - accuracy: 0.5119 - val_loss: 0.9521 - val_accuracy: 0.5607\n",
      "Epoch 132/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9620 - accuracy: 0.5233 - val_loss: 0.9436 - val_accuracy: 0.5981\n",
      "Epoch 133/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9511 - accuracy: 0.5221 - val_loss: 0.9494 - val_accuracy: 0.5701\n",
      "Epoch 134/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9611 - accuracy: 0.5276 - val_loss: 0.9479 - val_accuracy: 0.5701\n",
      "Epoch 135/200\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9567 - accuracy: 0.5151 - val_loss: 0.9486 - val_accuracy: 0.5794\n",
      "Epoch 136/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9557 - accuracy: 0.5260 - val_loss: 0.9462 - val_accuracy: 0.5794\n",
      "Epoch 137/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9596 - accuracy: 0.5174 - val_loss: 0.9461 - val_accuracy: 0.5794\n",
      "Epoch 138/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9606 - accuracy: 0.5174 - val_loss: 0.9482 - val_accuracy: 0.5794\n",
      "Epoch 139/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9599 - accuracy: 0.5206 - val_loss: 0.9485 - val_accuracy: 0.5794\n",
      "Epoch 140/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9620 - accuracy: 0.5178 - val_loss: 0.9472 - val_accuracy: 0.5794\n",
      "Epoch 141/200\n",
      "160/160 [==============================] - 1s 3ms/step - loss: 0.9706 - accuracy: 0.5069 - val_loss: 0.9478 - val_accuracy: 0.5794\n",
      "Epoch 142/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9580 - accuracy: 0.5257 - val_loss: 0.9512 - val_accuracy: 0.5607\n",
      "Epoch 143/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9616 - accuracy: 0.5194 - val_loss: 0.9515 - val_accuracy: 0.5607\n",
      "Epoch 144/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9560 - accuracy: 0.5225 - val_loss: 0.9493 - val_accuracy: 0.5701\n",
      "Epoch 145/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9608 - accuracy: 0.5155 - val_loss: 0.9486 - val_accuracy: 0.5607\n",
      "Epoch 146/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9663 - accuracy: 0.5155 - val_loss: 0.9498 - val_accuracy: 0.5701\n",
      "Epoch 147/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9528 - accuracy: 0.5241 - val_loss: 0.9511 - val_accuracy: 0.5701\n",
      "Epoch 148/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9659 - accuracy: 0.5100 - val_loss: 0.9500 - val_accuracy: 0.5701\n",
      "Epoch 149/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9673 - accuracy: 0.5198 - val_loss: 0.9523 - val_accuracy: 0.5607\n",
      "Epoch 150/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9638 - accuracy: 0.5166 - val_loss: 0.9488 - val_accuracy: 0.5607\n",
      "Epoch 151/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9661 - accuracy: 0.5096 - val_loss: 0.9474 - val_accuracy: 0.5794\n",
      "Epoch 152/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9538 - accuracy: 0.5268 - val_loss: 0.9451 - val_accuracy: 0.5794\n",
      "Epoch 153/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9627 - accuracy: 0.5147 - val_loss: 0.9474 - val_accuracy: 0.5794\n",
      "Epoch 154/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9559 - accuracy: 0.5210 - val_loss: 0.9455 - val_accuracy: 0.5888\n",
      "Epoch 155/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9588 - accuracy: 0.5166 - val_loss: 0.9501 - val_accuracy: 0.5701\n",
      "Epoch 156/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9605 - accuracy: 0.5170 - val_loss: 0.9485 - val_accuracy: 0.5701\n",
      "Epoch 157/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9636 - accuracy: 0.5092 - val_loss: 0.9458 - val_accuracy: 0.5888\n",
      "Epoch 158/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9573 - accuracy: 0.5276 - val_loss: 0.9466 - val_accuracy: 0.5794\n",
      "Epoch 159/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9607 - accuracy: 0.5210 - val_loss: 0.9453 - val_accuracy: 0.5888\n",
      "Epoch 160/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9638 - accuracy: 0.5221 - val_loss: 0.9454 - val_accuracy: 0.5888\n",
      "Epoch 161/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9561 - accuracy: 0.5233 - val_loss: 0.9451 - val_accuracy: 0.5888\n",
      "Epoch 162/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9553 - accuracy: 0.5135 - val_loss: 0.9457 - val_accuracy: 0.5794\n",
      "Epoch 163/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9632 - accuracy: 0.5104 - val_loss: 0.9466 - val_accuracy: 0.5794\n",
      "Epoch 164/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9560 - accuracy: 0.5206 - val_loss: 0.9457 - val_accuracy: 0.5888\n",
      "Epoch 165/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9606 - accuracy: 0.5151 - val_loss: 0.9465 - val_accuracy: 0.5794\n",
      "Epoch 166/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9563 - accuracy: 0.5268 - val_loss: 0.9490 - val_accuracy: 0.5701\n",
      "Epoch 167/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9589 - accuracy: 0.5276 - val_loss: 0.9482 - val_accuracy: 0.5794\n",
      "Epoch 168/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9546 - accuracy: 0.5225 - val_loss: 0.9487 - val_accuracy: 0.5701\n",
      "Epoch 169/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9593 - accuracy: 0.5170 - val_loss: 0.9481 - val_accuracy: 0.5794\n",
      "Epoch 170/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9575 - accuracy: 0.5233 - val_loss: 0.9473 - val_accuracy: 0.5794\n",
      "Epoch 171/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9654 - accuracy: 0.5159 - val_loss: 0.9468 - val_accuracy: 0.5794\n",
      "Epoch 172/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9572 - accuracy: 0.5249 - val_loss: 0.9480 - val_accuracy: 0.5701\n",
      "Epoch 173/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9584 - accuracy: 0.5166 - val_loss: 0.9468 - val_accuracy: 0.5888\n",
      "Epoch 174/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9728 - accuracy: 0.5174 - val_loss: 0.9491 - val_accuracy: 0.5701\n",
      "Epoch 175/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9588 - accuracy: 0.5292 - val_loss: 0.9484 - val_accuracy: 0.5794\n",
      "Epoch 176/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9700 - accuracy: 0.5135 - val_loss: 0.9517 - val_accuracy: 0.5701\n",
      "Epoch 177/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9585 - accuracy: 0.5296 - val_loss: 0.9484 - val_accuracy: 0.5794\n",
      "Epoch 178/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9592 - accuracy: 0.5108 - val_loss: 0.9471 - val_accuracy: 0.5888\n",
      "Epoch 179/200\n",
      "160/160 [==============================] - 1s 3ms/step - loss: 0.9548 - accuracy: 0.5276 - val_loss: 0.9483 - val_accuracy: 0.5794\n",
      "Epoch 180/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9620 - accuracy: 0.5147 - val_loss: 0.9481 - val_accuracy: 0.5794\n",
      "Epoch 181/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9563 - accuracy: 0.5194 - val_loss: 0.9467 - val_accuracy: 0.5888\n",
      "Epoch 182/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9555 - accuracy: 0.5186 - val_loss: 0.9478 - val_accuracy: 0.5701\n",
      "Epoch 183/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9501 - accuracy: 0.5202 - val_loss: 0.9458 - val_accuracy: 0.5888\n",
      "Epoch 184/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9500 - accuracy: 0.5194 - val_loss: 0.9449 - val_accuracy: 0.5888\n",
      "Epoch 185/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9646 - accuracy: 0.5253 - val_loss: 0.9478 - val_accuracy: 0.5888\n",
      "Epoch 186/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9516 - accuracy: 0.5315 - val_loss: 0.9486 - val_accuracy: 0.5888\n",
      "Epoch 187/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9594 - accuracy: 0.5202 - val_loss: 0.9469 - val_accuracy: 0.5888\n",
      "Epoch 188/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9523 - accuracy: 0.5292 - val_loss: 0.9452 - val_accuracy: 0.5981\n",
      "Epoch 189/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9630 - accuracy: 0.5108 - val_loss: 0.9490 - val_accuracy: 0.5701\n",
      "Epoch 190/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9528 - accuracy: 0.5104 - val_loss: 0.9500 - val_accuracy: 0.5701\n",
      "Epoch 191/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9575 - accuracy: 0.5241 - val_loss: 0.9483 - val_accuracy: 0.5888\n",
      "Epoch 192/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9663 - accuracy: 0.5190 - val_loss: 0.9481 - val_accuracy: 0.5794\n",
      "Epoch 193/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9586 - accuracy: 0.5257 - val_loss: 0.9475 - val_accuracy: 0.5888\n",
      "Epoch 194/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9535 - accuracy: 0.5253 - val_loss: 0.9481 - val_accuracy: 0.5888\n",
      "Epoch 195/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9524 - accuracy: 0.5280 - val_loss: 0.9442 - val_accuracy: 0.5981\n",
      "Epoch 196/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9550 - accuracy: 0.5304 - val_loss: 0.9490 - val_accuracy: 0.5701\n",
      "Epoch 197/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9626 - accuracy: 0.5233 - val_loss: 0.9450 - val_accuracy: 0.5981\n",
      "Epoch 198/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9570 - accuracy: 0.5268 - val_loss: 0.9453 - val_accuracy: 0.5981\n",
      "Epoch 199/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9548 - accuracy: 0.5354 - val_loss: 0.9444 - val_accuracy: 0.5981\n",
      "Epoch 200/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9606 - accuracy: 0.5096 - val_loss: 0.9475 - val_accuracy: 0.5701\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, batch_size=16, epochs=200, validation_split=0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'], label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.5605263157894737\n"
     ]
    }
   ],
   "source": [
    "predictions = np.argmax(model.predict(x_test), axis=1)\n",
    "accuracy = (predictions == y_test.values).sum() / y_test.values.shape[0]\n",
    "print(\"Test set accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Betting Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agencies = ['B365', 'BW', 'IW', 'LB', 'WH', 'SJ', 'VC', 'GB', 'BS']\n",
    "agencies = ['B365']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set size (380,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Test set size\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1,2,np.nan]\n",
    "np.nansum(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://github.com/raedovj/NN19_Project_Football/blob/master/ModelTester.py\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "def predict_always_on_one_thing_benefit(labels, betting_odds, predictable_value):\n",
    "    predictable_indices = np.zeros((labels.shape[0], 3))\n",
    "    predictable_indices[:, predictable_value] = 1\n",
    "    \n",
    "    agency = \"B365\"\n",
    "    odds = pd.concat([betting_odds[agency+'H'], betting_odds[agency+'D'], betting_odds[agency+'A']], axis=1)\n",
    "    # r holds betting results. 0 indicates loss, value otherwise shows the win amount\n",
    "    r = odds * predictable_indices * to_categorical(labels)\n",
    "    # Take max value of win, draw, other win. \n",
    "    r = r.values.max(axis=1)\n",
    "    # Let's say we bet 1. then our profit(or loss) would be = earnings - 1 per bet\n",
    "    r -= len(predictable_value)\n",
    "    r\n",
    "\n",
    "#     print(\"Agency %s, \\twin amount: %.2f\" % (agency, r.sum()))\n",
    "    print(\"Agency %s, \\twin amount: %.2f\" % (agency, np.nansum(r)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agency B365, \twin amount: 475.51\n",
      "Agency B365, \twin amount: -37.26\n"
     ]
    }
   ],
   "source": [
    "## Profit for Away Wins Only \n",
    "predict_always_on_one_thing_benefit(y_train, bet_train, [0]) #Data Train\n",
    "predict_always_on_one_thing_benefit(y_test, bet_test, [0]) #Data Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agency B365, \twin amount: -345.36\n",
      "Agency B365, \twin amount: -92.20\n"
     ]
    }
   ],
   "source": [
    "## Profit for Draw Only \n",
    "predict_always_on_one_thing_benefit(y_train, bet_train, [1]) #Data Train\n",
    "predict_always_on_one_thing_benefit(y_test, bet_test, [1]) #Data Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agency B365, \twin amount: 6797.98\n",
      "Agency B365, \twin amount: 582.66\n"
     ]
    }
   ],
   "source": [
    "## Profit for Home Wins Only \n",
    "predict_always_on_one_thing_benefit(y_train, bet_train, [2]) #Data Train\n",
    "predict_always_on_one_thing_benefit(y_test, bet_test, [2]) #Data Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agency B365, \twin amount: 7273.49\n",
      "Agency B365, \twin amount: 545.40\n"
     ]
    }
   ],
   "source": [
    "## Profit for Home or Away \n",
    "predict_always_on_one_thing_benefit(y_train, bet_train, [0, 2]) #Data Train\n",
    "predict_always_on_one_thing_benefit(y_test, bet_test, [0, 2]) #Data Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agency B365, \twin amount: 6928.13\n",
      "Agency B365, \twin amount: 453.20\n"
     ]
    }
   ],
   "source": [
    "## Profit for All Possibilities\n",
    "predict_always_on_one_thing_benefit(y_train, bet_train, [0, 1, 2]) #Data Train\n",
    "predict_always_on_one_thing_benefit(y_test, bet_test, [0, 1, 2]) #Data Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions_3x1 = model.predict(x_train)\n",
    "test_predictions_3x1 = model.predict(x_test)\n",
    "train_predictions = np.argmax(train_predictions_3x1 , axis=1)\n",
    "test_predictions = np.argmax(test_predictions_3x1 , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2660, 3)\n",
      "(380, 3)\n"
     ]
    }
   ],
   "source": [
    "# train_predictions\n",
    "# test_predictions\n",
    "print(train_predictions_3x1.shape)\n",
    "print(test_predictions_3x1.shape)\n",
    "# x_train.shape\n",
    "# x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def always_bet_predicted_winner_profit(predictions, labels, betting_odds):      \n",
    "    predictions_categorical = to_categorical(predictions)\n",
    "    \n",
    "    agency = \"B365\"\n",
    "    odds = pd.concat([betting_odds[agency+'H'], betting_odds[agency+'D'], betting_odds[agency+'A']], axis=1)\n",
    "    # r holds betting results. 0 indicates loss, value otherwise shows the win amount\n",
    "    r = odds * predictions_categorical * to_categorical(labels)\n",
    "    # Take max value of win, draw, other win. \n",
    "    r = r.values.max(axis=1)\n",
    "    # Let's say we bet 1. then our profit(or loss) would be = earnings - 1\n",
    "    r -= 1\n",
    "    \n",
    "#     print(\"Agency %s, \\twin amount: %.2f\" % (agency, r.sum()))\n",
    "    print(\"Agency %s, \\twin amount: %.2f\" % (agency, np.nansum(r)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agency B365, \twin amount: 8844.28\n",
      "Agency B365, \twin amount: 746.61\n"
     ]
    }
   ],
   "source": [
    "always_bet_predicted_winner_profit(train_predictions, y_train, bet_train)\n",
    "always_bet_predicted_winner_profit(test_predictions, y_test, bet_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bet_predicted_winner_with_threshold_profit(predictions_3x1, predictions, labels, \n",
    "                                                          betting_odds, threshold):\n",
    "    predictions_categorical = to_categorical(predictions)\n",
    "    \n",
    "    agency = \"B365\"\n",
    "    odds = pd.concat([betting_odds[agency+'H'], betting_odds[agency+'D'], betting_odds[agency+'A']], axis=1)\n",
    "    # r holds betting results. 0 indicates loss, value otherwise shows the win amount\n",
    "    bet = odds * predictions_categorical * predictions_3x1\n",
    "    bet = bet > threshold\n",
    "    r = odds * predictions_categorical * to_categorical(labels)\n",
    "    r -= 1\n",
    "    # Set win/lose amount to 0 on matched it didn't bet\n",
    "    r[np.invert(bet)] = 0\n",
    "    # Take max value of win, draw, other win. \n",
    "    r = r.values.max(axis=1)\n",
    "\n",
    "    skip_percentage = (r==0).sum() / r.shape[0] * 100   \n",
    "    print(\"Agency %s, \\twin amount: %.2f. Didn't bet on %.2f%% of matches\" % (agency, r.sum(), skip_percentage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agency B365, \twin amount: 9928.96. Didn't bet on 45.11% of matches\n",
      "Agency B365, \twin amount: 900.90. Didn't bet on 46.05% of matches\n"
     ]
    }
   ],
   "source": [
    "bet_predicted_winner_with_threshold_profit(train_predictions_3x1, train_predictions, y_train, bet_train, threshold=1)\n",
    "bet_predicted_winner_with_threshold_profit(test_predictions_3x1, test_predictions, y_test, bet_test, threshold=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_highest_return(predictions_3x1, labels, betting_odds, threshold):\n",
    "        agency = \"B365\"\n",
    "        odds = pd.concat([betting_odds[agency+'H'], betting_odds[agency+'D'], betting_odds[agency+'A']], axis=1)\n",
    "        # Expected earning value. Basically expects that our NN predicts real match outcomes\n",
    "        expected = (odds * predictions_3x1).values\n",
    "\n",
    "        # Threshold matches, when we'd actually would make a bet. If expected yield is too low, it'll pass\n",
    "        bet = np.max(expected > threshold, axis=1)\n",
    "\n",
    "        # Take the highest yield of [home win, draw, other win]\n",
    "        r = np.argmax(expected, axis=1) \n",
    "\n",
    "        # Calculate wins/losses according to real match results\n",
    "        r = to_categorical(r) * to_categorical(labels)\n",
    "        r -= 1 # subtract our input bet\n",
    "\n",
    "        # Calculate earnings\n",
    "        r = r.max(axis=1) # Take max value of win, draw, other win. \n",
    "        r[np.invert(bet)] = 0 # Set win/lose amount to 0 on matched it didn't bet\n",
    "\n",
    "        skip_percentage = (bet==0).sum() / bet.shape[0] * 100   \n",
    "        print(\"Agency %s, \\twin amount: %.2f. Didn't bet on %.2f%% of matches\" % (agency, r.sum(), skip_percentage))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agency B365, \twin amount: -277.00. Didn't bet on 61.47% of matches\n",
      "Agency B365, \twin amount: -32.00. Didn't bet on 72.37% of matches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in greater\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "predict_on_highest_return(train_predictions_3x1, y_train, bet_train, threshold=2.5)\n",
    "predict_on_highest_return(test_predictions_3x1, y_test, bet_test, threshold=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
