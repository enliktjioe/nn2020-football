{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "# making all pandas columns visable with display command\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EPL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Train (Season 2009/2010 until Season 2010/2011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>match_api_id</th>\n",
       "      <th>B365H</th>\n",
       "      <th>B365D</th>\n",
       "      <th>B365A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>658575</td>\n",
       "      <td>1.67</td>\n",
       "      <td>3.60</td>\n",
       "      <td>5.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>658578</td>\n",
       "      <td>3.60</td>\n",
       "      <td>3.25</td>\n",
       "      <td>2.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>658579</td>\n",
       "      <td>2.25</td>\n",
       "      <td>3.25</td>\n",
       "      <td>3.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>658582</td>\n",
       "      <td>1.17</td>\n",
       "      <td>6.50</td>\n",
       "      <td>21.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>658576</td>\n",
       "      <td>3.20</td>\n",
       "      <td>3.25</td>\n",
       "      <td>2.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     match_api_id  B365H  B365D  B365A\n",
       "382        658575   1.67   3.60   5.50\n",
       "385        658578   3.60   3.25   2.10\n",
       "386        658579   2.25   3.25   3.25\n",
       "389        658582   1.17   6.50  21.00\n",
       "383        658576   3.20   3.25   2.30"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv(\"datasets/EPL_sort.csv\", index_col=0)\n",
    "df1 = df1[df1['season'] != '2008/2009']\n",
    "df1 = df1[['match_api_id','B365H', 'B365D', 'B365A']]\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>match_api_id</th>\n",
       "      <th>FTR</th>\n",
       "      <th>HTGS</th>\n",
       "      <th>ATGS</th>\n",
       "      <th>HTGC</th>\n",
       "      <th>ATGC</th>\n",
       "      <th>HTP</th>\n",
       "      <th>ATP</th>\n",
       "      <th>HM1</th>\n",
       "      <th>HM2</th>\n",
       "      <th>HM3</th>\n",
       "      <th>HM4</th>\n",
       "      <th>HM5</th>\n",
       "      <th>AM1</th>\n",
       "      <th>AM2</th>\n",
       "      <th>AM3</th>\n",
       "      <th>AM4</th>\n",
       "      <th>AM5</th>\n",
       "      <th>HTWinStreak3</th>\n",
       "      <th>HTWinStreak5</th>\n",
       "      <th>HTLossStreak3</th>\n",
       "      <th>HTLossStreak5</th>\n",
       "      <th>ATWinStreak3</th>\n",
       "      <th>ATWinStreak5</th>\n",
       "      <th>ATLossStreak3</th>\n",
       "      <th>ATLossStreak5</th>\n",
       "      <th>HTGD</th>\n",
       "      <th>ATGD</th>\n",
       "      <th>DiffPts</th>\n",
       "      <th>DiffFormPts</th>\n",
       "      <th>DiffLP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1987033</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1987034</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1987035</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1987036</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1987037</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   match_api_id  FTR  HTGS  ATGS  HTGC  ATGC  HTP  ATP  HM1  HM2  HM3  HM4  \\\n",
       "0       1987033    0   0.0   0.0   0.0   0.0  0.0  0.0    2    2    2    2   \n",
       "1       1987034    1   0.0   0.0   0.0   0.0  0.0  0.0    2    2    2    2   \n",
       "2       1987035    1   0.0   0.0   0.0   0.0  0.0  0.0    2    2    2    2   \n",
       "3       1987036    2   0.0   0.0   0.0   0.0  0.0  0.0    2    2    2    2   \n",
       "4       1987037    2   0.0   0.0   0.0   0.0  0.0  0.0    2    2    2    2   \n",
       "\n",
       "   HM5  AM1  AM2  AM3  AM4  AM5  HTWinStreak3  HTWinStreak5  HTLossStreak3  \\\n",
       "0    2    2    2    2    2    2             0             0              0   \n",
       "1    2    2    2    2    2    2             0             0              0   \n",
       "2    2    2    2    2    2    2             0             0              0   \n",
       "3    2    2    2    2    2    2             0             0              0   \n",
       "4    2    2    2    2    2    2             0             0              0   \n",
       "\n",
       "   HTLossStreak5  ATWinStreak3  ATWinStreak5  ATLossStreak3  ATLossStreak5  \\\n",
       "0              0             0             0              0              0   \n",
       "1              0             0             0              0              0   \n",
       "2              0             0             0              0              0   \n",
       "3              0             0             0              0              0   \n",
       "4              0             0             0              0              0   \n",
       "\n",
       "   HTGD  ATGD  DiffPts  DiffFormPts  DiffLP  \n",
       "0   0.0   0.0      0.0          0.0       3  \n",
       "1   0.0   0.0      0.0          0.0      -9  \n",
       "2   0.0   0.0      0.0          0.0     -13  \n",
       "3   0.0   0.0      0.0          0.0       4  \n",
       "4   0.0   0.0      0.0          0.0       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv(\"datasets/epl_data_train_onehot_matchid.csv\")\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B365H</th>\n",
       "      <th>B365D</th>\n",
       "      <th>B365A</th>\n",
       "      <th>FTR</th>\n",
       "      <th>HTGS</th>\n",
       "      <th>ATGS</th>\n",
       "      <th>HTGC</th>\n",
       "      <th>ATGC</th>\n",
       "      <th>HTP</th>\n",
       "      <th>ATP</th>\n",
       "      <th>HM1</th>\n",
       "      <th>HM2</th>\n",
       "      <th>HM3</th>\n",
       "      <th>HM4</th>\n",
       "      <th>HM5</th>\n",
       "      <th>AM1</th>\n",
       "      <th>AM2</th>\n",
       "      <th>AM3</th>\n",
       "      <th>AM4</th>\n",
       "      <th>AM5</th>\n",
       "      <th>HTWinStreak3</th>\n",
       "      <th>HTWinStreak5</th>\n",
       "      <th>HTLossStreak3</th>\n",
       "      <th>HTLossStreak5</th>\n",
       "      <th>ATWinStreak3</th>\n",
       "      <th>ATWinStreak5</th>\n",
       "      <th>ATLossStreak3</th>\n",
       "      <th>ATLossStreak5</th>\n",
       "      <th>HTGD</th>\n",
       "      <th>ATGD</th>\n",
       "      <th>DiffPts</th>\n",
       "      <th>DiffFormPts</th>\n",
       "      <th>DiffLP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2650</th>\n",
       "      <td>1.17</td>\n",
       "      <td>9.00</td>\n",
       "      <td>17.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.911392</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>-1.184211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2651</th>\n",
       "      <td>2.30</td>\n",
       "      <td>3.75</td>\n",
       "      <td>3.10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.697917</td>\n",
       "      <td>0.626506</td>\n",
       "      <td>0.443038</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>-0.052632</td>\n",
       "      <td>-0.157895</td>\n",
       "      <td>-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2652</th>\n",
       "      <td>1.80</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.50</td>\n",
       "      <td>2</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>0.810127</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>-0.657895</td>\n",
       "      <td>-0.078947</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2653</th>\n",
       "      <td>4.50</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.80</td>\n",
       "      <td>2</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.771084</td>\n",
       "      <td>0.379747</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.657895</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2654</th>\n",
       "      <td>1.36</td>\n",
       "      <td>5.50</td>\n",
       "      <td>9.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.395833</td>\n",
       "      <td>0.481928</td>\n",
       "      <td>0.594937</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.394737</td>\n",
       "      <td>-0.236842</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2655</th>\n",
       "      <td>3.50</td>\n",
       "      <td>3.60</td>\n",
       "      <td>2.15</td>\n",
       "      <td>2</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.650602</td>\n",
       "      <td>0.620253</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.394737</td>\n",
       "      <td>0.394737</td>\n",
       "      <td>-0.078947</td>\n",
       "      <td>-0.236842</td>\n",
       "      <td>-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2656</th>\n",
       "      <td>6.00</td>\n",
       "      <td>4.75</td>\n",
       "      <td>1.53</td>\n",
       "      <td>1</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>0.614458</td>\n",
       "      <td>0.506329</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.263158</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2657</th>\n",
       "      <td>2.05</td>\n",
       "      <td>3.75</td>\n",
       "      <td>3.70</td>\n",
       "      <td>1</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>0.578313</td>\n",
       "      <td>0.759494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.263158</td>\n",
       "      <td>-0.368421</td>\n",
       "      <td>-0.078947</td>\n",
       "      <td>-0.131579</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2658</th>\n",
       "      <td>2.40</td>\n",
       "      <td>3.60</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.645833</td>\n",
       "      <td>0.566265</td>\n",
       "      <td>0.620253</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.368421</td>\n",
       "      <td>0.342105</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.157895</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2659</th>\n",
       "      <td>1.67</td>\n",
       "      <td>4.20</td>\n",
       "      <td>5.25</td>\n",
       "      <td>2</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.409639</td>\n",
       "      <td>0.810127</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>-0.526316</td>\n",
       "      <td>-0.026316</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>-11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      B365H  B365D  B365A  FTR  HTGS      ATGS      HTGC      ATGC       HTP  \\\n",
       "2650   1.17   9.00  17.00    2  0.61  0.281250  0.433735  0.911392  0.026316   \n",
       "2651   2.30   3.75   3.10    1  0.58  0.697917  0.626506  0.443038  0.026316   \n",
       "2652   1.80   4.00   4.50    2  0.56  0.406250  0.662651  0.810127  0.000000   \n",
       "2653   4.50   4.00   1.80    2  0.39  0.708333  0.771084  0.379747  0.026316   \n",
       "2654   1.36   5.50   9.00    2  0.55  0.395833  0.481928  0.594937  0.078947   \n",
       "2655   3.50   3.60   2.15    2  0.39  0.666667  0.650602  0.620253  0.000000   \n",
       "2656   6.00   4.75   1.53    1  0.41  0.729167  0.614458  0.506329  0.078947   \n",
       "2657   2.05   3.75   3.70    1  0.38  0.479167  0.578313  0.759494  0.000000   \n",
       "2658   2.40   3.60   3.00    1  0.33  0.645833  0.566265  0.620253  0.026316   \n",
       "2659   1.67   4.20   5.25    2  0.46  0.458333  0.409639  0.810127  0.000000   \n",
       "\n",
       "           ATP  HM1  HM2  HM3  HM4  HM5  AM1  AM2  AM3  AM4  AM5  \\\n",
       "2650  0.026316    0    3    0    3    0    0    1    1    1    1   \n",
       "2651  0.078947    0    1    0    3    1    3    0    3    0    3   \n",
       "2652  0.078947    1    1    3    1    0    3    1    1    1    1   \n",
       "2653  0.000000    0    3    0    0    3    1    0    0    3    3   \n",
       "2654  0.078947    3    3    3    0    3    3    1    1    0    0   \n",
       "2655  0.078947    1    0    1    1    1    3    1    3    3    0   \n",
       "2656  0.026316    3    3    1    1    3    0    1    3    0    3   \n",
       "2657  0.078947    1    1    3    1    3    3    3    0    0    3   \n",
       "2658  0.026316    0    1    0    1    1    0    3    1    0    3   \n",
       "2659  0.026316    1    3    0    3    3    0    1    1    1    3   \n",
       "\n",
       "      HTWinStreak3  HTWinStreak5  HTLossStreak3  HTLossStreak5  ATWinStreak3  \\\n",
       "2650             0             0              0              0             0   \n",
       "2651             0             0              0              0             0   \n",
       "2652             0             0              0              0             0   \n",
       "2653             0             0              0              0             0   \n",
       "2654             0             0              0              0             0   \n",
       "2655             0             0              1              0             0   \n",
       "2656             0             0              0              0             0   \n",
       "2657             0             0              0              0             0   \n",
       "2658             0             0              0              0             0   \n",
       "2659             0             0              0              0             0   \n",
       "\n",
       "      ATWinStreak5  ATLossStreak3  ATLossStreak5      HTGD      ATGD  \\\n",
       "2650             0              1              0  0.657895 -1.184211   \n",
       "2651             0              0              0  0.157895  0.842105   \n",
       "2652             0              1              0  0.026316 -0.657895   \n",
       "2653             0              0              0 -0.657895  1.000000   \n",
       "2654             0              0              0  0.394737 -0.236842   \n",
       "2655             0              0              0 -0.394737  0.394737   \n",
       "2656             0              0              0 -0.263158  0.789474   \n",
       "2657             0              0              0 -0.263158 -0.368421   \n",
       "2658             0              0              0 -0.368421  0.342105   \n",
       "2659             0              0              0  0.315789 -0.526316   \n",
       "\n",
       "       DiffPts  DiffFormPts  DiffLP  \n",
       "2650  0.000000     0.210526     -11  \n",
       "2651 -0.052632    -0.157895     -15  \n",
       "2652 -0.078947     0.026316     -13  \n",
       "2653  0.026316     0.026316       4  \n",
       "2654  0.000000     0.210526      -3  \n",
       "2655 -0.078947    -0.236842      -4  \n",
       "2656  0.052632     0.026316      11  \n",
       "2657 -0.078947    -0.131579       4  \n",
       "2658  0.000000    -0.157895      15  \n",
       "2659 -0.026316     0.157895     -11  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df1.set_index('match_api_id').join(df2.set_index('match_api_id'))\n",
    "df_train = df_train.reset_index()\n",
    "df_train = df_train.drop(columns = ['match_api_id'])\n",
    "df_train.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Test (Season 2008/2009)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>match_api_id</th>\n",
       "      <th>B365H</th>\n",
       "      <th>B365D</th>\n",
       "      <th>B365A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>489043</td>\n",
       "      <td>1.20</td>\n",
       "      <td>6.5</td>\n",
       "      <td>15.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>489049</td>\n",
       "      <td>1.83</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>489047</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.3</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>489050</td>\n",
       "      <td>2.60</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>489048</td>\n",
       "      <td>3.20</td>\n",
       "      <td>3.4</td>\n",
       "      <td>2.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   match_api_id  B365H  B365D  B365A\n",
       "1        489043   1.20    6.5  15.00\n",
       "7        489049   1.83    3.5   4.50\n",
       "5        489047   2.00    3.3   4.00\n",
       "8        489050   2.60    3.2   2.80\n",
       "6        489048   3.20    3.4   2.25"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = pd.read_csv(\"datasets/EPL_sort.csv\", index_col=0)\n",
    "df3 = df3[df3['season'] == '2008/2009']\n",
    "df3 = df3[['match_api_id','B365H', 'B365D', 'B365A']]\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>match_api_id</th>\n",
       "      <th>FTR</th>\n",
       "      <th>HTGS</th>\n",
       "      <th>ATGS</th>\n",
       "      <th>HTGC</th>\n",
       "      <th>ATGC</th>\n",
       "      <th>HTP</th>\n",
       "      <th>ATP</th>\n",
       "      <th>HM1</th>\n",
       "      <th>HM2</th>\n",
       "      <th>HM3</th>\n",
       "      <th>HM4</th>\n",
       "      <th>HM5</th>\n",
       "      <th>AM1</th>\n",
       "      <th>AM2</th>\n",
       "      <th>AM3</th>\n",
       "      <th>AM4</th>\n",
       "      <th>AM5</th>\n",
       "      <th>HTWinStreak3</th>\n",
       "      <th>HTWinStreak5</th>\n",
       "      <th>HTLossStreak3</th>\n",
       "      <th>HTLossStreak5</th>\n",
       "      <th>ATWinStreak3</th>\n",
       "      <th>ATWinStreak5</th>\n",
       "      <th>ATLossStreak3</th>\n",
       "      <th>ATLossStreak5</th>\n",
       "      <th>HTGD</th>\n",
       "      <th>ATGD</th>\n",
       "      <th>DiffPts</th>\n",
       "      <th>DiffFormPts</th>\n",
       "      <th>DiffLP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>489043</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>489049</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>489047</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>489050</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>489048</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   match_api_id  FTR  HTGS  ATGS  HTGC  ATGC  HTP  ATP  HM1  HM2  HM3  HM4  \\\n",
       "0        489043    2   0.0   0.0   0.0   0.0  0.0  0.0    2    2    2    2   \n",
       "1        489049    2   0.0   0.0   0.0   0.0  0.0  0.0    2    2    2    2   \n",
       "2        489047    0   0.0   0.0   0.0   0.0  0.0  0.0    2    2    2    2   \n",
       "3        489050    2   0.0   0.0   0.0   0.0  0.0  0.0    2    2    2    2   \n",
       "4        489048    2   0.0   0.0   0.0   0.0  0.0  0.0    2    2    2    2   \n",
       "\n",
       "   HM5  AM1  AM2  AM3  AM4  AM5  HTWinStreak3  HTWinStreak5  HTLossStreak3  \\\n",
       "0    2    2    2    2    2    2             0             0              0   \n",
       "1    2    2    2    2    2    2             0             0              0   \n",
       "2    2    2    2    2    2    2             0             0              0   \n",
       "3    2    2    2    2    2    2             0             0              0   \n",
       "4    2    2    2    2    2    2             0             0              0   \n",
       "\n",
       "   HTLossStreak5  ATWinStreak3  ATWinStreak5  ATLossStreak3  ATLossStreak5  \\\n",
       "0              0             0             0              0              0   \n",
       "1              0             0             0              0              0   \n",
       "2              0             0             0              0              0   \n",
       "3              0             0             0              0              0   \n",
       "4              0             0             0              0              0   \n",
       "\n",
       "   HTGD  ATGD  DiffPts  DiffFormPts  DiffLP  \n",
       "0   0.0   0.0      0.0          0.0     -14  \n",
       "1   0.0   0.0      0.0          0.0     -11  \n",
       "2   0.0   0.0      0.0          0.0      -4  \n",
       "3   0.0   0.0      0.0          0.0       0  \n",
       "4   0.0   0.0      0.0          0.0       7  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4 = pd.read_csv(\"datasets/epl_data_test_onehot_matchid.csv\")\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B365H</th>\n",
       "      <th>B365D</th>\n",
       "      <th>B365A</th>\n",
       "      <th>FTR</th>\n",
       "      <th>HTGS</th>\n",
       "      <th>ATGS</th>\n",
       "      <th>HTGC</th>\n",
       "      <th>ATGC</th>\n",
       "      <th>HTP</th>\n",
       "      <th>ATP</th>\n",
       "      <th>HM1</th>\n",
       "      <th>HM2</th>\n",
       "      <th>HM3</th>\n",
       "      <th>HM4</th>\n",
       "      <th>HM5</th>\n",
       "      <th>AM1</th>\n",
       "      <th>AM2</th>\n",
       "      <th>AM3</th>\n",
       "      <th>AM4</th>\n",
       "      <th>AM5</th>\n",
       "      <th>HTWinStreak3</th>\n",
       "      <th>HTWinStreak5</th>\n",
       "      <th>HTLossStreak3</th>\n",
       "      <th>HTLossStreak5</th>\n",
       "      <th>ATWinStreak3</th>\n",
       "      <th>ATWinStreak5</th>\n",
       "      <th>ATLossStreak3</th>\n",
       "      <th>ATLossStreak5</th>\n",
       "      <th>HTGD</th>\n",
       "      <th>ATGD</th>\n",
       "      <th>DiffPts</th>\n",
       "      <th>DiffFormPts</th>\n",
       "      <th>DiffLP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>1.33</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.385417</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.645570</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>-0.368421</td>\n",
       "      <td>-0.052632</td>\n",
       "      <td>-0.026316</td>\n",
       "      <td>-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>2.30</td>\n",
       "      <td>3.4</td>\n",
       "      <td>3.10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.578313</td>\n",
       "      <td>0.734177</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.131579</td>\n",
       "      <td>-0.473684</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>1.83</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.50</td>\n",
       "      <td>1</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.722892</td>\n",
       "      <td>0.848101</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.526316</td>\n",
       "      <td>-0.815789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>2.10</td>\n",
       "      <td>3.3</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.552083</td>\n",
       "      <td>0.385542</td>\n",
       "      <td>0.468354</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.184211</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>3.00</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.697917</td>\n",
       "      <td>0.759036</td>\n",
       "      <td>0.303797</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.631579</td>\n",
       "      <td>1.131579</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.315789</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>1.53</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.50</td>\n",
       "      <td>2</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.313253</td>\n",
       "      <td>0.531646</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.263158</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>1.73</td>\n",
       "      <td>3.6</td>\n",
       "      <td>5.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.427083</td>\n",
       "      <td>0.602410</td>\n",
       "      <td>0.658228</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.184211</td>\n",
       "      <td>-0.289474</td>\n",
       "      <td>-0.026316</td>\n",
       "      <td>0.131579</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>4.00</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.91</td>\n",
       "      <td>0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.677083</td>\n",
       "      <td>0.614458</td>\n",
       "      <td>0.278481</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>1.131579</td>\n",
       "      <td>-0.078947</td>\n",
       "      <td>-0.236842</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>2.25</td>\n",
       "      <td>3.4</td>\n",
       "      <td>3.20</td>\n",
       "      <td>2</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>0.530120</td>\n",
       "      <td>0.696203</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.105263</td>\n",
       "      <td>-0.736842</td>\n",
       "      <td>-0.026316</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>2.10</td>\n",
       "      <td>3.4</td>\n",
       "      <td>3.50</td>\n",
       "      <td>2</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.395833</td>\n",
       "      <td>0.542169</td>\n",
       "      <td>0.708861</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.315789</td>\n",
       "      <td>-0.473684</td>\n",
       "      <td>-0.078947</td>\n",
       "      <td>-0.078947</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     B365H  B365D  B365A  FTR  HTGS      ATGS      HTGC      ATGC       HTP  \\\n",
       "370   1.33    5.0  10.00    2  0.64  0.385417  0.433735  0.645570  0.026316   \n",
       "371   2.30    3.4   3.10    2  0.53  0.416667  0.578313  0.734177  0.026316   \n",
       "372   1.83    3.5   4.50    1  0.40  0.375000  0.722892  0.848101  0.000000   \n",
       "373   2.10    3.3   3.60    0  0.39  0.552083  0.385542  0.468354  0.078947   \n",
       "374   3.00    3.5   2.30    0  0.39  0.697917  0.759036  0.303797  0.026316   \n",
       "375   1.53    4.0   6.50    2  0.74  0.458333  0.313253  0.531646  0.078947   \n",
       "376   1.73    3.6   5.00    2  0.57  0.427083  0.602410  0.658228  0.000000   \n",
       "377   4.00    3.6   1.91    0  0.32  0.677083  0.614458  0.278481  0.000000   \n",
       "378   2.25    3.4   3.20    2  0.40  0.281250  0.530120  0.696203  0.000000   \n",
       "379   2.10    3.4   3.50    2  0.33  0.395833  0.542169  0.708861  0.000000   \n",
       "\n",
       "          ATP  HM1  HM2  HM3  HM4  HM5  AM1  AM2  AM3  AM4  AM5  HTWinStreak3  \\\n",
       "370  0.078947    0    1    3    3    0    3    3    1    1    3             0   \n",
       "371  0.000000    0    1    3    0    0    1    3    1    0    1             0   \n",
       "372  0.000000    1    3    1    3    1    1    3    1    3    1             0   \n",
       "373  0.078947    3    3    1    3    0    3    0    3    1    0             0   \n",
       "374  0.026316    0    1    1    1    1    0    3    3    3    3             0   \n",
       "375  0.078947    3    3    3    3    0    3    0    3    1    3             0   \n",
       "376  0.026316    1    1    3    3    3    0    0    0    0    1             1   \n",
       "377  0.078947    1    0    1    1    3    3    3    3    3    0             0   \n",
       "378  0.026316    1    1    3    1    0    0    1    1    1    0             0   \n",
       "379  0.078947    1    1    1    0    1    3    1    1    0    1             0   \n",
       "\n",
       "     HTWinStreak5  HTLossStreak3  HTLossStreak5  ATWinStreak3  ATWinStreak5  \\\n",
       "370             0              0              0             0             0   \n",
       "371             0              0              0             0             0   \n",
       "372             0              0              0             0             0   \n",
       "373             0              0              0             0             0   \n",
       "374             0              1              0             1             0   \n",
       "375             0              0              0             0             0   \n",
       "376             0              0              0             0             0   \n",
       "377             0              0              0             0             0   \n",
       "378             0              0              0             0             0   \n",
       "379             0              0              0             0             0   \n",
       "\n",
       "     ATLossStreak3  ATLossStreak5      HTGD      ATGD   DiffPts  DiffFormPts  \\\n",
       "370              0              0  0.736842 -0.368421 -0.052632    -0.026316   \n",
       "371              0              0  0.131579 -0.473684  0.026316     0.052632   \n",
       "372              0              0 -0.526316 -0.815789  0.000000     0.000000   \n",
       "373              0              0  0.184211  0.421053  0.000000     0.052632   \n",
       "374              0              0 -0.631579  1.131579  0.000000    -0.315789   \n",
       "375              0              0  1.263158  0.052632  0.000000     0.078947   \n",
       "376              0              0  0.184211 -0.289474 -0.026316     0.131579   \n",
       "377              0              0 -0.500000  1.131579 -0.078947    -0.236842   \n",
       "378              0              0 -0.105263 -0.736842 -0.026316     0.052632   \n",
       "379              0              0 -0.315789 -0.473684 -0.078947    -0.078947   \n",
       "\n",
       "     DiffLP  \n",
       "370     -14  \n",
       "371      -2  \n",
       "372      -8  \n",
       "373      12  \n",
       "374      17  \n",
       "375      -2  \n",
       "376       7  \n",
       "377      16  \n",
       "378       3  \n",
       "379       9  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = df3.set_index('match_api_id').join(df4.set_index('match_api_id'))\n",
    "df_test = df_test.reset_index()\n",
    "df_test = df_test.drop(columns = ['match_api_id'])\n",
    "df_test.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2660, 32)\n",
      "(2660,)\n",
      "(380, 32)\n",
      "(380,)\n",
      "(2660, 3)\n",
      "(380, 3)\n"
     ]
    }
   ],
   "source": [
    "x_train = df_train.drop(columns = ['FTR'])\n",
    "y_train = df_train['FTR']\n",
    "x_test = df_test.drop(columns = ['FTR'])\n",
    "y_test = df_test['FTR']\n",
    "bet_train = df_train[['B365H', 'B365D', 'B365A']]\n",
    "bet_test = df_test[['B365H', 'B365D', 'B365A']]\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "print(bet_train.shape)\n",
    "print(bet_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = x_train.values.shape[1]\n",
    "columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First neural model start here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 32)]              0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 41)                1353      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 75)                3150      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 228       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 4,731\n",
      "Trainable params: 4,731\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Activation, Flatten, Dense, Dropout, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# 41-75-3\n",
    "# We first need to define the sequence of dependencies (the computational graph)\n",
    "# x = Input(shape=(20,32))\n",
    "x = Input(shape=(columns,))\n",
    "# h = Conv2D(32, (7, 7), strides=(2, 2))(x)\n",
    "# h = Activation('relu')(x)\n",
    "h = Flatten()(x)\n",
    "h = Dense(41)(h)\n",
    "h = Dense(75)(h)\n",
    "h = Dense(3)(h)\n",
    "# for i in range(10):\n",
    "#     h = Dense(75)(h)\n",
    "h = Dropout(0.3)(h)\n",
    "p = Activation('softmax')(h)\n",
    "\n",
    "# Now that we have defined how to find p from x, we can create a \n",
    "# model simply by saying what is input and what is output\n",
    "model = Model(inputs=x, outputs=p)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(lr=0.00001), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 2.8387 - accuracy: 0.3541 - val_loss: 2.3016 - val_accuracy: 0.3551\n",
      "Epoch 2/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.3561 - accuracy: 0.3431 - val_loss: 1.9034 - val_accuracy: 0.3645\n",
      "Epoch 3/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 1.9331 - accuracy: 0.3486 - val_loss: 1.5751 - val_accuracy: 0.3551\n",
      "Epoch 4/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 1.5518 - accuracy: 0.3725 - val_loss: 1.3501 - val_accuracy: 0.3925\n",
      "Epoch 5/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 1.3400 - accuracy: 0.3780 - val_loss: 1.2376 - val_accuracy: 0.3832\n",
      "Epoch 6/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.2308 - accuracy: 0.3854 - val_loss: 1.1971 - val_accuracy: 0.3925\n",
      "Epoch 7/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1846 - accuracy: 0.3878 - val_loss: 1.1799 - val_accuracy: 0.4112\n",
      "Epoch 8/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1556 - accuracy: 0.4058 - val_loss: 1.1688 - val_accuracy: 0.4112\n",
      "Epoch 9/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1386 - accuracy: 0.4168 - val_loss: 1.1556 - val_accuracy: 0.4206\n",
      "Epoch 10/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1282 - accuracy: 0.4175 - val_loss: 1.1459 - val_accuracy: 0.4299\n",
      "Epoch 11/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1151 - accuracy: 0.4191 - val_loss: 1.1368 - val_accuracy: 0.4486\n",
      "Epoch 12/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1065 - accuracy: 0.4277 - val_loss: 1.1261 - val_accuracy: 0.4299\n",
      "Epoch 13/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1091 - accuracy: 0.4140 - val_loss: 1.1167 - val_accuracy: 0.4206\n",
      "Epoch 14/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1022 - accuracy: 0.4168 - val_loss: 1.1079 - val_accuracy: 0.4206\n",
      "Epoch 15/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0943 - accuracy: 0.4285 - val_loss: 1.1010 - val_accuracy: 0.4393\n",
      "Epoch 16/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0909 - accuracy: 0.4293 - val_loss: 1.0938 - val_accuracy: 0.4393\n",
      "Epoch 17/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0692 - accuracy: 0.4340 - val_loss: 1.0867 - val_accuracy: 0.4393\n",
      "Epoch 18/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0852 - accuracy: 0.4269 - val_loss: 1.0795 - val_accuracy: 0.4393\n",
      "Epoch 19/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0693 - accuracy: 0.4230 - val_loss: 1.0750 - val_accuracy: 0.4299\n",
      "Epoch 20/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0525 - accuracy: 0.4461 - val_loss: 1.0690 - val_accuracy: 0.4393\n",
      "Epoch 21/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 1.0535 - accuracy: 0.4387 - val_loss: 1.0639 - val_accuracy: 0.4393\n",
      "Epoch 22/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0570 - accuracy: 0.4430 - val_loss: 1.0577 - val_accuracy: 0.4579\n",
      "Epoch 23/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0614 - accuracy: 0.4371 - val_loss: 1.0524 - val_accuracy: 0.4579\n",
      "Epoch 24/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0643 - accuracy: 0.4269 - val_loss: 1.0481 - val_accuracy: 0.4673\n",
      "Epoch 25/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0534 - accuracy: 0.4375 - val_loss: 1.0430 - val_accuracy: 0.4579\n",
      "Epoch 26/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0441 - accuracy: 0.4489 - val_loss: 1.0405 - val_accuracy: 0.4393\n",
      "Epoch 27/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 1.0422 - accuracy: 0.4618 - val_loss: 1.0355 - val_accuracy: 0.4486\n",
      "Epoch 28/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0392 - accuracy: 0.4469 - val_loss: 1.0318 - val_accuracy: 0.4673\n",
      "Epoch 29/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0402 - accuracy: 0.4548 - val_loss: 1.0286 - val_accuracy: 0.4579\n",
      "Epoch 30/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0343 - accuracy: 0.4438 - val_loss: 1.0269 - val_accuracy: 0.4486\n",
      "Epoch 31/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0381 - accuracy: 0.4497 - val_loss: 1.0233 - val_accuracy: 0.4486\n",
      "Epoch 32/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0424 - accuracy: 0.4454 - val_loss: 1.0212 - val_accuracy: 0.4579\n",
      "Epoch 33/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0308 - accuracy: 0.4606 - val_loss: 1.0202 - val_accuracy: 0.4766\n",
      "Epoch 34/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 1.0270 - accuracy: 0.4528 - val_loss: 1.0165 - val_accuracy: 0.4673\n",
      "Epoch 35/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0382 - accuracy: 0.4559 - val_loss: 1.0160 - val_accuracy: 0.4673\n",
      "Epoch 36/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0301 - accuracy: 0.4520 - val_loss: 1.0135 - val_accuracy: 0.4766\n",
      "Epoch 37/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0252 - accuracy: 0.4501 - val_loss: 1.0121 - val_accuracy: 0.4766\n",
      "Epoch 38/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0299 - accuracy: 0.4755 - val_loss: 1.0088 - val_accuracy: 0.4860\n",
      "Epoch 39/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0203 - accuracy: 0.4696 - val_loss: 1.0083 - val_accuracy: 0.4953\n",
      "Epoch 40/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0036 - accuracy: 0.4834 - val_loss: 1.0061 - val_accuracy: 0.5140\n",
      "Epoch 41/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0253 - accuracy: 0.4591 - val_loss: 1.0043 - val_accuracy: 0.4860\n",
      "Epoch 42/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0194 - accuracy: 0.4610 - val_loss: 1.0041 - val_accuracy: 0.5140\n",
      "Epoch 43/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0244 - accuracy: 0.4665 - val_loss: 1.0028 - val_accuracy: 0.5140\n",
      "Epoch 44/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0222 - accuracy: 0.4704 - val_loss: 1.0007 - val_accuracy: 0.5234\n",
      "Epoch 45/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0203 - accuracy: 0.4689 - val_loss: 0.9998 - val_accuracy: 0.5140\n",
      "Epoch 46/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0250 - accuracy: 0.4528 - val_loss: 1.0009 - val_accuracy: 0.5140\n",
      "Epoch 47/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0139 - accuracy: 0.4665 - val_loss: 0.9997 - val_accuracy: 0.5234\n",
      "Epoch 48/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0200 - accuracy: 0.4626 - val_loss: 0.9972 - val_accuracy: 0.5234\n",
      "Epoch 49/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 1.0134 - accuracy: 0.4700 - val_loss: 0.9969 - val_accuracy: 0.5327\n",
      "Epoch 50/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 1.0195 - accuracy: 0.4599 - val_loss: 0.9959 - val_accuracy: 0.5140\n",
      "Epoch 51/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0183 - accuracy: 0.4771 - val_loss: 0.9945 - val_accuracy: 0.5140\n",
      "Epoch 52/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0164 - accuracy: 0.4606 - val_loss: 0.9941 - val_accuracy: 0.5140\n",
      "Epoch 53/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0183 - accuracy: 0.4677 - val_loss: 0.9936 - val_accuracy: 0.5234\n",
      "Epoch 54/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0113 - accuracy: 0.4783 - val_loss: 0.9932 - val_accuracy: 0.5234\n",
      "Epoch 55/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0123 - accuracy: 0.4747 - val_loss: 0.9931 - val_accuracy: 0.5234\n",
      "Epoch 56/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0122 - accuracy: 0.4681 - val_loss: 0.9912 - val_accuracy: 0.5327\n",
      "Epoch 57/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0180 - accuracy: 0.4599 - val_loss: 0.9901 - val_accuracy: 0.5234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9995 - accuracy: 0.4869 - val_loss: 0.9895 - val_accuracy: 0.5234\n",
      "Epoch 59/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0117 - accuracy: 0.4712 - val_loss: 0.9909 - val_accuracy: 0.5234\n",
      "Epoch 60/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0018 - accuracy: 0.4783 - val_loss: 0.9904 - val_accuracy: 0.5234\n",
      "Epoch 61/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0066 - accuracy: 0.4814 - val_loss: 0.9890 - val_accuracy: 0.5234\n",
      "Epoch 62/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0031 - accuracy: 0.4837 - val_loss: 0.9891 - val_accuracy: 0.5140\n",
      "Epoch 63/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0116 - accuracy: 0.4818 - val_loss: 0.9865 - val_accuracy: 0.5327\n",
      "Epoch 64/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0037 - accuracy: 0.4689 - val_loss: 0.9861 - val_accuracy: 0.5327\n",
      "Epoch 65/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0076 - accuracy: 0.4732 - val_loss: 0.9865 - val_accuracy: 0.5234\n",
      "Epoch 66/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0153 - accuracy: 0.4602 - val_loss: 0.9863 - val_accuracy: 0.5234\n",
      "Epoch 67/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0002 - accuracy: 0.4790 - val_loss: 0.9844 - val_accuracy: 0.5047\n",
      "Epoch 68/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0015 - accuracy: 0.4759 - val_loss: 0.9861 - val_accuracy: 0.5421\n",
      "Epoch 69/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0073 - accuracy: 0.4837 - val_loss: 0.9854 - val_accuracy: 0.5140\n",
      "Epoch 70/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0032 - accuracy: 0.4740 - val_loss: 0.9848 - val_accuracy: 0.5234\n",
      "Epoch 71/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0024 - accuracy: 0.4763 - val_loss: 0.9843 - val_accuracy: 0.5234\n",
      "Epoch 72/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0001 - accuracy: 0.4877 - val_loss: 0.9847 - val_accuracy: 0.5327\n",
      "Epoch 73/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0088 - accuracy: 0.4755 - val_loss: 0.9825 - val_accuracy: 0.5234\n",
      "Epoch 74/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0062 - accuracy: 0.4779 - val_loss: 0.9831 - val_accuracy: 0.5327\n",
      "Epoch 75/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9993 - accuracy: 0.4837 - val_loss: 0.9840 - val_accuracy: 0.5327\n",
      "Epoch 76/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0041 - accuracy: 0.4830 - val_loss: 0.9838 - val_accuracy: 0.5421\n",
      "Epoch 77/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0056 - accuracy: 0.4771 - val_loss: 0.9836 - val_accuracy: 0.5421\n",
      "Epoch 78/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0084 - accuracy: 0.4661 - val_loss: 0.9845 - val_accuracy: 0.5421\n",
      "Epoch 79/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0049 - accuracy: 0.4822 - val_loss: 0.9821 - val_accuracy: 0.5234\n",
      "Epoch 80/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9995 - accuracy: 0.4802 - val_loss: 0.9837 - val_accuracy: 0.5421\n",
      "Epoch 81/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0037 - accuracy: 0.4818 - val_loss: 0.9818 - val_accuracy: 0.5327\n",
      "Epoch 82/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9961 - accuracy: 0.4826 - val_loss: 0.9813 - val_accuracy: 0.5327\n",
      "Epoch 83/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0049 - accuracy: 0.4736 - val_loss: 0.9829 - val_accuracy: 0.5421\n",
      "Epoch 84/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0004 - accuracy: 0.4814 - val_loss: 0.9832 - val_accuracy: 0.5327\n",
      "Epoch 85/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0087 - accuracy: 0.4759 - val_loss: 0.9816 - val_accuracy: 0.5327\n",
      "Epoch 86/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0075 - accuracy: 0.4826 - val_loss: 0.9821 - val_accuracy: 0.5327\n",
      "Epoch 87/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0083 - accuracy: 0.4767 - val_loss: 0.9845 - val_accuracy: 0.5327\n",
      "Epoch 88/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0065 - accuracy: 0.4826 - val_loss: 0.9830 - val_accuracy: 0.5421\n",
      "Epoch 89/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0095 - accuracy: 0.4783 - val_loss: 0.9822 - val_accuracy: 0.5327\n",
      "Epoch 90/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0032 - accuracy: 0.4783 - val_loss: 0.9829 - val_accuracy: 0.5327\n",
      "Epoch 91/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0000 - accuracy: 0.4834 - val_loss: 0.9846 - val_accuracy: 0.5234\n",
      "Epoch 92/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9995 - accuracy: 0.4767 - val_loss: 0.9837 - val_accuracy: 0.5421\n",
      "Epoch 93/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0038 - accuracy: 0.4736 - val_loss: 0.9835 - val_accuracy: 0.5421\n",
      "Epoch 94/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9994 - accuracy: 0.4865 - val_loss: 0.9825 - val_accuracy: 0.5234\n",
      "Epoch 95/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9953 - accuracy: 0.4939 - val_loss: 0.9823 - val_accuracy: 0.5421\n",
      "Epoch 96/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9979 - accuracy: 0.4892 - val_loss: 0.9820 - val_accuracy: 0.5421\n",
      "Epoch 97/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0041 - accuracy: 0.4845 - val_loss: 0.9820 - val_accuracy: 0.5327\n",
      "Epoch 98/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9989 - accuracy: 0.4896 - val_loss: 0.9823 - val_accuracy: 0.5421\n",
      "Epoch 99/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9974 - accuracy: 0.4779 - val_loss: 0.9821 - val_accuracy: 0.5421\n",
      "Epoch 100/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9943 - accuracy: 0.4877 - val_loss: 0.9814 - val_accuracy: 0.5327\n",
      "Epoch 101/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9965 - accuracy: 0.4822 - val_loss: 0.9812 - val_accuracy: 0.5327\n",
      "Epoch 102/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9985 - accuracy: 0.4947 - val_loss: 0.9825 - val_accuracy: 0.5327\n",
      "Epoch 103/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9968 - accuracy: 0.4986 - val_loss: 0.9824 - val_accuracy: 0.5327\n",
      "Epoch 104/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9959 - accuracy: 0.4900 - val_loss: 0.9819 - val_accuracy: 0.5327\n",
      "Epoch 105/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9918 - accuracy: 0.4963 - val_loss: 0.9837 - val_accuracy: 0.5327\n",
      "Epoch 106/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9976 - accuracy: 0.4900 - val_loss: 0.9817 - val_accuracy: 0.5327\n",
      "Epoch 107/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9936 - accuracy: 0.4857 - val_loss: 0.9822 - val_accuracy: 0.5327\n",
      "Epoch 108/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9985 - accuracy: 0.4759 - val_loss: 0.9816 - val_accuracy: 0.5327\n",
      "Epoch 109/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0054 - accuracy: 0.4845 - val_loss: 0.9812 - val_accuracy: 0.5327\n",
      "Epoch 110/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9924 - accuracy: 0.4959 - val_loss: 0.9813 - val_accuracy: 0.5327\n",
      "Epoch 111/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9947 - accuracy: 0.4881 - val_loss: 0.9796 - val_accuracy: 0.5327\n",
      "Epoch 112/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9911 - accuracy: 0.4892 - val_loss: 0.9814 - val_accuracy: 0.5327\n",
      "Epoch 113/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9915 - accuracy: 0.4865 - val_loss: 0.9808 - val_accuracy: 0.5327\n",
      "Epoch 114/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9998 - accuracy: 0.4904 - val_loss: 0.9799 - val_accuracy: 0.5327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9912 - accuracy: 0.4959 - val_loss: 0.9814 - val_accuracy: 0.5234\n",
      "Epoch 116/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9983 - accuracy: 0.4884 - val_loss: 0.9819 - val_accuracy: 0.5421\n",
      "Epoch 117/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9974 - accuracy: 0.4888 - val_loss: 0.9811 - val_accuracy: 0.5327\n",
      "Epoch 118/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9957 - accuracy: 0.4943 - val_loss: 0.9813 - val_accuracy: 0.5327\n",
      "Epoch 119/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0015 - accuracy: 0.4881 - val_loss: 0.9814 - val_accuracy: 0.5327\n",
      "Epoch 120/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9933 - accuracy: 0.4849 - val_loss: 0.9819 - val_accuracy: 0.5327\n",
      "Epoch 121/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9996 - accuracy: 0.4834 - val_loss: 0.9822 - val_accuracy: 0.5327\n",
      "Epoch 122/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9968 - accuracy: 0.4869 - val_loss: 0.9820 - val_accuracy: 0.5327\n",
      "Epoch 123/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9939 - accuracy: 0.4963 - val_loss: 0.9816 - val_accuracy: 0.5327\n",
      "Epoch 124/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9964 - accuracy: 0.4904 - val_loss: 0.9834 - val_accuracy: 0.5327\n",
      "Epoch 125/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9981 - accuracy: 0.5045 - val_loss: 0.9831 - val_accuracy: 0.5327\n",
      "Epoch 126/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9955 - accuracy: 0.4881 - val_loss: 0.9840 - val_accuracy: 0.5234\n",
      "Epoch 127/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9924 - accuracy: 0.4892 - val_loss: 0.9826 - val_accuracy: 0.5327\n",
      "Epoch 128/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9931 - accuracy: 0.4888 - val_loss: 0.9838 - val_accuracy: 0.5327\n",
      "Epoch 129/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9952 - accuracy: 0.4892 - val_loss: 0.9830 - val_accuracy: 0.5327\n",
      "Epoch 130/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9999 - accuracy: 0.4837 - val_loss: 0.9820 - val_accuracy: 0.5327\n",
      "Epoch 131/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9982 - accuracy: 0.4947 - val_loss: 0.9837 - val_accuracy: 0.5327\n",
      "Epoch 132/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0039 - accuracy: 0.4916 - val_loss: 0.9831 - val_accuracy: 0.5327\n",
      "Epoch 133/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0004 - accuracy: 0.4975 - val_loss: 0.9829 - val_accuracy: 0.5327\n",
      "Epoch 134/200\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 1.0031 - accuracy: 0.4912 - val_loss: 0.9822 - val_accuracy: 0.5327\n",
      "Epoch 135/200\n",
      "160/160 [==============================] - 1s 3ms/step - loss: 1.0006 - accuracy: 0.4916 - val_loss: 0.9833 - val_accuracy: 0.5327\n",
      "Epoch 136/200\n",
      "160/160 [==============================] - 1s 6ms/step - loss: 0.9958 - accuracy: 0.4943 - val_loss: 0.9827 - val_accuracy: 0.5327\n",
      "Epoch 137/200\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9950 - accuracy: 0.4994 - val_loss: 0.9831 - val_accuracy: 0.5327\n",
      "Epoch 138/200\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.9999 - accuracy: 0.5006 - val_loss: 0.9835 - val_accuracy: 0.5327\n",
      "Epoch 139/200\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9899 - accuracy: 0.4912 - val_loss: 0.9834 - val_accuracy: 0.5327\n",
      "Epoch 140/200\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9928 - accuracy: 0.5025 - val_loss: 0.9831 - val_accuracy: 0.5327\n",
      "Epoch 141/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9913 - accuracy: 0.4994 - val_loss: 0.9830 - val_accuracy: 0.5327\n",
      "Epoch 142/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9983 - accuracy: 0.4971 - val_loss: 0.9813 - val_accuracy: 0.5327\n",
      "Epoch 143/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9901 - accuracy: 0.4908 - val_loss: 0.9831 - val_accuracy: 0.5327\n",
      "Epoch 144/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9972 - accuracy: 0.4877 - val_loss: 0.9834 - val_accuracy: 0.5327\n",
      "Epoch 145/200\n",
      "160/160 [==============================] - 1s 3ms/step - loss: 0.9968 - accuracy: 0.4900 - val_loss: 0.9836 - val_accuracy: 0.5327\n",
      "Epoch 146/200\n",
      "160/160 [==============================] - 1s 6ms/step - loss: 0.9869 - accuracy: 0.4990 - val_loss: 0.9858 - val_accuracy: 0.5234\n",
      "Epoch 147/200\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.9913 - accuracy: 0.4912 - val_loss: 0.9845 - val_accuracy: 0.5327\n",
      "Epoch 148/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9943 - accuracy: 0.5014 - val_loss: 0.9856 - val_accuracy: 0.5234\n",
      "Epoch 149/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9908 - accuracy: 0.4971 - val_loss: 0.9851 - val_accuracy: 0.5327\n",
      "Epoch 150/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9878 - accuracy: 0.5053 - val_loss: 0.9824 - val_accuracy: 0.5327\n",
      "Epoch 151/200\n",
      "160/160 [==============================] - 1s 5ms/step - loss: 0.9954 - accuracy: 0.5029 - val_loss: 0.9841 - val_accuracy: 0.5327\n",
      "Epoch 152/200\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9952 - accuracy: 0.4787 - val_loss: 0.9840 - val_accuracy: 0.5327\n",
      "Epoch 153/200\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9928 - accuracy: 0.4939 - val_loss: 0.9835 - val_accuracy: 0.5327\n",
      "Epoch 154/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9990 - accuracy: 0.4881 - val_loss: 0.9843 - val_accuracy: 0.5327\n",
      "Epoch 155/200\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9929 - accuracy: 0.4845 - val_loss: 0.9839 - val_accuracy: 0.5327\n",
      "Epoch 156/200\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 0.9961 - accuracy: 0.5010 - val_loss: 0.9852 - val_accuracy: 0.5327\n",
      "Epoch 157/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9936 - accuracy: 0.4939 - val_loss: 0.9840 - val_accuracy: 0.5327\n",
      "Epoch 158/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9968 - accuracy: 0.4834 - val_loss: 0.9833 - val_accuracy: 0.5327\n",
      "Epoch 159/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9877 - accuracy: 0.5010 - val_loss: 0.9842 - val_accuracy: 0.5327\n",
      "Epoch 160/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9916 - accuracy: 0.4955 - val_loss: 0.9854 - val_accuracy: 0.5327\n",
      "Epoch 161/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9974 - accuracy: 0.4849 - val_loss: 0.9854 - val_accuracy: 0.5327\n",
      "Epoch 162/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0037 - accuracy: 0.4931 - val_loss: 0.9851 - val_accuracy: 0.5234\n",
      "Epoch 163/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9902 - accuracy: 0.5033 - val_loss: 0.9833 - val_accuracy: 0.5234\n",
      "Epoch 164/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9963 - accuracy: 0.4994 - val_loss: 0.9830 - val_accuracy: 0.5421\n",
      "Epoch 165/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9903 - accuracy: 0.4947 - val_loss: 0.9829 - val_accuracy: 0.5327\n",
      "Epoch 166/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9948 - accuracy: 0.4912 - val_loss: 0.9838 - val_accuracy: 0.5421\n",
      "Epoch 167/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9913 - accuracy: 0.5002 - val_loss: 0.9839 - val_accuracy: 0.5327\n",
      "Epoch 168/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9948 - accuracy: 0.4924 - val_loss: 0.9838 - val_accuracy: 0.5421\n",
      "Epoch 169/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9984 - accuracy: 0.4881 - val_loss: 0.9843 - val_accuracy: 0.5421\n",
      "Epoch 170/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9875 - accuracy: 0.5014 - val_loss: 0.9836 - val_accuracy: 0.5421\n",
      "Epoch 171/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9874 - accuracy: 0.5037 - val_loss: 0.9828 - val_accuracy: 0.5421\n",
      "Epoch 172/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9892 - accuracy: 0.5069 - val_loss: 0.9838 - val_accuracy: 0.5421\n",
      "Epoch 173/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9885 - accuracy: 0.4975 - val_loss: 0.9840 - val_accuracy: 0.5327\n",
      "Epoch 174/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9970 - accuracy: 0.4904 - val_loss: 0.9846 - val_accuracy: 0.5421\n",
      "Epoch 175/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9934 - accuracy: 0.4975 - val_loss: 0.9840 - val_accuracy: 0.5327\n",
      "Epoch 176/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9942 - accuracy: 0.4975 - val_loss: 0.9831 - val_accuracy: 0.5327\n",
      "Epoch 177/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9923 - accuracy: 0.4928 - val_loss: 0.9832 - val_accuracy: 0.5421\n",
      "Epoch 178/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9966 - accuracy: 0.4939 - val_loss: 0.9864 - val_accuracy: 0.5327\n",
      "Epoch 179/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9921 - accuracy: 0.4912 - val_loss: 0.9845 - val_accuracy: 0.5327\n",
      "Epoch 180/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9856 - accuracy: 0.5057 - val_loss: 0.9842 - val_accuracy: 0.5234\n",
      "Epoch 181/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9952 - accuracy: 0.4873 - val_loss: 0.9846 - val_accuracy: 0.5421\n",
      "Epoch 182/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9908 - accuracy: 0.4947 - val_loss: 0.9843 - val_accuracy: 0.5327\n",
      "Epoch 183/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9869 - accuracy: 0.5053 - val_loss: 0.9841 - val_accuracy: 0.5421\n",
      "Epoch 184/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9899 - accuracy: 0.4892 - val_loss: 0.9852 - val_accuracy: 0.5327\n",
      "Epoch 185/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9893 - accuracy: 0.5033 - val_loss: 0.9841 - val_accuracy: 0.5234\n",
      "Epoch 186/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9905 - accuracy: 0.4928 - val_loss: 0.9849 - val_accuracy: 0.5327\n",
      "Epoch 187/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9931 - accuracy: 0.4916 - val_loss: 0.9829 - val_accuracy: 0.5327\n",
      "Epoch 188/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9966 - accuracy: 0.4908 - val_loss: 0.9837 - val_accuracy: 0.5421\n",
      "Epoch 189/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9930 - accuracy: 0.4928 - val_loss: 0.9844 - val_accuracy: 0.5327\n",
      "Epoch 190/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9949 - accuracy: 0.4994 - val_loss: 0.9832 - val_accuracy: 0.5421\n",
      "Epoch 191/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9951 - accuracy: 0.4982 - val_loss: 0.9841 - val_accuracy: 0.5421\n",
      "Epoch 192/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9945 - accuracy: 0.4963 - val_loss: 0.9828 - val_accuracy: 0.5421\n",
      "Epoch 193/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9942 - accuracy: 0.4943 - val_loss: 0.9836 - val_accuracy: 0.5327\n",
      "Epoch 194/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0010 - accuracy: 0.4865 - val_loss: 0.9852 - val_accuracy: 0.5421\n",
      "Epoch 195/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9970 - accuracy: 0.4967 - val_loss: 0.9846 - val_accuracy: 0.5234\n",
      "Epoch 196/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9883 - accuracy: 0.4967 - val_loss: 0.9860 - val_accuracy: 0.5327\n",
      "Epoch 197/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9906 - accuracy: 0.5041 - val_loss: 0.9863 - val_accuracy: 0.5327\n",
      "Epoch 198/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9856 - accuracy: 0.4935 - val_loss: 0.9842 - val_accuracy: 0.5327\n",
      "Epoch 199/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9909 - accuracy: 0.5049 - val_loss: 0.9837 - val_accuracy: 0.5421\n",
      "Epoch 200/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9916 - accuracy: 0.4912 - val_loss: 0.9852 - val_accuracy: 0.5327\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, batch_size=16, epochs=200, validation_split=0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'], label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.5368421052631579\n"
     ]
    }
   ],
   "source": [
    "predictions = np.argmax(model.predict(x_test), axis=1)\n",
    "accuracy = (predictions == y_test.values).sum() / y_test.values.shape[0]\n",
    "print(\"Test set accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Betting time..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outcome when bet on home team\n",
    "Just a sanity check. Should not see high/any yields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agencies = ['B365', 'BW', 'IW', 'LB', 'WH', 'SJ', 'VC', 'GB', 'BS']\n",
    "agencies = ['B365']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set size (380,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Test set size\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B365H</th>\n",
       "      <th>B365D</th>\n",
       "      <th>B365A</th>\n",
       "      <th>HTGS</th>\n",
       "      <th>ATGS</th>\n",
       "      <th>HTGC</th>\n",
       "      <th>ATGC</th>\n",
       "      <th>HTP</th>\n",
       "      <th>ATP</th>\n",
       "      <th>HM1</th>\n",
       "      <th>HM2</th>\n",
       "      <th>HM3</th>\n",
       "      <th>HM4</th>\n",
       "      <th>HM5</th>\n",
       "      <th>AM1</th>\n",
       "      <th>AM2</th>\n",
       "      <th>AM3</th>\n",
       "      <th>AM4</th>\n",
       "      <th>AM5</th>\n",
       "      <th>HTWinStreak3</th>\n",
       "      <th>HTWinStreak5</th>\n",
       "      <th>HTLossStreak3</th>\n",
       "      <th>HTLossStreak5</th>\n",
       "      <th>ATWinStreak3</th>\n",
       "      <th>ATWinStreak5</th>\n",
       "      <th>ATLossStreak3</th>\n",
       "      <th>ATLossStreak5</th>\n",
       "      <th>HTGD</th>\n",
       "      <th>ATGD</th>\n",
       "      <th>DiffPts</th>\n",
       "      <th>DiffFormPts</th>\n",
       "      <th>DiffLP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.67</td>\n",
       "      <td>3.60</td>\n",
       "      <td>5.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.60</td>\n",
       "      <td>3.25</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.25</td>\n",
       "      <td>3.25</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.17</td>\n",
       "      <td>6.50</td>\n",
       "      <td>21.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.20</td>\n",
       "      <td>3.25</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2655</th>\n",
       "      <td>3.50</td>\n",
       "      <td>3.60</td>\n",
       "      <td>2.15</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.650602</td>\n",
       "      <td>0.620253</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.394737</td>\n",
       "      <td>0.394737</td>\n",
       "      <td>-0.078947</td>\n",
       "      <td>-0.236842</td>\n",
       "      <td>-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2656</th>\n",
       "      <td>6.00</td>\n",
       "      <td>4.75</td>\n",
       "      <td>1.53</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>0.614458</td>\n",
       "      <td>0.506329</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.263158</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2657</th>\n",
       "      <td>2.05</td>\n",
       "      <td>3.75</td>\n",
       "      <td>3.70</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>0.578313</td>\n",
       "      <td>0.759494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.263158</td>\n",
       "      <td>-0.368421</td>\n",
       "      <td>-0.078947</td>\n",
       "      <td>-0.131579</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2658</th>\n",
       "      <td>2.40</td>\n",
       "      <td>3.60</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.645833</td>\n",
       "      <td>0.566265</td>\n",
       "      <td>0.620253</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.368421</td>\n",
       "      <td>0.342105</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.157895</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2659</th>\n",
       "      <td>1.67</td>\n",
       "      <td>4.20</td>\n",
       "      <td>5.25</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.409639</td>\n",
       "      <td>0.810127</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>-0.526316</td>\n",
       "      <td>-0.026316</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>-11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2660 rows  32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      B365H  B365D  B365A  HTGS      ATGS      HTGC      ATGC       HTP  \\\n",
       "0      1.67   3.60   5.50  0.00  0.000000  0.000000  0.000000  0.000000   \n",
       "1      3.60   3.25   2.10  0.00  0.000000  0.000000  0.000000  0.000000   \n",
       "2      2.25   3.25   3.25  0.00  0.000000  0.000000  0.000000  0.000000   \n",
       "3      1.17   6.50  21.00  0.00  0.000000  0.000000  0.000000  0.000000   \n",
       "4      3.20   3.25   2.30  0.00  0.000000  0.000000  0.000000  0.000000   \n",
       "...     ...    ...    ...   ...       ...       ...       ...       ...   \n",
       "2655   3.50   3.60   2.15  0.39  0.666667  0.650602  0.620253  0.000000   \n",
       "2656   6.00   4.75   1.53  0.41  0.729167  0.614458  0.506329  0.078947   \n",
       "2657   2.05   3.75   3.70  0.38  0.479167  0.578313  0.759494  0.000000   \n",
       "2658   2.40   3.60   3.00  0.33  0.645833  0.566265  0.620253  0.026316   \n",
       "2659   1.67   4.20   5.25  0.46  0.458333  0.409639  0.810127  0.000000   \n",
       "\n",
       "           ATP  HM1  HM2  HM3  HM4  HM5  AM1  AM2  AM3  AM4  AM5  \\\n",
       "0     0.000000    2    2    2    2    2    2    2    2    2    2   \n",
       "1     0.000000    2    2    2    2    2    2    2    2    2    2   \n",
       "2     0.000000    2    2    2    2    2    2    2    2    2    2   \n",
       "3     0.000000    2    2    2    2    2    2    2    2    2    2   \n",
       "4     0.000000    2    2    2    2    2    2    2    2    2    2   \n",
       "...        ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "2655  0.078947    1    0    1    1    1    3    1    3    3    0   \n",
       "2656  0.026316    3    3    1    1    3    0    1    3    0    3   \n",
       "2657  0.078947    1    1    3    1    3    3    3    0    0    3   \n",
       "2658  0.026316    0    1    0    1    1    0    3    1    0    3   \n",
       "2659  0.026316    1    3    0    3    3    0    1    1    1    3   \n",
       "\n",
       "      HTWinStreak3  HTWinStreak5  HTLossStreak3  HTLossStreak5  ATWinStreak3  \\\n",
       "0                0             0              0              0             0   \n",
       "1                0             0              0              0             0   \n",
       "2                0             0              0              0             0   \n",
       "3                0             0              0              0             0   \n",
       "4                0             0              0              0             0   \n",
       "...            ...           ...            ...            ...           ...   \n",
       "2655             0             0              1              0             0   \n",
       "2656             0             0              0              0             0   \n",
       "2657             0             0              0              0             0   \n",
       "2658             0             0              0              0             0   \n",
       "2659             0             0              0              0             0   \n",
       "\n",
       "      ATWinStreak5  ATLossStreak3  ATLossStreak5      HTGD      ATGD  \\\n",
       "0                0              0              0  0.000000  0.000000   \n",
       "1                0              0              0  0.000000  0.000000   \n",
       "2                0              0              0  0.000000  0.000000   \n",
       "3                0              0              0  0.000000  0.000000   \n",
       "4                0              0              0  0.000000  0.000000   \n",
       "...            ...            ...            ...       ...       ...   \n",
       "2655             0              0              0 -0.394737  0.394737   \n",
       "2656             0              0              0 -0.263158  0.789474   \n",
       "2657             0              0              0 -0.263158 -0.368421   \n",
       "2658             0              0              0 -0.368421  0.342105   \n",
       "2659             0              0              0  0.315789 -0.526316   \n",
       "\n",
       "       DiffPts  DiffFormPts  DiffLP  \n",
       "0     0.000000     0.000000      -8  \n",
       "1     0.000000     0.000000      -2  \n",
       "2     0.000000     0.000000       1  \n",
       "3     0.000000     0.000000     -16  \n",
       "4     0.000000     0.000000       2  \n",
       "...        ...          ...     ...  \n",
       "2655 -0.078947    -0.236842      -4  \n",
       "2656  0.052632     0.026316      11  \n",
       "2657 -0.078947    -0.131579       4  \n",
       "2658  0.000000    -0.157895      15  \n",
       "2659 -0.026316     0.157895     -11  \n",
       "\n",
       "[2660 rows x 32 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bet_train\n",
    "bet_test\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Reference: https://github.com/raedovj/NN19_Project_Football/blob/master/ModelTester.py\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "def predict_always_on_one_thing_benefit(labels, betting_odds, predictable_value):\n",
    "    predictable_indices = np.zeros((labels.shape[0], 3))\n",
    "    predictable_indices[:, predictable_value] = 1\n",
    "    \n",
    "    print(predictable_value)\n",
    "    \n",
    "    agency = \"B365\"\n",
    "    odds = pd.concat([betting_odds[agency+'H'], betting_odds[agency+'D'], betting_odds[agency+'A']], axis=1)\n",
    "    # r holds betting results. 0 indicates loss, value otherwise shows the win amount\n",
    "    r = odds * predictable_indices * to_categorical(labels)\n",
    "    # Take max value of win, draw, other win. \n",
    "    r = r.values.max(axis=1)\n",
    "    # Let's say we bet 1. then our profit(or loss) would be = earnings - 1 per bet\n",
    "    r -= len(predictable_value)\n",
    "\n",
    "    print(\"Agency %s, \\twin amount: %.2f\" % (agency, r.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "Agency B365, \twin amount: 33.66\n",
      "[0]\n",
      "Agency B365, \twin amount: 50.57\n"
     ]
    }
   ],
   "source": [
    "## Profit for Away Wins Only \n",
    "predict_always_on_one_thing_benefit(y_train, bet_train, [0]) #Data Train\n",
    "predict_always_on_one_thing_benefit(y_test, bet_test, [0]) #Data Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "Agency B365, \twin amount: -96.47\n",
      "[1]\n",
      "Agency B365, \twin amount: -25.46\n"
     ]
    }
   ],
   "source": [
    "## Profit for Draw Only \n",
    "predict_always_on_one_thing_benefit(y_train, bet_train, [1]) #Data Train\n",
    "predict_always_on_one_thing_benefit(y_test, bet_test, [1]) #Data Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n",
      "Agency B365, \twin amount: 5112.06\n",
      "[2]\n",
      "Agency B365, \twin amount: 685.15\n"
     ]
    }
   ],
   "source": [
    "## Profit for Home Wins Only \n",
    "predict_always_on_one_thing_benefit(y_train, bet_train, [2]) #Data Train\n",
    "predict_always_on_one_thing_benefit(y_test, bet_test, [2]) #Data Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2]\n",
      "Agency B365, \twin amount: 5145.72\n",
      "[0, 2]\n",
      "Agency B365, \twin amount: 735.72\n"
     ]
    }
   ],
   "source": [
    "## Profit for Home or Away \n",
    "predict_always_on_one_thing_benefit(y_train, bet_train, [0, 2]) #Data Train\n",
    "predict_always_on_one_thing_benefit(y_test, bet_test, [0, 2]) #Data Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2]\n",
      "Agency B365, \twin amount: 5049.25\n",
      "[0, 1, 2]\n",
      "Agency B365, \twin amount: 710.26\n"
     ]
    }
   ],
   "source": [
    "## Profit for All Possibilities\n",
    "predict_always_on_one_thing_benefit(y_train, bet_train, [0, 1, 2]) #Data Train\n",
    "predict_always_on_one_thing_benefit(y_test, bet_test, [0, 1, 2]) #Data Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bet on every match\n",
    "Always bet on the predicted winner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions_3x1 = model.predict(x_train)\n",
    "test_predictions_3x1 = model.predict(x_test)\n",
    "train_predictions = np.argmax(train_predictions_3x1 , axis=1)\n",
    "test_predictions = np.argmax(test_predictions_3x1 , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2660, 3)\n",
      "(380, 3)\n"
     ]
    }
   ],
   "source": [
    "# train_predictions\n",
    "# test_predictions\n",
    "print(train_predictions_3x1.shape)\n",
    "print(test_predictions_3x1.shape)\n",
    "# x_train.shape\n",
    "# x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def always_bet_predicted_winner_profit(predictions, labels, betting_odds):      \n",
    "    predictions_categorical = to_categorical(predictions)\n",
    "    \n",
    "    agency = \"B365\"\n",
    "    odds = pd.concat([betting_odds[agency+'H'], betting_odds[agency+'D'], betting_odds[agency+'A']], axis=1)\n",
    "    # r holds betting results. 0 indicates loss, value otherwise shows the win amount\n",
    "    r = odds * predictions_categorical * to_categorical(labels)\n",
    "    # Take max value of win, draw, other win. \n",
    "    r = r.values.max(axis=1)\n",
    "    # Let's say we bet 1. then our profit(or loss) would be = earnings - 1\n",
    "    r -= 1\n",
    "    \n",
    "    print(\"Agency %s, \\twin amount: %.2f\" % (agency, r.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agency B365, \twin amount: 6653.02\n",
      "Agency B365, \twin amount: 932.93\n"
     ]
    }
   ],
   "source": [
    "always_bet_predicted_winner_profit(train_predictions, y_train, bet_train)\n",
    "always_bet_predicted_winner_profit(test_predictions, y_test, bet_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bet when expected return high enough\n",
    "First calculate the expected return of the team expected to win. If yield is high enough, then bet. \n",
    "* yield = prediction probability * odds. \n",
    "* Bet if yield > threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bet_predicted_winner_with_threshold_profit(predictions_3x1, predictions, labels, \n",
    "                                                          betting_odds, threshold):\n",
    "    predictions_categorical = to_categorical(predictions)\n",
    "    \n",
    "    agency = \"B365\"\n",
    "    odds = pd.concat([betting_odds[agency+'H'], betting_odds[agency+'D'], betting_odds[agency+'A']], axis=1)\n",
    "    # r holds betting results. 0 indicates loss, value otherwise shows the win amount\n",
    "    bet = odds * predictions_categorical * predictions_3x1\n",
    "    bet = bet > threshold\n",
    "    r = odds * predictions_categorical * to_categorical(labels)\n",
    "    r -= 1\n",
    "    # Set win/lose amount to 0 on matched it didn't bet\n",
    "    r[np.invert(bet)] = 0\n",
    "    # Take max value of win, draw, other win. \n",
    "    r = r.values.max(axis=1)\n",
    "\n",
    "    skip_percentage = (r==0).sum() / r.shape[0] * 100   \n",
    "    print(\"Agency %s, \\twin amount: %.2f. Didn't bet on %.2f%% of matches\" % (agency, r.sum(), skip_percentage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agency B365, \twin amount: 7779.47. Didn't bet on 48.83% of matches\n",
      "Agency B365, \twin amount: 1091.28. Didn't bet on 49.74% of matches\n"
     ]
    }
   ],
   "source": [
    "bet_predicted_winner_with_threshold_profit(train_predictions_3x1, train_predictions, y_train, bet_train, threshold=1)\n",
    "bet_predicted_winner_with_threshold_profit(test_predictions_3x1, test_predictions, y_test, bet_test, threshold=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bet on highest return\n",
    "Multiplies the neural network match predictions with betting odds and from these multiplications chooses from home win, draw, away win the highest expected return value. A threshhold can be set to choose if the yield is high enough to bet.  [prediction probability * odds > threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_highest_return(predictions_3x1, labels, betting_odds, threshold):\n",
    "        agency = \"B365\"\n",
    "        odds = pd.concat([betting_odds[agency+'H'], betting_odds[agency+'D'], betting_odds[agency+'A']], axis=1)\n",
    "        # Expected earning value. Basically expects that our NN predicts real match outcomes\n",
    "        expected = (odds * predictions_3x1).values\n",
    "\n",
    "        # Threshold matches, when we'd actually would make a bet. If expected yield is too low, it'll pass\n",
    "        bet = np.max(expected > threshold, axis=1)\n",
    "\n",
    "        # Take the highest yield of [home win, draw, other win]\n",
    "        r = np.argmax(expected, axis=1) \n",
    "\n",
    "        # Calculate wins/losses according to real match results\n",
    "        r = to_categorical(r) * to_categorical(labels)\n",
    "        r -= 1 # subtract our input bet\n",
    "\n",
    "        # Calculate earnings\n",
    "        r = r.max(axis=1) # Take max value of win, draw, other win. \n",
    "        r[np.invert(bet)] = 0 # Set win/lose amount to 0 on matched it didn't bet\n",
    "\n",
    "        skip_percentage = (bet==0).sum() / bet.shape[0] * 100   \n",
    "        print(\"Agency %s, \\twin amount: %.2f. Didn't bet on %.2f%% of matches\" % (agency, r.sum(), skip_percentage))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agency B365, \twin amount: -310.00. Didn't bet on 62.26% of matches\n",
      "Agency B365, \twin amount: -40.00. Didn't bet on 62.89% of matches\n"
     ]
    }
   ],
   "source": [
    "predict_on_highest_return(train_predictions_3x1, y_train, bet_train, threshold=2.5)\n",
    "predict_on_highest_return(test_predictions_3x1, y_test, bet_test, threshold=2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La Liga"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Train (Season 2009/2010 until Season 2010/2011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>match_api_id</th>\n",
       "      <th>B365H</th>\n",
       "      <th>B365D</th>\n",
       "      <th>B365A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>684585</td>\n",
       "      <td>1.25</td>\n",
       "      <td>5.5</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>684589</td>\n",
       "      <td>1.91</td>\n",
       "      <td>3.4</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>684586</td>\n",
       "      <td>2.10</td>\n",
       "      <td>3.3</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>684588</td>\n",
       "      <td>2.88</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>684594</td>\n",
       "      <td>3.40</td>\n",
       "      <td>3.4</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     match_api_id  B365H  B365D  B365A\n",
       "380        684585   1.25    5.5   13.0\n",
       "384        684589   1.91    3.4    4.2\n",
       "381        684586   2.10    3.3    3.5\n",
       "383        684588   2.88    3.2    2.5\n",
       "389        684594   3.40    3.4    2.1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv(\"datasets/LaLiga_sort.csv\", index_col=0)\n",
    "df1 = df1[df1['season'] != '2008/2009']\n",
    "df1 = df1[['match_api_id','B365H', 'B365D', 'B365A']]\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>match_api_id</th>\n",
       "      <th>FTR</th>\n",
       "      <th>HTGS</th>\n",
       "      <th>ATGS</th>\n",
       "      <th>HTGC</th>\n",
       "      <th>ATGC</th>\n",
       "      <th>HTP</th>\n",
       "      <th>ATP</th>\n",
       "      <th>HM1</th>\n",
       "      <th>HM2</th>\n",
       "      <th>HM3</th>\n",
       "      <th>HM4</th>\n",
       "      <th>HM5</th>\n",
       "      <th>AM1</th>\n",
       "      <th>AM2</th>\n",
       "      <th>AM3</th>\n",
       "      <th>AM4</th>\n",
       "      <th>AM5</th>\n",
       "      <th>HTWinStreak3</th>\n",
       "      <th>HTWinStreak5</th>\n",
       "      <th>HTLossStreak3</th>\n",
       "      <th>HTLossStreak5</th>\n",
       "      <th>ATWinStreak3</th>\n",
       "      <th>ATWinStreak5</th>\n",
       "      <th>ATLossStreak3</th>\n",
       "      <th>ATLossStreak5</th>\n",
       "      <th>HTGD</th>\n",
       "      <th>ATGD</th>\n",
       "      <th>DiffPts</th>\n",
       "      <th>DiffFormPts</th>\n",
       "      <th>DiffLP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2030084</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2030083</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2030090</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2030087</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2030091</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   match_api_id  FTR  HTGS  ATGS  HTGC  ATGC  HTP  ATP  HM1  HM2  HM3  HM4  \\\n",
       "0       2030084    1   0.0   0.0   0.0   0.0  0.0  0.0    2    2    2    2   \n",
       "1       2030083    2   0.0   0.0   0.0   0.0  0.0  0.0    2    2    2    2   \n",
       "2       2030090    1   0.0   0.0   0.0   0.0  0.0  0.0    2    2    2    2   \n",
       "3       2030087    2   0.0   0.0   0.0   0.0  0.0  0.0    2    2    2    2   \n",
       "4       2030091    1   0.0   0.0   0.0   0.0  0.0  0.0    2    2    2    2   \n",
       "\n",
       "   HM5  AM1  AM2  AM3  AM4  AM5  HTWinStreak3  HTWinStreak5  HTLossStreak3  \\\n",
       "0    2    2    2    2    2    2             0             0              0   \n",
       "1    2    2    2    2    2    2             0             0              0   \n",
       "2    2    2    2    2    2    2             0             0              0   \n",
       "3    2    2    2    2    2    2             0             0              0   \n",
       "4    2    2    2    2    2    2             0             0              0   \n",
       "\n",
       "   HTLossStreak5  ATWinStreak3  ATWinStreak5  ATLossStreak3  ATLossStreak5  \\\n",
       "0              0             0             0              0              0   \n",
       "1              0             0             0              0              0   \n",
       "2              0             0             0              0              0   \n",
       "3              0             0             0              0              0   \n",
       "4              0             0             0              0              0   \n",
       "\n",
       "   HTGD  ATGD  DiffPts  DiffFormPts  DiffLP  \n",
       "0   0.0   0.0      0.0          0.0       4  \n",
       "1   0.0   0.0      0.0          0.0     -15  \n",
       "2   0.0   0.0      0.0          0.0       6  \n",
       "3   0.0   0.0      0.0          0.0      -5  \n",
       "4   0.0   0.0      0.0          0.0       7  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv(\"datasets/laliga_data_train_onehot_matchid.csv\")\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B365H</th>\n",
       "      <th>B365D</th>\n",
       "      <th>B365A</th>\n",
       "      <th>FTR</th>\n",
       "      <th>HTGS</th>\n",
       "      <th>ATGS</th>\n",
       "      <th>HTGC</th>\n",
       "      <th>ATGC</th>\n",
       "      <th>HTP</th>\n",
       "      <th>ATP</th>\n",
       "      <th>HM1</th>\n",
       "      <th>HM2</th>\n",
       "      <th>HM3</th>\n",
       "      <th>HM4</th>\n",
       "      <th>HM5</th>\n",
       "      <th>AM1</th>\n",
       "      <th>AM2</th>\n",
       "      <th>AM3</th>\n",
       "      <th>AM4</th>\n",
       "      <th>AM5</th>\n",
       "      <th>HTWinStreak3</th>\n",
       "      <th>HTWinStreak5</th>\n",
       "      <th>HTLossStreak3</th>\n",
       "      <th>HTLossStreak5</th>\n",
       "      <th>ATWinStreak3</th>\n",
       "      <th>ATWinStreak5</th>\n",
       "      <th>ATLossStreak3</th>\n",
       "      <th>ATLossStreak5</th>\n",
       "      <th>HTGD</th>\n",
       "      <th>ATGD</th>\n",
       "      <th>DiffPts</th>\n",
       "      <th>DiffFormPts</th>\n",
       "      <th>DiffLP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2650</th>\n",
       "      <td>1.95</td>\n",
       "      <td>3.75</td>\n",
       "      <td>3.75</td>\n",
       "      <td>0</td>\n",
       "      <td>0.393162</td>\n",
       "      <td>0.382609</td>\n",
       "      <td>0.602564</td>\n",
       "      <td>0.623377</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.026316</td>\n",
       "      <td>-0.105263</td>\n",
       "      <td>-0.078947</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2651</th>\n",
       "      <td>1.33</td>\n",
       "      <td>5.75</td>\n",
       "      <td>8.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.470085</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.564103</td>\n",
       "      <td>0.610390</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.289474</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2652</th>\n",
       "      <td>1.75</td>\n",
       "      <td>3.75</td>\n",
       "      <td>4.50</td>\n",
       "      <td>2</td>\n",
       "      <td>0.521368</td>\n",
       "      <td>0.443478</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.740260</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.131579</td>\n",
       "      <td>-0.157895</td>\n",
       "      <td>-0.078947</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2653</th>\n",
       "      <td>26.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0</td>\n",
       "      <td>0.393162</td>\n",
       "      <td>0.947826</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.376623</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.526316</td>\n",
       "      <td>2.105263</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.078947</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2654</th>\n",
       "      <td>13.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.939130</td>\n",
       "      <td>0.756410</td>\n",
       "      <td>0.441558</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.368421</td>\n",
       "      <td>1.947368</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.263158</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2655</th>\n",
       "      <td>1.80</td>\n",
       "      <td>3.75</td>\n",
       "      <td>4.50</td>\n",
       "      <td>2</td>\n",
       "      <td>0.290598</td>\n",
       "      <td>0.382609</td>\n",
       "      <td>0.435897</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.131579</td>\n",
       "      <td>-0.026316</td>\n",
       "      <td>-0.026316</td>\n",
       "      <td>-9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2656</th>\n",
       "      <td>2.00</td>\n",
       "      <td>3.60</td>\n",
       "      <td>3.70</td>\n",
       "      <td>2</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.408696</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.740260</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.947368</td>\n",
       "      <td>-0.263158</td>\n",
       "      <td>-0.026316</td>\n",
       "      <td>-0.026316</td>\n",
       "      <td>-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2657</th>\n",
       "      <td>1.33</td>\n",
       "      <td>5.25</td>\n",
       "      <td>9.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.418803</td>\n",
       "      <td>0.313043</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.870130</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.605263</td>\n",
       "      <td>-0.815789</td>\n",
       "      <td>-0.078947</td>\n",
       "      <td>-0.078947</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2658</th>\n",
       "      <td>5.00</td>\n",
       "      <td>3.80</td>\n",
       "      <td>1.70</td>\n",
       "      <td>2</td>\n",
       "      <td>0.273504</td>\n",
       "      <td>0.313043</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.844156</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.763158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.078947</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2659</th>\n",
       "      <td>1.44</td>\n",
       "      <td>4.50</td>\n",
       "      <td>7.50</td>\n",
       "      <td>2</td>\n",
       "      <td>0.324786</td>\n",
       "      <td>0.382609</td>\n",
       "      <td>0.794872</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.631579</td>\n",
       "      <td>0.289474</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      B365H  B365D  B365A  FTR      HTGS      ATGS      HTGC      ATGC  \\\n",
       "2650   1.95   3.75   3.75    0  0.393162  0.382609  0.602564  0.623377   \n",
       "2651   1.33   5.75   8.00    2  0.470085  0.434783  0.564103  0.610390   \n",
       "2652   1.75   3.75   4.50    2  0.521368  0.443478  0.230769  0.740260   \n",
       "2653  26.00  11.00   1.08    0  0.393162  0.947826  0.846154  0.376623   \n",
       "2654  13.00   8.00   1.18    0  0.384615  0.939130  0.756410  0.441558   \n",
       "2655   1.80   3.75   4.50    2  0.290598  0.382609  0.435897  0.636364   \n",
       "2656   2.00   3.60   3.70    2  0.307692  0.408696  0.923077  0.740260   \n",
       "2657   1.33   5.25   9.00    2  0.418803  0.313043  0.923077  0.870130   \n",
       "2658   5.00   3.80   1.70    2  0.273504  0.313043  0.653846  0.844156   \n",
       "2659   1.44   4.50   7.50    2  0.324786  0.382609  0.794872  0.428571   \n",
       "\n",
       "           HTP       ATP  HM1  HM2  HM3  HM4  HM5  AM1  AM2  AM3  AM4  AM5  \\\n",
       "2650  0.000000  0.078947    1    1    0    3    3    3    1    0    1    1   \n",
       "2651  0.026316  0.000000    0    3    0    1    3    1    1    3    1    0   \n",
       "2652  0.000000  0.078947    1    3    3    3    3    3    1    3    0    0   \n",
       "2653  0.078947  0.078947    3    3    1    3    1    3    3    3    3    1   \n",
       "2654  0.078947  0.078947    3    1    0    1    0    3    3    3    3    3   \n",
       "2655  0.000000  0.026316    1    3    1    0    1    0    1    3    1    0   \n",
       "2656  0.000000  0.026316    1    3    1    0    1    0    1    0    1    3   \n",
       "2657  0.000000  0.078947    1    1    1    0    3    3    1    0    1    3   \n",
       "2658  0.026316  0.026316    0    1    1    3    0    0    3    0    3    1   \n",
       "2659  0.026316  0.000000    0    3    1    3    0    1    3    0    1    1   \n",
       "\n",
       "      HTWinStreak3  HTWinStreak5  HTLossStreak3  HTLossStreak5  ATWinStreak3  \\\n",
       "2650             0             0              0              0             0   \n",
       "2651             0             0              0              0             0   \n",
       "2652             1             0              0              0             0   \n",
       "2653             0             0              0              0             0   \n",
       "2654             0             0              0              0             1   \n",
       "2655             0             0              0              0             0   \n",
       "2656             0             0              0              0             0   \n",
       "2657             0             0              0              0             0   \n",
       "2658             0             0              0              0             0   \n",
       "2659             0             0              0              0             0   \n",
       "\n",
       "      ATWinStreak5  ATLossStreak3  ATLossStreak5      HTGD      ATGD  \\\n",
       "2650             0              0              0 -0.026316 -0.105263   \n",
       "2651             0              0              0  0.289474  0.078947   \n",
       "2652             0              0              0  1.131579 -0.157895   \n",
       "2653             0              0              0 -0.526316  2.105263   \n",
       "2654             1              0              0 -0.368421  1.947368   \n",
       "2655             0              0              0  0.000000 -0.131579   \n",
       "2656             0              0              0 -0.947368 -0.263158   \n",
       "2657             0              0              0 -0.605263 -0.815789   \n",
       "2658             0              0              0 -0.500000 -0.763158   \n",
       "2659             0              0              0 -0.631579  0.289474   \n",
       "\n",
       "       DiffPts  DiffFormPts  DiffLP  \n",
       "2650 -0.078947     0.078947      -8  \n",
       "2651  0.026316     0.105263       2  \n",
       "2652 -0.078947     0.105263      -5  \n",
       "2653  0.000000    -0.078947      16  \n",
       "2654  0.000000    -0.263158      16  \n",
       "2655 -0.026316    -0.026316      -9  \n",
       "2656 -0.026316    -0.026316      -8  \n",
       "2657 -0.078947    -0.078947      -3  \n",
       "2658  0.000000    -0.078947       3  \n",
       "2659  0.026316     0.105263      12  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df1.set_index('match_api_id').join(df2.set_index('match_api_id'))\n",
    "df_train = df_train.reset_index()\n",
    "df_train = df_train.drop(columns = ['match_api_id'])\n",
    "df_train.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Test (Season 2008/2009)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>match_api_id</th>\n",
       "      <th>B365H</th>\n",
       "      <th>B365D</th>\n",
       "      <th>B365A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>530090</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.3</td>\n",
       "      <td>3.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>530023</td>\n",
       "      <td>1.70</td>\n",
       "      <td>3.6</td>\n",
       "      <td>5.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>530091</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.3</td>\n",
       "      <td>3.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>530092</td>\n",
       "      <td>1.44</td>\n",
       "      <td>4.2</td>\n",
       "      <td>7.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>530084</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.3</td>\n",
       "      <td>2.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   match_api_id  B365H  B365D  B365A\n",
       "7        530090   2.00    3.3   3.80\n",
       "0        530023   1.70    3.6   5.25\n",
       "8        530091   2.00    3.3   3.80\n",
       "9        530092   1.44    4.2   7.50\n",
       "1        530084   2.80    3.3   2.50"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = pd.read_csv(\"datasets/LaLiga_sort.csv\", index_col=0)\n",
    "df3 = df3[df3['season'] == '2008/2009']\n",
    "df3 = df3[['match_api_id','B365H', 'B365D', 'B365A']]\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>match_api_id</th>\n",
       "      <th>FTR</th>\n",
       "      <th>HTGS</th>\n",
       "      <th>ATGS</th>\n",
       "      <th>HTGC</th>\n",
       "      <th>ATGC</th>\n",
       "      <th>HTP</th>\n",
       "      <th>ATP</th>\n",
       "      <th>HM1</th>\n",
       "      <th>HM2</th>\n",
       "      <th>HM3</th>\n",
       "      <th>HM4</th>\n",
       "      <th>HM5</th>\n",
       "      <th>AM1</th>\n",
       "      <th>AM2</th>\n",
       "      <th>AM3</th>\n",
       "      <th>AM4</th>\n",
       "      <th>AM5</th>\n",
       "      <th>HTWinStreak3</th>\n",
       "      <th>HTWinStreak5</th>\n",
       "      <th>HTLossStreak3</th>\n",
       "      <th>HTLossStreak5</th>\n",
       "      <th>ATWinStreak3</th>\n",
       "      <th>ATWinStreak5</th>\n",
       "      <th>ATLossStreak3</th>\n",
       "      <th>ATLossStreak5</th>\n",
       "      <th>HTGD</th>\n",
       "      <th>ATGD</th>\n",
       "      <th>DiffPts</th>\n",
       "      <th>DiffFormPts</th>\n",
       "      <th>DiffLP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>530090</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>530023</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>530091</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>530092</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>530084</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   match_api_id  FTR  HTGS  ATGS  HTGC  ATGC  HTP  ATP  HM1  HM2  HM3  HM4  \\\n",
       "0        530090    2   0.0   0.0   0.0   0.0  0.0  0.0    2    2    2    2   \n",
       "1        530023    2   0.0   0.0   0.0   0.0  0.0  0.0    2    2    2    2   \n",
       "2        530091    0   0.0   0.0   0.0   0.0  0.0  0.0    2    2    2    2   \n",
       "3        530092    2   0.0   0.0   0.0   0.0  0.0  0.0    2    2    2    2   \n",
       "4        530084    1   0.0   0.0   0.0   0.0  0.0  0.0    2    2    2    2   \n",
       "\n",
       "   HM5  AM1  AM2  AM3  AM4  AM5  HTWinStreak3  HTWinStreak5  HTLossStreak3  \\\n",
       "0    2    2    2    2    2    2             0             0              0   \n",
       "1    2    2    2    2    2    2             0             0              0   \n",
       "2    2    2    2    2    2    2             0             0              0   \n",
       "3    2    2    2    2    2    2             0             0              0   \n",
       "4    2    2    2    2    2    2             0             0              0   \n",
       "\n",
       "   HTLossStreak5  ATWinStreak3  ATWinStreak5  ATLossStreak3  ATLossStreak5  \\\n",
       "0              0             0             0              0              0   \n",
       "1              0             0             0              0              0   \n",
       "2              0             0             0              0              0   \n",
       "3              0             0             0              0              0   \n",
       "4              0             0             0              0              0   \n",
       "\n",
       "   HTGD  ATGD  DiffPts  DiffFormPts  DiffLP  \n",
       "0   0.0   0.0      0.0          0.0      -3  \n",
       "1   0.0   0.0      0.0          0.0       3  \n",
       "2   0.0   0.0      0.0          0.0       3  \n",
       "3   0.0   0.0      0.0          0.0     -14  \n",
       "4   0.0   0.0      0.0          0.0      15  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4 = pd.read_csv(\"datasets/laliga_data_test_onehot_matchid.csv\")\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B365H</th>\n",
       "      <th>B365D</th>\n",
       "      <th>B365A</th>\n",
       "      <th>FTR</th>\n",
       "      <th>HTGS</th>\n",
       "      <th>ATGS</th>\n",
       "      <th>HTGC</th>\n",
       "      <th>ATGC</th>\n",
       "      <th>HTP</th>\n",
       "      <th>ATP</th>\n",
       "      <th>HM1</th>\n",
       "      <th>HM2</th>\n",
       "      <th>HM3</th>\n",
       "      <th>HM4</th>\n",
       "      <th>HM5</th>\n",
       "      <th>AM1</th>\n",
       "      <th>AM2</th>\n",
       "      <th>AM3</th>\n",
       "      <th>AM4</th>\n",
       "      <th>AM5</th>\n",
       "      <th>HTWinStreak3</th>\n",
       "      <th>HTWinStreak5</th>\n",
       "      <th>HTLossStreak3</th>\n",
       "      <th>HTLossStreak5</th>\n",
       "      <th>ATWinStreak3</th>\n",
       "      <th>ATWinStreak5</th>\n",
       "      <th>ATLossStreak3</th>\n",
       "      <th>ATLossStreak5</th>\n",
       "      <th>HTGD</th>\n",
       "      <th>ATGD</th>\n",
       "      <th>DiffPts</th>\n",
       "      <th>DiffFormPts</th>\n",
       "      <th>DiffLP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>1.25</td>\n",
       "      <td>5.25</td>\n",
       "      <td>13.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.658120</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.753247</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>-0.342105</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>2.00</td>\n",
       "      <td>3.60</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1</td>\n",
       "      <td>0.401709</td>\n",
       "      <td>0.904348</td>\n",
       "      <td>0.589744</td>\n",
       "      <td>0.441558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>1.842105</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>3.40</td>\n",
       "      <td>3.30</td>\n",
       "      <td>2.15</td>\n",
       "      <td>0</td>\n",
       "      <td>0.435897</td>\n",
       "      <td>0.504348</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.675325</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.157895</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>1.36</td>\n",
       "      <td>4.75</td>\n",
       "      <td>8.50</td>\n",
       "      <td>2</td>\n",
       "      <td>0.564103</td>\n",
       "      <td>0.408696</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.779221</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>-0.342105</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.078947</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>1.70</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.33</td>\n",
       "      <td>2</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.713043</td>\n",
       "      <td>0.589744</td>\n",
       "      <td>0.649351</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.184211</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>4.20</td>\n",
       "      <td>3.60</td>\n",
       "      <td>1.83</td>\n",
       "      <td>0</td>\n",
       "      <td>0.324786</td>\n",
       "      <td>0.452174</td>\n",
       "      <td>0.858974</td>\n",
       "      <td>0.506494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.763158</td>\n",
       "      <td>0.342105</td>\n",
       "      <td>-0.078947</td>\n",
       "      <td>-0.078947</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>1.91</td>\n",
       "      <td>3.60</td>\n",
       "      <td>3.80</td>\n",
       "      <td>2</td>\n",
       "      <td>0.367521</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.628205</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.157895</td>\n",
       "      <td>-0.026316</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.184211</td>\n",
       "      <td>-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>3.30</td>\n",
       "      <td>3.30</td>\n",
       "      <td>2.15</td>\n",
       "      <td>1</td>\n",
       "      <td>0.410256</td>\n",
       "      <td>0.426087</td>\n",
       "      <td>0.602564</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>-0.157895</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.026316</td>\n",
       "      <td>-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>1.83</td>\n",
       "      <td>3.60</td>\n",
       "      <td>4.20</td>\n",
       "      <td>1</td>\n",
       "      <td>0.427350</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.740260</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.184211</td>\n",
       "      <td>-0.315789</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>1.33</td>\n",
       "      <td>5.00</td>\n",
       "      <td>8.50</td>\n",
       "      <td>2</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.286957</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.868421</td>\n",
       "      <td>-0.578947</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     B365H  B365D  B365A  FTR      HTGS      ATGS      HTGC      ATGC  \\\n",
       "370   1.25   5.25  13.00    2  0.658120  0.391304  0.730769  0.753247   \n",
       "371   2.00   3.60   3.40    1  0.401709  0.904348  0.589744  0.441558   \n",
       "372   3.40   3.30   2.15    0  0.435897  0.504348  0.730769  0.675325   \n",
       "373   1.36   4.75   8.50    2  0.564103  0.408696  0.692308  0.779221   \n",
       "374   1.70   4.00   4.33    2  0.333333  0.713043  0.589744  0.649351   \n",
       "375   4.20   3.60   1.83    0  0.324786  0.452174  0.858974  0.506494   \n",
       "376   1.91   3.60   3.80    2  0.367521  0.478261  0.628205  0.727273   \n",
       "377   3.30   3.30   2.15    1  0.410256  0.426087  0.602564  0.714286   \n",
       "378   1.83   3.60   4.20    1  0.427350  0.391304  0.730769  0.740260   \n",
       "379   1.33   5.00   8.50    2  0.384615  0.286957  1.000000  0.714286   \n",
       "\n",
       "          HTP       ATP  HM1  HM2  HM3  HM4  HM5  AM1  AM2  AM3  AM4  AM5  \\\n",
       "370  0.078947  0.000000    3    3    3    3    3    1    1    3    3    3   \n",
       "371  0.000000  0.000000    1    0    3    3    0    1    1    0    3    0   \n",
       "372  0.078947  0.078947    3    3    1    3    3    3    3    0    1    3   \n",
       "373  0.000000  0.000000    1    1    3    1    0    1    1    3    0    3   \n",
       "374  0.078947  0.000000    3    0    1    1    0    1    1    1    1    3   \n",
       "375  0.000000  0.078947    1    3    0    3    1    3    0    3    3    1   \n",
       "376  0.078947  0.026316    3    3    1    3    3    0    1    3    1    0   \n",
       "377  0.078947  0.078947    3    3    1    1    1    3    0    3    1    1   \n",
       "378  0.026316  0.000000    0    3    1    1    1    1    1    0    1    0   \n",
       "379  0.078947  0.000000    3    3    1    0    1    1    1    1    3    1   \n",
       "\n",
       "     HTWinStreak3  HTWinStreak5  HTLossStreak3  HTLossStreak5  ATWinStreak3  \\\n",
       "370             1             1              0              0             1   \n",
       "371             0             0              0              0             0   \n",
       "372             0             0              0              0             0   \n",
       "373             0             0              0              0             0   \n",
       "374             0             0              0              0             0   \n",
       "375             0             0              0              0             0   \n",
       "376             0             0              0              0             0   \n",
       "377             0             0              1              0             0   \n",
       "378             0             0              1              0             0   \n",
       "379             0             0              0              0             0   \n",
       "\n",
       "     ATWinStreak5  ATLossStreak3  ATLossStreak5      HTGD      ATGD   DiffPts  \\\n",
       "370             0              0              0  0.526316 -0.342105  0.078947   \n",
       "371             0              0              0  0.026316  1.842105  0.000000   \n",
       "372             0              0              0 -0.157895  0.157895  0.000000   \n",
       "373             0              0              0  0.315789 -0.342105  0.000000   \n",
       "374             0              0              0 -0.184211  0.842105  0.078947   \n",
       "375             0              0              0 -0.763158  0.342105 -0.078947   \n",
       "376             0              0              0 -0.157895 -0.026316  0.052632   \n",
       "377             0              0              0  0.026316 -0.157895  0.000000   \n",
       "378             0              0              0 -0.184211 -0.315789  0.026316   \n",
       "379             0              0              0 -0.868421 -0.578947  0.078947   \n",
       "\n",
       "     DiffFormPts  DiffLP  \n",
       "370     0.157895      -4  \n",
       "371     0.078947       6  \n",
       "372     0.052632       5  \n",
       "373    -0.078947      -1  \n",
       "374     0.052632      16  \n",
       "375    -0.078947      13  \n",
       "376     0.184211      -6  \n",
       "377    -0.026316      -8  \n",
       "378     0.052632      -2  \n",
       "379     0.105263       0  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = df3.set_index('match_api_id').join(df4.set_index('match_api_id'))\n",
    "df_test = df_test.reset_index()\n",
    "df_test = df_test.drop(columns = ['match_api_id'])\n",
    "df_test.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2660, 32)\n",
      "(2660,)\n",
      "(380, 32)\n",
      "(380,)\n",
      "(2660, 3)\n",
      "(380, 3)\n"
     ]
    }
   ],
   "source": [
    "x_train = df_train.drop(columns = ['FTR'])\n",
    "y_train = df_train['FTR']\n",
    "x_test = df_test.drop(columns = ['FTR'])\n",
    "y_test = df_test['FTR']\n",
    "bet_train = df_train[['B365H', 'B365D', 'B365A']]\n",
    "bet_test = df_test[['B365H', 'B365D', 'B365A']]\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "print(bet_train.shape)\n",
    "print(bet_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = x_train.values.shape[1]\n",
    "columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First neural model start here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 32)]              0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 41)                1353      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 75)                3150      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 228       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 4,731\n",
      "Trainable params: 4,731\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Activation, Flatten, Dense, Dropout, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# 41-75-3\n",
    "# We first need to define the sequence of dependencies (the computational graph)\n",
    "# x = Input(shape=(20,32))\n",
    "x = Input(shape=(columns,))\n",
    "# h = Conv2D(32, (7, 7), strides=(2, 2))(x)\n",
    "# h = Activation('relu')(x)\n",
    "h = Flatten()(x)\n",
    "h = Dense(41)(h)\n",
    "h = Dense(75)(h)\n",
    "h = Dense(3)(h)\n",
    "# for i in range(10):\n",
    "#     h = Dense(75)(h)\n",
    "h = Dropout(0.3)(h)\n",
    "p = Activation('softmax')(h)\n",
    "\n",
    "# Now that we have defined how to find p from x, we can create a \n",
    "# model simply by saying what is input and what is output\n",
    "model = Model(inputs=x, outputs=p)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(lr=0.00001), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 3.1051 - accuracy: 0.3568 - val_loss: 2.7341 - val_accuracy: 0.3364\n",
      "Epoch 2/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.5437 - accuracy: 0.3729 - val_loss: 2.2448 - val_accuracy: 0.3832\n",
      "Epoch 3/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0986 - accuracy: 0.3866 - val_loss: 1.9073 - val_accuracy: 0.4019\n",
      "Epoch 4/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.8287 - accuracy: 0.3984 - val_loss: 1.6796 - val_accuracy: 0.4206\n",
      "Epoch 5/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6482 - accuracy: 0.3968 - val_loss: 1.5078 - val_accuracy: 0.4112\n",
      "Epoch 6/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.4530 - accuracy: 0.3905 - val_loss: 1.3882 - val_accuracy: 0.3925\n",
      "Epoch 7/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.3675 - accuracy: 0.3976 - val_loss: 1.3059 - val_accuracy: 0.3925\n",
      "Epoch 8/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.2900 - accuracy: 0.4089 - val_loss: 1.2549 - val_accuracy: 0.4019\n",
      "Epoch 9/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.2308 - accuracy: 0.3913 - val_loss: 1.2222 - val_accuracy: 0.4019\n",
      "Epoch 10/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1939 - accuracy: 0.4203 - val_loss: 1.2035 - val_accuracy: 0.4112\n",
      "Epoch 11/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1573 - accuracy: 0.4219 - val_loss: 1.1877 - val_accuracy: 0.4299\n",
      "Epoch 12/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1499 - accuracy: 0.4222 - val_loss: 1.1743 - val_accuracy: 0.4486\n",
      "Epoch 13/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1408 - accuracy: 0.4250 - val_loss: 1.1604 - val_accuracy: 0.4579\n",
      "Epoch 14/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1212 - accuracy: 0.4379 - val_loss: 1.1492 - val_accuracy: 0.4579\n",
      "Epoch 15/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1065 - accuracy: 0.4367 - val_loss: 1.1389 - val_accuracy: 0.4579\n",
      "Epoch 16/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0982 - accuracy: 0.4493 - val_loss: 1.1286 - val_accuracy: 0.4579\n",
      "Epoch 17/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0851 - accuracy: 0.4665 - val_loss: 1.1197 - val_accuracy: 0.4673\n",
      "Epoch 18/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 1.0862 - accuracy: 0.4481 - val_loss: 1.1132 - val_accuracy: 0.4673\n",
      "Epoch 19/200\n",
      "160/160 [==============================] - 1s 3ms/step - loss: 1.0702 - accuracy: 0.4587 - val_loss: 1.1064 - val_accuracy: 0.4673\n",
      "Epoch 20/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0873 - accuracy: 0.4497 - val_loss: 1.1002 - val_accuracy: 0.4860\n",
      "Epoch 21/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0695 - accuracy: 0.4532 - val_loss: 1.0935 - val_accuracy: 0.4766\n",
      "Epoch 22/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0579 - accuracy: 0.4544 - val_loss: 1.0856 - val_accuracy: 0.4766\n",
      "Epoch 23/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0638 - accuracy: 0.4508 - val_loss: 1.0800 - val_accuracy: 0.4766\n",
      "Epoch 24/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0600 - accuracy: 0.4524 - val_loss: 1.0721 - val_accuracy: 0.4766\n",
      "Epoch 25/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0456 - accuracy: 0.4563 - val_loss: 1.0674 - val_accuracy: 0.4860\n",
      "Epoch 26/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0509 - accuracy: 0.4528 - val_loss: 1.0620 - val_accuracy: 0.5047\n",
      "Epoch 27/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0401 - accuracy: 0.4610 - val_loss: 1.0560 - val_accuracy: 0.5140\n",
      "Epoch 28/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0351 - accuracy: 0.4552 - val_loss: 1.0509 - val_accuracy: 0.5140\n",
      "Epoch 29/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0301 - accuracy: 0.4681 - val_loss: 1.0477 - val_accuracy: 0.5140\n",
      "Epoch 30/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0258 - accuracy: 0.4591 - val_loss: 1.0441 - val_accuracy: 0.5140\n",
      "Epoch 31/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0177 - accuracy: 0.4606 - val_loss: 1.0412 - val_accuracy: 0.5140\n",
      "Epoch 32/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0275 - accuracy: 0.4630 - val_loss: 1.0364 - val_accuracy: 0.5140\n",
      "Epoch 33/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0157 - accuracy: 0.4681 - val_loss: 1.0342 - val_accuracy: 0.5234\n",
      "Epoch 34/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0085 - accuracy: 0.4790 - val_loss: 1.0292 - val_accuracy: 0.5234\n",
      "Epoch 35/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0166 - accuracy: 0.4622 - val_loss: 1.0287 - val_accuracy: 0.5234\n",
      "Epoch 36/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0127 - accuracy: 0.4736 - val_loss: 1.0253 - val_accuracy: 0.5327\n",
      "Epoch 37/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0137 - accuracy: 0.4673 - val_loss: 1.0205 - val_accuracy: 0.5234\n",
      "Epoch 38/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0239 - accuracy: 0.4743 - val_loss: 1.0202 - val_accuracy: 0.5327\n",
      "Epoch 39/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0010 - accuracy: 0.4693 - val_loss: 1.0154 - val_accuracy: 0.5421\n",
      "Epoch 40/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0111 - accuracy: 0.4689 - val_loss: 1.0142 - val_accuracy: 0.5327\n",
      "Epoch 41/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9998 - accuracy: 0.4716 - val_loss: 1.0112 - val_accuracy: 0.5327\n",
      "Epoch 42/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0092 - accuracy: 0.4630 - val_loss: 1.0095 - val_accuracy: 0.5327\n",
      "Epoch 43/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9866 - accuracy: 0.4857 - val_loss: 1.0070 - val_accuracy: 0.5421\n",
      "Epoch 44/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9975 - accuracy: 0.4849 - val_loss: 1.0061 - val_accuracy: 0.5327\n",
      "Epoch 45/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9977 - accuracy: 0.4830 - val_loss: 1.0036 - val_accuracy: 0.5234\n",
      "Epoch 46/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9980 - accuracy: 0.4830 - val_loss: 1.0026 - val_accuracy: 0.5140\n",
      "Epoch 47/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0018 - accuracy: 0.4759 - val_loss: 1.0009 - val_accuracy: 0.5140\n",
      "Epoch 48/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9913 - accuracy: 0.4802 - val_loss: 0.9973 - val_accuracy: 0.5327\n",
      "Epoch 49/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9925 - accuracy: 0.4892 - val_loss: 0.9936 - val_accuracy: 0.5514\n",
      "Epoch 50/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9887 - accuracy: 0.4990 - val_loss: 0.9932 - val_accuracy: 0.5421\n",
      "Epoch 51/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9853 - accuracy: 0.4869 - val_loss: 0.9939 - val_accuracy: 0.5234\n",
      "Epoch 52/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9824 - accuracy: 0.4881 - val_loss: 0.9921 - val_accuracy: 0.5234\n",
      "Epoch 53/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9909 - accuracy: 0.4826 - val_loss: 0.9911 - val_accuracy: 0.5234\n",
      "Epoch 54/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9766 - accuracy: 0.4877 - val_loss: 0.9904 - val_accuracy: 0.5140\n",
      "Epoch 55/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9817 - accuracy: 0.4931 - val_loss: 0.9884 - val_accuracy: 0.5234\n",
      "Epoch 56/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9838 - accuracy: 0.4837 - val_loss: 0.9869 - val_accuracy: 0.5234\n",
      "Epoch 57/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9883 - accuracy: 0.4861 - val_loss: 0.9855 - val_accuracy: 0.5140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9752 - accuracy: 0.4834 - val_loss: 0.9850 - val_accuracy: 0.5140\n",
      "Epoch 59/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9892 - accuracy: 0.4841 - val_loss: 0.9848 - val_accuracy: 0.5047\n",
      "Epoch 60/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9785 - accuracy: 0.4884 - val_loss: 0.9848 - val_accuracy: 0.5047\n",
      "Epoch 61/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9864 - accuracy: 0.4869 - val_loss: 0.9811 - val_accuracy: 0.5234\n",
      "Epoch 62/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9799 - accuracy: 0.4884 - val_loss: 0.9787 - val_accuracy: 0.5140\n",
      "Epoch 63/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9748 - accuracy: 0.4986 - val_loss: 0.9791 - val_accuracy: 0.5234\n",
      "Epoch 64/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9817 - accuracy: 0.4920 - val_loss: 0.9773 - val_accuracy: 0.5327\n",
      "Epoch 65/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9805 - accuracy: 0.4908 - val_loss: 0.9776 - val_accuracy: 0.5234\n",
      "Epoch 66/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9707 - accuracy: 0.5119 - val_loss: 0.9746 - val_accuracy: 0.5327\n",
      "Epoch 67/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9794 - accuracy: 0.5072 - val_loss: 0.9761 - val_accuracy: 0.5140\n",
      "Epoch 68/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9785 - accuracy: 0.4986 - val_loss: 0.9736 - val_accuracy: 0.5234\n",
      "Epoch 69/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9730 - accuracy: 0.5006 - val_loss: 0.9745 - val_accuracy: 0.5234\n",
      "Epoch 70/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9665 - accuracy: 0.5002 - val_loss: 0.9727 - val_accuracy: 0.5140\n",
      "Epoch 71/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9779 - accuracy: 0.5029 - val_loss: 0.9741 - val_accuracy: 0.5140\n",
      "Epoch 72/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9760 - accuracy: 0.4998 - val_loss: 0.9716 - val_accuracy: 0.5140\n",
      "Epoch 73/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9718 - accuracy: 0.4971 - val_loss: 0.9707 - val_accuracy: 0.5047\n",
      "Epoch 74/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9717 - accuracy: 0.4920 - val_loss: 0.9702 - val_accuracy: 0.5140\n",
      "Epoch 75/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9665 - accuracy: 0.4990 - val_loss: 0.9681 - val_accuracy: 0.5140\n",
      "Epoch 76/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9641 - accuracy: 0.5080 - val_loss: 0.9670 - val_accuracy: 0.5140\n",
      "Epoch 77/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9738 - accuracy: 0.5096 - val_loss: 0.9673 - val_accuracy: 0.5140\n",
      "Epoch 78/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9763 - accuracy: 0.5096 - val_loss: 0.9682 - val_accuracy: 0.5140\n",
      "Epoch 79/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9769 - accuracy: 0.5072 - val_loss: 0.9674 - val_accuracy: 0.5140\n",
      "Epoch 80/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9808 - accuracy: 0.5014 - val_loss: 0.9674 - val_accuracy: 0.5234\n",
      "Epoch 81/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9723 - accuracy: 0.5092 - val_loss: 0.9673 - val_accuracy: 0.5234\n",
      "Epoch 82/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9727 - accuracy: 0.4971 - val_loss: 0.9667 - val_accuracy: 0.5327\n",
      "Epoch 83/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9657 - accuracy: 0.5135 - val_loss: 0.9657 - val_accuracy: 0.5140\n",
      "Epoch 84/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9745 - accuracy: 0.5049 - val_loss: 0.9656 - val_accuracy: 0.5140\n",
      "Epoch 85/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9639 - accuracy: 0.5092 - val_loss: 0.9641 - val_accuracy: 0.5047\n",
      "Epoch 86/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9707 - accuracy: 0.5112 - val_loss: 0.9630 - val_accuracy: 0.5047\n",
      "Epoch 87/200\n",
      "160/160 [==============================] - 1s 3ms/step - loss: 0.9693 - accuracy: 0.5010 - val_loss: 0.9632 - val_accuracy: 0.5140\n",
      "Epoch 88/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9760 - accuracy: 0.4982 - val_loss: 0.9641 - val_accuracy: 0.5234\n",
      "Epoch 89/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9709 - accuracy: 0.4896 - val_loss: 0.9618 - val_accuracy: 0.5234\n",
      "Epoch 90/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9753 - accuracy: 0.5057 - val_loss: 0.9628 - val_accuracy: 0.5234\n",
      "Epoch 91/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9665 - accuracy: 0.5025 - val_loss: 0.9629 - val_accuracy: 0.5234\n",
      "Epoch 92/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9701 - accuracy: 0.5037 - val_loss: 0.9613 - val_accuracy: 0.5234\n",
      "Epoch 93/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9696 - accuracy: 0.4990 - val_loss: 0.9612 - val_accuracy: 0.5327\n",
      "Epoch 94/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9669 - accuracy: 0.5033 - val_loss: 0.9617 - val_accuracy: 0.5234\n",
      "Epoch 95/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9675 - accuracy: 0.5037 - val_loss: 0.9605 - val_accuracy: 0.5234\n",
      "Epoch 96/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9690 - accuracy: 0.5100 - val_loss: 0.9604 - val_accuracy: 0.5234\n",
      "Epoch 97/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9660 - accuracy: 0.5022 - val_loss: 0.9589 - val_accuracy: 0.5234\n",
      "Epoch 98/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9640 - accuracy: 0.5014 - val_loss: 0.9595 - val_accuracy: 0.5140\n",
      "Epoch 99/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9668 - accuracy: 0.5025 - val_loss: 0.9598 - val_accuracy: 0.5234\n",
      "Epoch 100/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9561 - accuracy: 0.5163 - val_loss: 0.9598 - val_accuracy: 0.5421\n",
      "Epoch 101/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9557 - accuracy: 0.5139 - val_loss: 0.9594 - val_accuracy: 0.5421\n",
      "Epoch 102/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9627 - accuracy: 0.4998 - val_loss: 0.9595 - val_accuracy: 0.5234\n",
      "Epoch 103/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9592 - accuracy: 0.5139 - val_loss: 0.9617 - val_accuracy: 0.5234\n",
      "Epoch 104/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9660 - accuracy: 0.5112 - val_loss: 0.9603 - val_accuracy: 0.5327\n",
      "Epoch 105/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9620 - accuracy: 0.5057 - val_loss: 0.9613 - val_accuracy: 0.5234\n",
      "Epoch 106/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9701 - accuracy: 0.4982 - val_loss: 0.9597 - val_accuracy: 0.5327\n",
      "Epoch 107/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9582 - accuracy: 0.5029 - val_loss: 0.9602 - val_accuracy: 0.5327\n",
      "Epoch 108/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9603 - accuracy: 0.5151 - val_loss: 0.9574 - val_accuracy: 0.5421\n",
      "Epoch 109/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9633 - accuracy: 0.5088 - val_loss: 0.9576 - val_accuracy: 0.5327\n",
      "Epoch 110/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9703 - accuracy: 0.5025 - val_loss: 0.9590 - val_accuracy: 0.5327\n",
      "Epoch 111/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9695 - accuracy: 0.4982 - val_loss: 0.9579 - val_accuracy: 0.5327\n",
      "Epoch 112/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9633 - accuracy: 0.5076 - val_loss: 0.9585 - val_accuracy: 0.5327\n",
      "Epoch 113/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9700 - accuracy: 0.4998 - val_loss: 0.9578 - val_accuracy: 0.5327\n",
      "Epoch 114/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9638 - accuracy: 0.5053 - val_loss: 0.9570 - val_accuracy: 0.5421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9576 - accuracy: 0.5053 - val_loss: 0.9578 - val_accuracy: 0.5421\n",
      "Epoch 116/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9620 - accuracy: 0.5135 - val_loss: 0.9578 - val_accuracy: 0.5421\n",
      "Epoch 117/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9625 - accuracy: 0.5100 - val_loss: 0.9561 - val_accuracy: 0.5327\n",
      "Epoch 118/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9546 - accuracy: 0.5135 - val_loss: 0.9546 - val_accuracy: 0.5327\n",
      "Epoch 119/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9697 - accuracy: 0.5022 - val_loss: 0.9543 - val_accuracy: 0.5421\n",
      "Epoch 120/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9603 - accuracy: 0.5119 - val_loss: 0.9545 - val_accuracy: 0.5327\n",
      "Epoch 121/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9527 - accuracy: 0.5116 - val_loss: 0.9567 - val_accuracy: 0.5421\n",
      "Epoch 122/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9608 - accuracy: 0.5018 - val_loss: 0.9595 - val_accuracy: 0.5327\n",
      "Epoch 123/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9669 - accuracy: 0.5104 - val_loss: 0.9543 - val_accuracy: 0.5421\n",
      "Epoch 124/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9698 - accuracy: 0.5076 - val_loss: 0.9529 - val_accuracy: 0.5327\n",
      "Epoch 125/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9555 - accuracy: 0.5119 - val_loss: 0.9543 - val_accuracy: 0.5327\n",
      "Epoch 126/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9623 - accuracy: 0.5151 - val_loss: 0.9546 - val_accuracy: 0.5327\n",
      "Epoch 127/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9648 - accuracy: 0.5096 - val_loss: 0.9542 - val_accuracy: 0.5421\n",
      "Epoch 128/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9559 - accuracy: 0.5049 - val_loss: 0.9523 - val_accuracy: 0.5327\n",
      "Epoch 129/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9608 - accuracy: 0.5155 - val_loss: 0.9494 - val_accuracy: 0.5514\n",
      "Epoch 130/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9625 - accuracy: 0.5155 - val_loss: 0.9517 - val_accuracy: 0.5421\n",
      "Epoch 131/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9583 - accuracy: 0.5225 - val_loss: 0.9506 - val_accuracy: 0.5514\n",
      "Epoch 132/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9656 - accuracy: 0.5010 - val_loss: 0.9526 - val_accuracy: 0.5327\n",
      "Epoch 133/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9709 - accuracy: 0.4967 - val_loss: 0.9548 - val_accuracy: 0.5327\n",
      "Epoch 134/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9680 - accuracy: 0.5037 - val_loss: 0.9549 - val_accuracy: 0.5327\n",
      "Epoch 135/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9633 - accuracy: 0.5049 - val_loss: 0.9545 - val_accuracy: 0.5234\n",
      "Epoch 136/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9748 - accuracy: 0.4986 - val_loss: 0.9531 - val_accuracy: 0.5327\n",
      "Epoch 137/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9611 - accuracy: 0.5080 - val_loss: 0.9531 - val_accuracy: 0.5514\n",
      "Epoch 138/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9579 - accuracy: 0.5112 - val_loss: 0.9543 - val_accuracy: 0.5234\n",
      "Epoch 139/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9525 - accuracy: 0.5166 - val_loss: 0.9521 - val_accuracy: 0.5421\n",
      "Epoch 140/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9545 - accuracy: 0.5155 - val_loss: 0.9531 - val_accuracy: 0.5514\n",
      "Epoch 141/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9679 - accuracy: 0.5022 - val_loss: 0.9527 - val_accuracy: 0.5607\n",
      "Epoch 142/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9676 - accuracy: 0.5014 - val_loss: 0.9500 - val_accuracy: 0.5421\n",
      "Epoch 143/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9589 - accuracy: 0.5135 - val_loss: 0.9522 - val_accuracy: 0.5514\n",
      "Epoch 144/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9640 - accuracy: 0.5069 - val_loss: 0.9515 - val_accuracy: 0.5421\n",
      "Epoch 145/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9599 - accuracy: 0.5217 - val_loss: 0.9537 - val_accuracy: 0.5327\n",
      "Epoch 146/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9601 - accuracy: 0.5112 - val_loss: 0.9519 - val_accuracy: 0.5421\n",
      "Epoch 147/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9574 - accuracy: 0.5131 - val_loss: 0.9527 - val_accuracy: 0.5421\n",
      "Epoch 148/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9686 - accuracy: 0.5033 - val_loss: 0.9518 - val_accuracy: 0.5421\n",
      "Epoch 149/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9591 - accuracy: 0.5022 - val_loss: 0.9539 - val_accuracy: 0.5421\n",
      "Epoch 150/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9497 - accuracy: 0.5206 - val_loss: 0.9504 - val_accuracy: 0.5327\n",
      "Epoch 151/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9589 - accuracy: 0.5061 - val_loss: 0.9523 - val_accuracy: 0.5514\n",
      "Epoch 152/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9621 - accuracy: 0.5112 - val_loss: 0.9487 - val_accuracy: 0.5514\n",
      "Epoch 153/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9579 - accuracy: 0.5100 - val_loss: 0.9518 - val_accuracy: 0.5607\n",
      "Epoch 154/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9635 - accuracy: 0.5037 - val_loss: 0.9537 - val_accuracy: 0.5607\n",
      "Epoch 155/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9605 - accuracy: 0.5131 - val_loss: 0.9493 - val_accuracy: 0.5607\n",
      "Epoch 156/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9542 - accuracy: 0.5166 - val_loss: 0.9512 - val_accuracy: 0.5607\n",
      "Epoch 157/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9528 - accuracy: 0.5159 - val_loss: 0.9503 - val_accuracy: 0.5607\n",
      "Epoch 158/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9660 - accuracy: 0.5057 - val_loss: 0.9513 - val_accuracy: 0.5421\n",
      "Epoch 159/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9609 - accuracy: 0.5166 - val_loss: 0.9477 - val_accuracy: 0.5607\n",
      "Epoch 160/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9556 - accuracy: 0.5213 - val_loss: 0.9480 - val_accuracy: 0.5514\n",
      "Epoch 161/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9602 - accuracy: 0.5190 - val_loss: 0.9490 - val_accuracy: 0.5607\n",
      "Epoch 162/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9571 - accuracy: 0.5135 - val_loss: 0.9503 - val_accuracy: 0.5514\n",
      "Epoch 163/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9649 - accuracy: 0.5080 - val_loss: 0.9513 - val_accuracy: 0.5701\n",
      "Epoch 164/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9617 - accuracy: 0.5108 - val_loss: 0.9510 - val_accuracy: 0.5701\n",
      "Epoch 165/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9600 - accuracy: 0.5159 - val_loss: 0.9481 - val_accuracy: 0.5514\n",
      "Epoch 166/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9607 - accuracy: 0.5104 - val_loss: 0.9471 - val_accuracy: 0.5888\n",
      "Epoch 167/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9618 - accuracy: 0.5096 - val_loss: 0.9494 - val_accuracy: 0.5701\n",
      "Epoch 168/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9508 - accuracy: 0.5127 - val_loss: 0.9487 - val_accuracy: 0.5701\n",
      "Epoch 169/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9558 - accuracy: 0.5096 - val_loss: 0.9501 - val_accuracy: 0.5421\n",
      "Epoch 170/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9568 - accuracy: 0.5210 - val_loss: 0.9486 - val_accuracy: 0.5607\n",
      "Epoch 171/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9690 - accuracy: 0.5147 - val_loss: 0.9465 - val_accuracy: 0.5607\n",
      "Epoch 172/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9644 - accuracy: 0.5100 - val_loss: 0.9461 - val_accuracy: 0.5607\n",
      "Epoch 173/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9560 - accuracy: 0.5147 - val_loss: 0.9486 - val_accuracy: 0.5701\n",
      "Epoch 174/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9670 - accuracy: 0.5061 - val_loss: 0.9502 - val_accuracy: 0.5701\n",
      "Epoch 175/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9603 - accuracy: 0.5092 - val_loss: 0.9516 - val_accuracy: 0.5701\n",
      "Epoch 176/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9686 - accuracy: 0.5108 - val_loss: 0.9479 - val_accuracy: 0.5701\n",
      "Epoch 177/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9634 - accuracy: 0.5100 - val_loss: 0.9500 - val_accuracy: 0.5421\n",
      "Epoch 178/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9637 - accuracy: 0.5174 - val_loss: 0.9498 - val_accuracy: 0.5607\n",
      "Epoch 179/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9558 - accuracy: 0.5241 - val_loss: 0.9479 - val_accuracy: 0.5701\n",
      "Epoch 180/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9620 - accuracy: 0.5029 - val_loss: 0.9504 - val_accuracy: 0.5794\n",
      "Epoch 181/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9572 - accuracy: 0.5194 - val_loss: 0.9515 - val_accuracy: 0.5514\n",
      "Epoch 182/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9563 - accuracy: 0.5182 - val_loss: 0.9486 - val_accuracy: 0.5794\n",
      "Epoch 183/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9616 - accuracy: 0.5061 - val_loss: 0.9485 - val_accuracy: 0.5701\n",
      "Epoch 184/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9585 - accuracy: 0.5151 - val_loss: 0.9492 - val_accuracy: 0.5701\n",
      "Epoch 185/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9618 - accuracy: 0.5104 - val_loss: 0.9484 - val_accuracy: 0.5794\n",
      "Epoch 186/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9603 - accuracy: 0.5225 - val_loss: 0.9484 - val_accuracy: 0.5794\n",
      "Epoch 187/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9616 - accuracy: 0.5104 - val_loss: 0.9480 - val_accuracy: 0.5607\n",
      "Epoch 188/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9611 - accuracy: 0.5104 - val_loss: 0.9478 - val_accuracy: 0.5794\n",
      "Epoch 189/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9577 - accuracy: 0.5272 - val_loss: 0.9474 - val_accuracy: 0.5794\n",
      "Epoch 190/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.9522 - accuracy: 0.5276 - val_loss: 0.9504 - val_accuracy: 0.5794\n",
      "Epoch 191/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9620 - accuracy: 0.5143 - val_loss: 0.9498 - val_accuracy: 0.5794\n",
      "Epoch 192/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9647 - accuracy: 0.5053 - val_loss: 0.9515 - val_accuracy: 0.5701\n",
      "Epoch 193/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9636 - accuracy: 0.5096 - val_loss: 0.9485 - val_accuracy: 0.5888\n",
      "Epoch 194/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9611 - accuracy: 0.5253 - val_loss: 0.9470 - val_accuracy: 0.5701\n",
      "Epoch 195/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9668 - accuracy: 0.4971 - val_loss: 0.9483 - val_accuracy: 0.5794\n",
      "Epoch 196/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9619 - accuracy: 0.5096 - val_loss: 0.9488 - val_accuracy: 0.5794\n",
      "Epoch 197/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9606 - accuracy: 0.5072 - val_loss: 0.9515 - val_accuracy: 0.5701\n",
      "Epoch 198/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9668 - accuracy: 0.5159 - val_loss: 0.9502 - val_accuracy: 0.5794\n",
      "Epoch 199/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9585 - accuracy: 0.5194 - val_loss: 0.9493 - val_accuracy: 0.5701\n",
      "Epoch 200/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9603 - accuracy: 0.5151 - val_loss: 0.9498 - val_accuracy: 0.5794\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, batch_size=16, epochs=200, validation_split=0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'], label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.5473684210526316\n"
     ]
    }
   ],
   "source": [
    "predictions = np.argmax(model.predict(x_test), axis=1)\n",
    "accuracy = (predictions == y_test.values).sum() / y_test.values.shape[0]\n",
    "print(\"Test set accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Betting Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agencies = ['B365', 'BW', 'IW', 'LB', 'WH', 'SJ', 'VC', 'GB', 'BS']\n",
    "agencies = ['B365']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set size (380,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Test set size\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1,2,np.nan]\n",
    "np.nansum(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://github.com/raedovj/NN19_Project_Football/blob/master/ModelTester.py\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "def predict_always_on_one_thing_benefit(labels, betting_odds, predictable_value):\n",
    "    predictable_indices = np.zeros((labels.shape[0], 3))\n",
    "    predictable_indices[:, predictable_value] = 1\n",
    "    \n",
    "    agency = \"B365\"\n",
    "    odds = pd.concat([betting_odds[agency+'H'], betting_odds[agency+'D'], betting_odds[agency+'A']], axis=1)\n",
    "    # r holds betting results. 0 indicates loss, value otherwise shows the win amount\n",
    "    r = odds * predictable_indices * to_categorical(labels)\n",
    "    # Take max value of win, draw, other win. \n",
    "    r = r.values.max(axis=1)\n",
    "    # Let's say we bet 1. then our profit(or loss) would be = earnings - 1 per bet\n",
    "    r -= len(predictable_value)\n",
    "    r\n",
    "\n",
    "#     print(\"Agency %s, \\twin amount: %.2f\" % (agency, r.sum()))\n",
    "    print(\"Agency %s, \\twin amount: %.2f\" % (agency, np.nansum(r)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agency B365, \twin amount: 475.51\n",
      "Agency B365, \twin amount: -37.26\n"
     ]
    }
   ],
   "source": [
    "## Profit for Away Wins Only \n",
    "predict_always_on_one_thing_benefit(y_train, bet_train, [0]) #Data Train\n",
    "predict_always_on_one_thing_benefit(y_test, bet_test, [0]) #Data Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agency B365, \twin amount: -345.36\n",
      "Agency B365, \twin amount: -92.20\n"
     ]
    }
   ],
   "source": [
    "## Profit for Draw Only \n",
    "predict_always_on_one_thing_benefit(y_train, bet_train, [1]) #Data Train\n",
    "predict_always_on_one_thing_benefit(y_test, bet_test, [1]) #Data Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agency B365, \twin amount: 6797.98\n",
      "Agency B365, \twin amount: 582.66\n"
     ]
    }
   ],
   "source": [
    "## Profit for Home Wins Only \n",
    "predict_always_on_one_thing_benefit(y_train, bet_train, [2]) #Data Train\n",
    "predict_always_on_one_thing_benefit(y_test, bet_test, [2]) #Data Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agency B365, \twin amount: 7273.49\n",
      "Agency B365, \twin amount: 545.40\n"
     ]
    }
   ],
   "source": [
    "## Profit for Home or Away \n",
    "predict_always_on_one_thing_benefit(y_train, bet_train, [0, 2]) #Data Train\n",
    "predict_always_on_one_thing_benefit(y_test, bet_test, [0, 2]) #Data Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agency B365, \twin amount: 6928.13\n",
      "Agency B365, \twin amount: 453.20\n"
     ]
    }
   ],
   "source": [
    "## Profit for All Possibilities\n",
    "predict_always_on_one_thing_benefit(y_train, bet_train, [0, 1, 2]) #Data Train\n",
    "predict_always_on_one_thing_benefit(y_test, bet_test, [0, 1, 2]) #Data Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions_3x1 = model.predict(x_train)\n",
    "test_predictions_3x1 = model.predict(x_test)\n",
    "train_predictions = np.argmax(train_predictions_3x1 , axis=1)\n",
    "test_predictions = np.argmax(test_predictions_3x1 , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2660, 3)\n",
      "(380, 3)\n"
     ]
    }
   ],
   "source": [
    "# train_predictions\n",
    "# test_predictions\n",
    "print(train_predictions_3x1.shape)\n",
    "print(test_predictions_3x1.shape)\n",
    "# x_train.shape\n",
    "# x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def always_bet_predicted_winner_profit(predictions, labels, betting_odds):      \n",
    "    predictions_categorical = to_categorical(predictions)\n",
    "    \n",
    "    agency = \"B365\"\n",
    "    odds = pd.concat([betting_odds[agency+'H'], betting_odds[agency+'D'], betting_odds[agency+'A']], axis=1)\n",
    "    # r holds betting results. 0 indicates loss, value otherwise shows the win amount\n",
    "    r = odds * predictions_categorical * to_categorical(labels)\n",
    "    # Take max value of win, draw, other win. \n",
    "    r = r.values.max(axis=1)\n",
    "    # Let's say we bet 1. then our profit(or loss) would be = earnings - 1\n",
    "    r -= 1\n",
    "    \n",
    "#     print(\"Agency %s, \\twin amount: %.2f\" % (agency, r.sum()))\n",
    "    print(\"Agency %s, \\twin amount: %.2f\" % (agency, np.nansum(r)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agency B365, \twin amount: 8815.84\n",
      "Agency B365, \twin amount: 731.20\n"
     ]
    }
   ],
   "source": [
    "always_bet_predicted_winner_profit(train_predictions, y_train, bet_train)\n",
    "always_bet_predicted_winner_profit(test_predictions, y_test, bet_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bet_predicted_winner_with_threshold_profit(predictions_3x1, predictions, labels, \n",
    "                                                          betting_odds, threshold):\n",
    "    predictions_categorical = to_categorical(predictions)\n",
    "    \n",
    "    agency = \"B365\"\n",
    "    odds = pd.concat([betting_odds[agency+'H'], betting_odds[agency+'D'], betting_odds[agency+'A']], axis=1)\n",
    "    # r holds betting results. 0 indicates loss, value otherwise shows the win amount\n",
    "    bet = odds * predictions_categorical * predictions_3x1\n",
    "    bet = bet > threshold\n",
    "    r = odds * predictions_categorical * to_categorical(labels)\n",
    "    r -= 1\n",
    "    # Set win/lose amount to 0 on matched it didn't bet\n",
    "    r[np.invert(bet)] = 0\n",
    "    # Take max value of win, draw, other win. \n",
    "    r = r.values.max(axis=1)\n",
    "\n",
    "    skip_percentage = (r==0).sum() / r.shape[0] * 100   \n",
    "    print(\"Agency %s, \\twin amount: %.2f. Didn't bet on %.2f%% of matches\" % (agency, r.sum(), skip_percentage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agency B365, \twin amount: 9915.12. Didn't bet on 45.30% of matches\n",
      "Agency B365, \twin amount: 885.91. Didn't bet on 48.16% of matches\n"
     ]
    }
   ],
   "source": [
    "bet_predicted_winner_with_threshold_profit(train_predictions_3x1, train_predictions, y_train, bet_train, threshold=1)\n",
    "bet_predicted_winner_with_threshold_profit(test_predictions_3x1, test_predictions, y_test, bet_test, threshold=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_highest_return(predictions_3x1, labels, betting_odds, threshold):\n",
    "        agency = \"B365\"\n",
    "        odds = pd.concat([betting_odds[agency+'H'], betting_odds[agency+'D'], betting_odds[agency+'A']], axis=1)\n",
    "        # Expected earning value. Basically expects that our NN predicts real match outcomes\n",
    "        expected = (odds * predictions_3x1).values\n",
    "\n",
    "        # Threshold matches, when we'd actually would make a bet. If expected yield is too low, it'll pass\n",
    "        bet = np.max(expected > threshold, axis=1)\n",
    "\n",
    "        # Take the highest yield of [home win, draw, other win]\n",
    "        r = np.argmax(expected, axis=1) \n",
    "\n",
    "        # Calculate wins/losses according to real match results\n",
    "        r = to_categorical(r) * to_categorical(labels)\n",
    "        r -= 1 # subtract our input bet\n",
    "\n",
    "        # Calculate earnings\n",
    "        r = r.max(axis=1) # Take max value of win, draw, other win. \n",
    "        r[np.invert(bet)] = 0 # Set win/lose amount to 0 on matched it didn't bet\n",
    "\n",
    "        skip_percentage = (bet==0).sum() / bet.shape[0] * 100   \n",
    "        print(\"Agency %s, \\twin amount: %.2f. Didn't bet on %.2f%% of matches\" % (agency, r.sum(), skip_percentage))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agency B365, \twin amount: -270.00. Didn't bet on 61.39% of matches\n",
      "Agency B365, \twin amount: -29.00. Didn't bet on 72.89% of matches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in greater\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "predict_on_highest_return(train_predictions_3x1, y_train, bet_train, threshold=2.5)\n",
    "predict_on_highest_return(test_predictions_3x1, y_test, bet_test, threshold=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
