{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "# making all pandas columns visable with display command\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Train (Season 2009/2010 until Season 2010/2011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>match_api_id</th>\n",
       "      <th>B365H</th>\n",
       "      <th>B365D</th>\n",
       "      <th>B365A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>658575</td>\n",
       "      <td>1.67</td>\n",
       "      <td>3.60</td>\n",
       "      <td>5.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>658578</td>\n",
       "      <td>3.60</td>\n",
       "      <td>3.25</td>\n",
       "      <td>2.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>658579</td>\n",
       "      <td>2.25</td>\n",
       "      <td>3.25</td>\n",
       "      <td>3.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>658582</td>\n",
       "      <td>1.17</td>\n",
       "      <td>6.50</td>\n",
       "      <td>21.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>658576</td>\n",
       "      <td>3.20</td>\n",
       "      <td>3.25</td>\n",
       "      <td>2.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     match_api_id  B365H  B365D  B365A\n",
       "382        658575   1.67   3.60   5.50\n",
       "385        658578   3.60   3.25   2.10\n",
       "386        658579   2.25   3.25   3.25\n",
       "389        658582   1.17   6.50  21.00\n",
       "383        658576   3.20   3.25   2.30"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv(\"datasets/EPL_sort.csv\", index_col=0)\n",
    "df1 = df1[df1['season'] != '2008/2009']\n",
    "df1 = df1[['match_api_id','B365H', 'B365D', 'B365A']]\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>match_api_id</th>\n",
       "      <th>FTR</th>\n",
       "      <th>HTGS</th>\n",
       "      <th>ATGS</th>\n",
       "      <th>HTGC</th>\n",
       "      <th>ATGC</th>\n",
       "      <th>HTP</th>\n",
       "      <th>ATP</th>\n",
       "      <th>HM1</th>\n",
       "      <th>HM2</th>\n",
       "      <th>HM3</th>\n",
       "      <th>HM4</th>\n",
       "      <th>HM5</th>\n",
       "      <th>AM1</th>\n",
       "      <th>AM2</th>\n",
       "      <th>AM3</th>\n",
       "      <th>AM4</th>\n",
       "      <th>AM5</th>\n",
       "      <th>HTWinStreak3</th>\n",
       "      <th>HTWinStreak5</th>\n",
       "      <th>HTLossStreak3</th>\n",
       "      <th>HTLossStreak5</th>\n",
       "      <th>ATWinStreak3</th>\n",
       "      <th>ATWinStreak5</th>\n",
       "      <th>ATLossStreak3</th>\n",
       "      <th>ATLossStreak5</th>\n",
       "      <th>HTGD</th>\n",
       "      <th>ATGD</th>\n",
       "      <th>DiffPts</th>\n",
       "      <th>DiffFormPts</th>\n",
       "      <th>DiffLP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1987033</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1987034</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1987035</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1987036</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1987037</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   match_api_id  FTR  HTGS  ATGS  HTGC  ATGC  HTP  ATP  HM1  HM2  HM3  HM4  \\\n",
       "0       1987033    0   0.0   0.0   0.0   0.0  0.0  0.0    2    2    2    2   \n",
       "1       1987034    1   0.0   0.0   0.0   0.0  0.0  0.0    2    2    2    2   \n",
       "2       1987035    1   0.0   0.0   0.0   0.0  0.0  0.0    2    2    2    2   \n",
       "3       1987036    2   0.0   0.0   0.0   0.0  0.0  0.0    2    2    2    2   \n",
       "4       1987037    2   0.0   0.0   0.0   0.0  0.0  0.0    2    2    2    2   \n",
       "\n",
       "   HM5  AM1  AM2  AM3  AM4  AM5  HTWinStreak3  HTWinStreak5  HTLossStreak3  \\\n",
       "0    2    2    2    2    2    2             0             0              0   \n",
       "1    2    2    2    2    2    2             0             0              0   \n",
       "2    2    2    2    2    2    2             0             0              0   \n",
       "3    2    2    2    2    2    2             0             0              0   \n",
       "4    2    2    2    2    2    2             0             0              0   \n",
       "\n",
       "   HTLossStreak5  ATWinStreak3  ATWinStreak5  ATLossStreak3  ATLossStreak5  \\\n",
       "0              0             0             0              0              0   \n",
       "1              0             0             0              0              0   \n",
       "2              0             0             0              0              0   \n",
       "3              0             0             0              0              0   \n",
       "4              0             0             0              0              0   \n",
       "\n",
       "   HTGD  ATGD  DiffPts  DiffFormPts  DiffLP  \n",
       "0   0.0   0.0      0.0          0.0       3  \n",
       "1   0.0   0.0      0.0          0.0      -9  \n",
       "2   0.0   0.0      0.0          0.0     -13  \n",
       "3   0.0   0.0      0.0          0.0       4  \n",
       "4   0.0   0.0      0.0          0.0       1  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv(\"datasets/epl_data_train_onehot_matchid.csv\")\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B365H</th>\n",
       "      <th>B365D</th>\n",
       "      <th>B365A</th>\n",
       "      <th>FTR</th>\n",
       "      <th>HTGS</th>\n",
       "      <th>ATGS</th>\n",
       "      <th>HTGC</th>\n",
       "      <th>ATGC</th>\n",
       "      <th>HTP</th>\n",
       "      <th>ATP</th>\n",
       "      <th>HM1</th>\n",
       "      <th>HM2</th>\n",
       "      <th>HM3</th>\n",
       "      <th>HM4</th>\n",
       "      <th>HM5</th>\n",
       "      <th>AM1</th>\n",
       "      <th>AM2</th>\n",
       "      <th>AM3</th>\n",
       "      <th>AM4</th>\n",
       "      <th>AM5</th>\n",
       "      <th>HTWinStreak3</th>\n",
       "      <th>HTWinStreak5</th>\n",
       "      <th>HTLossStreak3</th>\n",
       "      <th>HTLossStreak5</th>\n",
       "      <th>ATWinStreak3</th>\n",
       "      <th>ATWinStreak5</th>\n",
       "      <th>ATLossStreak3</th>\n",
       "      <th>ATLossStreak5</th>\n",
       "      <th>HTGD</th>\n",
       "      <th>ATGD</th>\n",
       "      <th>DiffPts</th>\n",
       "      <th>DiffFormPts</th>\n",
       "      <th>DiffLP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2650</th>\n",
       "      <td>1.17</td>\n",
       "      <td>9.00</td>\n",
       "      <td>17.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.911392</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>-1.184211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2651</th>\n",
       "      <td>2.30</td>\n",
       "      <td>3.75</td>\n",
       "      <td>3.10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.697917</td>\n",
       "      <td>0.626506</td>\n",
       "      <td>0.443038</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>-0.052632</td>\n",
       "      <td>-0.157895</td>\n",
       "      <td>-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2652</th>\n",
       "      <td>1.80</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.50</td>\n",
       "      <td>2</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>0.810127</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>-0.657895</td>\n",
       "      <td>-0.078947</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2653</th>\n",
       "      <td>4.50</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.80</td>\n",
       "      <td>2</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.771084</td>\n",
       "      <td>0.379747</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.657895</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2654</th>\n",
       "      <td>1.36</td>\n",
       "      <td>5.50</td>\n",
       "      <td>9.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.395833</td>\n",
       "      <td>0.481928</td>\n",
       "      <td>0.594937</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.394737</td>\n",
       "      <td>-0.236842</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2655</th>\n",
       "      <td>3.50</td>\n",
       "      <td>3.60</td>\n",
       "      <td>2.15</td>\n",
       "      <td>2</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.650602</td>\n",
       "      <td>0.620253</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.394737</td>\n",
       "      <td>0.394737</td>\n",
       "      <td>-0.078947</td>\n",
       "      <td>-0.236842</td>\n",
       "      <td>-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2656</th>\n",
       "      <td>6.00</td>\n",
       "      <td>4.75</td>\n",
       "      <td>1.53</td>\n",
       "      <td>1</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>0.614458</td>\n",
       "      <td>0.506329</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.263158</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2657</th>\n",
       "      <td>2.05</td>\n",
       "      <td>3.75</td>\n",
       "      <td>3.70</td>\n",
       "      <td>1</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>0.578313</td>\n",
       "      <td>0.759494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.263158</td>\n",
       "      <td>-0.368421</td>\n",
       "      <td>-0.078947</td>\n",
       "      <td>-0.131579</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2658</th>\n",
       "      <td>2.40</td>\n",
       "      <td>3.60</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.645833</td>\n",
       "      <td>0.566265</td>\n",
       "      <td>0.620253</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.368421</td>\n",
       "      <td>0.342105</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.157895</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2659</th>\n",
       "      <td>1.67</td>\n",
       "      <td>4.20</td>\n",
       "      <td>5.25</td>\n",
       "      <td>2</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.409639</td>\n",
       "      <td>0.810127</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>-0.526316</td>\n",
       "      <td>-0.026316</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>-11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      B365H  B365D  B365A  FTR  HTGS      ATGS      HTGC      ATGC       HTP  \\\n",
       "2650   1.17   9.00  17.00    2  0.61  0.281250  0.433735  0.911392  0.026316   \n",
       "2651   2.30   3.75   3.10    1  0.58  0.697917  0.626506  0.443038  0.026316   \n",
       "2652   1.80   4.00   4.50    2  0.56  0.406250  0.662651  0.810127  0.000000   \n",
       "2653   4.50   4.00   1.80    2  0.39  0.708333  0.771084  0.379747  0.026316   \n",
       "2654   1.36   5.50   9.00    2  0.55  0.395833  0.481928  0.594937  0.078947   \n",
       "2655   3.50   3.60   2.15    2  0.39  0.666667  0.650602  0.620253  0.000000   \n",
       "2656   6.00   4.75   1.53    1  0.41  0.729167  0.614458  0.506329  0.078947   \n",
       "2657   2.05   3.75   3.70    1  0.38  0.479167  0.578313  0.759494  0.000000   \n",
       "2658   2.40   3.60   3.00    1  0.33  0.645833  0.566265  0.620253  0.026316   \n",
       "2659   1.67   4.20   5.25    2  0.46  0.458333  0.409639  0.810127  0.000000   \n",
       "\n",
       "           ATP  HM1  HM2  HM3  HM4  HM5  AM1  AM2  AM3  AM4  AM5  \\\n",
       "2650  0.026316    0    3    0    3    0    0    1    1    1    1   \n",
       "2651  0.078947    0    1    0    3    1    3    0    3    0    3   \n",
       "2652  0.078947    1    1    3    1    0    3    1    1    1    1   \n",
       "2653  0.000000    0    3    0    0    3    1    0    0    3    3   \n",
       "2654  0.078947    3    3    3    0    3    3    1    1    0    0   \n",
       "2655  0.078947    1    0    1    1    1    3    1    3    3    0   \n",
       "2656  0.026316    3    3    1    1    3    0    1    3    0    3   \n",
       "2657  0.078947    1    1    3    1    3    3    3    0    0    3   \n",
       "2658  0.026316    0    1    0    1    1    0    3    1    0    3   \n",
       "2659  0.026316    1    3    0    3    3    0    1    1    1    3   \n",
       "\n",
       "      HTWinStreak3  HTWinStreak5  HTLossStreak3  HTLossStreak5  ATWinStreak3  \\\n",
       "2650             0             0              0              0             0   \n",
       "2651             0             0              0              0             0   \n",
       "2652             0             0              0              0             0   \n",
       "2653             0             0              0              0             0   \n",
       "2654             0             0              0              0             0   \n",
       "2655             0             0              1              0             0   \n",
       "2656             0             0              0              0             0   \n",
       "2657             0             0              0              0             0   \n",
       "2658             0             0              0              0             0   \n",
       "2659             0             0              0              0             0   \n",
       "\n",
       "      ATWinStreak5  ATLossStreak3  ATLossStreak5      HTGD      ATGD  \\\n",
       "2650             0              1              0  0.657895 -1.184211   \n",
       "2651             0              0              0  0.157895  0.842105   \n",
       "2652             0              1              0  0.026316 -0.657895   \n",
       "2653             0              0              0 -0.657895  1.000000   \n",
       "2654             0              0              0  0.394737 -0.236842   \n",
       "2655             0              0              0 -0.394737  0.394737   \n",
       "2656             0              0              0 -0.263158  0.789474   \n",
       "2657             0              0              0 -0.263158 -0.368421   \n",
       "2658             0              0              0 -0.368421  0.342105   \n",
       "2659             0              0              0  0.315789 -0.526316   \n",
       "\n",
       "       DiffPts  DiffFormPts  DiffLP  \n",
       "2650  0.000000     0.210526     -11  \n",
       "2651 -0.052632    -0.157895     -15  \n",
       "2652 -0.078947     0.026316     -13  \n",
       "2653  0.026316     0.026316       4  \n",
       "2654  0.000000     0.210526      -3  \n",
       "2655 -0.078947    -0.236842      -4  \n",
       "2656  0.052632     0.026316      11  \n",
       "2657 -0.078947    -0.131579       4  \n",
       "2658  0.000000    -0.157895      15  \n",
       "2659 -0.026316     0.157895     -11  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df1.set_index('match_api_id').join(df2.set_index('match_api_id'))\n",
    "df_train = df_train.reset_index()\n",
    "df_train = df_train.drop(columns = ['match_api_id'])\n",
    "df_train.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Test (Season 2008/2009)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>match_api_id</th>\n",
       "      <th>B365H</th>\n",
       "      <th>B365D</th>\n",
       "      <th>B365A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>489043</td>\n",
       "      <td>1.20</td>\n",
       "      <td>6.5</td>\n",
       "      <td>15.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>489049</td>\n",
       "      <td>1.83</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>489047</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.3</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>489050</td>\n",
       "      <td>2.60</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>489048</td>\n",
       "      <td>3.20</td>\n",
       "      <td>3.4</td>\n",
       "      <td>2.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   match_api_id  B365H  B365D  B365A\n",
       "1        489043   1.20    6.5  15.00\n",
       "7        489049   1.83    3.5   4.50\n",
       "5        489047   2.00    3.3   4.00\n",
       "8        489050   2.60    3.2   2.80\n",
       "6        489048   3.20    3.4   2.25"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = pd.read_csv(\"datasets/EPL_sort.csv\", index_col=0)\n",
    "df3 = df3[df3['season'] == '2008/2009']\n",
    "df3 = df3[['match_api_id','B365H', 'B365D', 'B365A']]\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>match_api_id</th>\n",
       "      <th>FTR</th>\n",
       "      <th>HTGS</th>\n",
       "      <th>ATGS</th>\n",
       "      <th>HTGC</th>\n",
       "      <th>ATGC</th>\n",
       "      <th>HTP</th>\n",
       "      <th>ATP</th>\n",
       "      <th>HM1</th>\n",
       "      <th>HM2</th>\n",
       "      <th>HM3</th>\n",
       "      <th>HM4</th>\n",
       "      <th>HM5</th>\n",
       "      <th>AM1</th>\n",
       "      <th>AM2</th>\n",
       "      <th>AM3</th>\n",
       "      <th>AM4</th>\n",
       "      <th>AM5</th>\n",
       "      <th>HTWinStreak3</th>\n",
       "      <th>HTWinStreak5</th>\n",
       "      <th>HTLossStreak3</th>\n",
       "      <th>HTLossStreak5</th>\n",
       "      <th>ATWinStreak3</th>\n",
       "      <th>ATWinStreak5</th>\n",
       "      <th>ATLossStreak3</th>\n",
       "      <th>ATLossStreak5</th>\n",
       "      <th>HTGD</th>\n",
       "      <th>ATGD</th>\n",
       "      <th>DiffPts</th>\n",
       "      <th>DiffFormPts</th>\n",
       "      <th>DiffLP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>489043</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>489049</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>489047</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>489050</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>489048</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   match_api_id  FTR  HTGS  ATGS  HTGC  ATGC  HTP  ATP  HM1  HM2  HM3  HM4  \\\n",
       "0        489043    2   0.0   0.0   0.0   0.0  0.0  0.0    2    2    2    2   \n",
       "1        489049    2   0.0   0.0   0.0   0.0  0.0  0.0    2    2    2    2   \n",
       "2        489047    0   0.0   0.0   0.0   0.0  0.0  0.0    2    2    2    2   \n",
       "3        489050    2   0.0   0.0   0.0   0.0  0.0  0.0    2    2    2    2   \n",
       "4        489048    2   0.0   0.0   0.0   0.0  0.0  0.0    2    2    2    2   \n",
       "\n",
       "   HM5  AM1  AM2  AM3  AM4  AM5  HTWinStreak3  HTWinStreak5  HTLossStreak3  \\\n",
       "0    2    2    2    2    2    2             0             0              0   \n",
       "1    2    2    2    2    2    2             0             0              0   \n",
       "2    2    2    2    2    2    2             0             0              0   \n",
       "3    2    2    2    2    2    2             0             0              0   \n",
       "4    2    2    2    2    2    2             0             0              0   \n",
       "\n",
       "   HTLossStreak5  ATWinStreak3  ATWinStreak5  ATLossStreak3  ATLossStreak5  \\\n",
       "0              0             0             0              0              0   \n",
       "1              0             0             0              0              0   \n",
       "2              0             0             0              0              0   \n",
       "3              0             0             0              0              0   \n",
       "4              0             0             0              0              0   \n",
       "\n",
       "   HTGD  ATGD  DiffPts  DiffFormPts  DiffLP  \n",
       "0   0.0   0.0      0.0          0.0     -14  \n",
       "1   0.0   0.0      0.0          0.0     -11  \n",
       "2   0.0   0.0      0.0          0.0      -4  \n",
       "3   0.0   0.0      0.0          0.0       0  \n",
       "4   0.0   0.0      0.0          0.0       7  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4 = pd.read_csv(\"datasets/epl_data_test_onehot_matchid.csv\")\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B365H</th>\n",
       "      <th>B365D</th>\n",
       "      <th>B365A</th>\n",
       "      <th>FTR</th>\n",
       "      <th>HTGS</th>\n",
       "      <th>ATGS</th>\n",
       "      <th>HTGC</th>\n",
       "      <th>ATGC</th>\n",
       "      <th>HTP</th>\n",
       "      <th>ATP</th>\n",
       "      <th>HM1</th>\n",
       "      <th>HM2</th>\n",
       "      <th>HM3</th>\n",
       "      <th>HM4</th>\n",
       "      <th>HM5</th>\n",
       "      <th>AM1</th>\n",
       "      <th>AM2</th>\n",
       "      <th>AM3</th>\n",
       "      <th>AM4</th>\n",
       "      <th>AM5</th>\n",
       "      <th>HTWinStreak3</th>\n",
       "      <th>HTWinStreak5</th>\n",
       "      <th>HTLossStreak3</th>\n",
       "      <th>HTLossStreak5</th>\n",
       "      <th>ATWinStreak3</th>\n",
       "      <th>ATWinStreak5</th>\n",
       "      <th>ATLossStreak3</th>\n",
       "      <th>ATLossStreak5</th>\n",
       "      <th>HTGD</th>\n",
       "      <th>ATGD</th>\n",
       "      <th>DiffPts</th>\n",
       "      <th>DiffFormPts</th>\n",
       "      <th>DiffLP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>1.33</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.385417</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.645570</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>-0.368421</td>\n",
       "      <td>-0.052632</td>\n",
       "      <td>-0.026316</td>\n",
       "      <td>-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>2.30</td>\n",
       "      <td>3.4</td>\n",
       "      <td>3.10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.578313</td>\n",
       "      <td>0.734177</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.131579</td>\n",
       "      <td>-0.473684</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>1.83</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.50</td>\n",
       "      <td>1</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.722892</td>\n",
       "      <td>0.848101</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.526316</td>\n",
       "      <td>-0.815789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>2.10</td>\n",
       "      <td>3.3</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.552083</td>\n",
       "      <td>0.385542</td>\n",
       "      <td>0.468354</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.184211</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>3.00</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.697917</td>\n",
       "      <td>0.759036</td>\n",
       "      <td>0.303797</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.631579</td>\n",
       "      <td>1.131579</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.315789</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>1.53</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.50</td>\n",
       "      <td>2</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.313253</td>\n",
       "      <td>0.531646</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.263158</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>1.73</td>\n",
       "      <td>3.6</td>\n",
       "      <td>5.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.427083</td>\n",
       "      <td>0.602410</td>\n",
       "      <td>0.658228</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.184211</td>\n",
       "      <td>-0.289474</td>\n",
       "      <td>-0.026316</td>\n",
       "      <td>0.131579</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>4.00</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.91</td>\n",
       "      <td>0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.677083</td>\n",
       "      <td>0.614458</td>\n",
       "      <td>0.278481</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>1.131579</td>\n",
       "      <td>-0.078947</td>\n",
       "      <td>-0.236842</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>2.25</td>\n",
       "      <td>3.4</td>\n",
       "      <td>3.20</td>\n",
       "      <td>2</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>0.530120</td>\n",
       "      <td>0.696203</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.105263</td>\n",
       "      <td>-0.736842</td>\n",
       "      <td>-0.026316</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>2.10</td>\n",
       "      <td>3.4</td>\n",
       "      <td>3.50</td>\n",
       "      <td>2</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.395833</td>\n",
       "      <td>0.542169</td>\n",
       "      <td>0.708861</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.315789</td>\n",
       "      <td>-0.473684</td>\n",
       "      <td>-0.078947</td>\n",
       "      <td>-0.078947</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     B365H  B365D  B365A  FTR  HTGS      ATGS      HTGC      ATGC       HTP  \\\n",
       "370   1.33    5.0  10.00    2  0.64  0.385417  0.433735  0.645570  0.026316   \n",
       "371   2.30    3.4   3.10    2  0.53  0.416667  0.578313  0.734177  0.026316   \n",
       "372   1.83    3.5   4.50    1  0.40  0.375000  0.722892  0.848101  0.000000   \n",
       "373   2.10    3.3   3.60    0  0.39  0.552083  0.385542  0.468354  0.078947   \n",
       "374   3.00    3.5   2.30    0  0.39  0.697917  0.759036  0.303797  0.026316   \n",
       "375   1.53    4.0   6.50    2  0.74  0.458333  0.313253  0.531646  0.078947   \n",
       "376   1.73    3.6   5.00    2  0.57  0.427083  0.602410  0.658228  0.000000   \n",
       "377   4.00    3.6   1.91    0  0.32  0.677083  0.614458  0.278481  0.000000   \n",
       "378   2.25    3.4   3.20    2  0.40  0.281250  0.530120  0.696203  0.000000   \n",
       "379   2.10    3.4   3.50    2  0.33  0.395833  0.542169  0.708861  0.000000   \n",
       "\n",
       "          ATP  HM1  HM2  HM3  HM4  HM5  AM1  AM2  AM3  AM4  AM5  HTWinStreak3  \\\n",
       "370  0.078947    0    1    3    3    0    3    3    1    1    3             0   \n",
       "371  0.000000    0    1    3    0    0    1    3    1    0    1             0   \n",
       "372  0.000000    1    3    1    3    1    1    3    1    3    1             0   \n",
       "373  0.078947    3    3    1    3    0    3    0    3    1    0             0   \n",
       "374  0.026316    0    1    1    1    1    0    3    3    3    3             0   \n",
       "375  0.078947    3    3    3    3    0    3    0    3    1    3             0   \n",
       "376  0.026316    1    1    3    3    3    0    0    0    0    1             1   \n",
       "377  0.078947    1    0    1    1    3    3    3    3    3    0             0   \n",
       "378  0.026316    1    1    3    1    0    0    1    1    1    0             0   \n",
       "379  0.078947    1    1    1    0    1    3    1    1    0    1             0   \n",
       "\n",
       "     HTWinStreak5  HTLossStreak3  HTLossStreak5  ATWinStreak3  ATWinStreak5  \\\n",
       "370             0              0              0             0             0   \n",
       "371             0              0              0             0             0   \n",
       "372             0              0              0             0             0   \n",
       "373             0              0              0             0             0   \n",
       "374             0              1              0             1             0   \n",
       "375             0              0              0             0             0   \n",
       "376             0              0              0             0             0   \n",
       "377             0              0              0             0             0   \n",
       "378             0              0              0             0             0   \n",
       "379             0              0              0             0             0   \n",
       "\n",
       "     ATLossStreak3  ATLossStreak5      HTGD      ATGD   DiffPts  DiffFormPts  \\\n",
       "370              0              0  0.736842 -0.368421 -0.052632    -0.026316   \n",
       "371              0              0  0.131579 -0.473684  0.026316     0.052632   \n",
       "372              0              0 -0.526316 -0.815789  0.000000     0.000000   \n",
       "373              0              0  0.184211  0.421053  0.000000     0.052632   \n",
       "374              0              0 -0.631579  1.131579  0.000000    -0.315789   \n",
       "375              0              0  1.263158  0.052632  0.000000     0.078947   \n",
       "376              0              0  0.184211 -0.289474 -0.026316     0.131579   \n",
       "377              0              0 -0.500000  1.131579 -0.078947    -0.236842   \n",
       "378              0              0 -0.105263 -0.736842 -0.026316     0.052632   \n",
       "379              0              0 -0.315789 -0.473684 -0.078947    -0.078947   \n",
       "\n",
       "     DiffLP  \n",
       "370     -14  \n",
       "371      -2  \n",
       "372      -8  \n",
       "373      12  \n",
       "374      17  \n",
       "375      -2  \n",
       "376       7  \n",
       "377      16  \n",
       "378       3  \n",
       "379       9  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = df3.set_index('match_api_id').join(df4.set_index('match_api_id'))\n",
    "df_test = df_test.reset_index()\n",
    "df_test = df_test.drop(columns = ['match_api_id'])\n",
    "df_test.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2660, 32)\n",
      "(2660,)\n",
      "(380, 32)\n",
      "(380,)\n",
      "(2660, 3)\n",
      "(380, 3)\n"
     ]
    }
   ],
   "source": [
    "x_train = df_train.drop(columns = ['FTR'])\n",
    "y_train = df_train['FTR']\n",
    "x_test = df_test.drop(columns = ['FTR'])\n",
    "y_test = df_test['FTR']\n",
    "bet_train = df_train[['B365H', 'B365D', 'B365A']]\n",
    "bet_test = df_test[['B365H', 'B365D', 'B365A']]\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "print(bet_train.shape)\n",
    "print(bet_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = x_train.values.shape[1]\n",
    "columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First neural model start here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 32)]              0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 41)                1353      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 75)                3150      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 75)                0         \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 75)                0         \n",
      "=================================================================\n",
      "Total params: 4,503\n",
      "Trainable params: 4,503\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 41-75-3\n",
    "# We first need to define the sequence of dependencies (the computational graph)\n",
    "# x = Input(shape=(20,32))\n",
    "x = Input(shape=(columns,))\n",
    "# h = Conv2D(32, (7, 7), strides=(2, 2))(x)\n",
    "# h = Activation('relu')(x)\n",
    "h = Flatten()(x)\n",
    "h = Dense(41)(h)\n",
    "h = Dense(75)(h)\n",
    "# for i in range(10):\n",
    "#     h = Dense(75)(h)\n",
    "h = Dropout(0.3)(h)\n",
    "p = Activation('softmax')(h)\n",
    "\n",
    "# Now that we have defined how to find p from x, we can create a \n",
    "# model simply by saying what is input and what is output\n",
    "model = Model(inputs=x, outputs=p)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(lr=0.00001), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 7.3906 - accuracy: 0.0000e+00 - val_loss: 6.2621 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 7.0629 - accuracy: 0.0000e+00 - val_loss: 6.0429 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 6.8111 - accuracy: 7.8339e-04 - val_loss: 5.8278 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 6.4751 - accuracy: 7.8339e-04 - val_loss: 5.6184 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 6.2017 - accuracy: 0.0027 - val_loss: 5.4155 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 5.9376 - accuracy: 0.0067 - val_loss: 5.2154 - val_accuracy: 0.0093\n",
      "Epoch 7/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 5.7034 - accuracy: 0.0129 - val_loss: 5.0213 - val_accuracy: 0.0093\n",
      "Epoch 8/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 5.4596 - accuracy: 0.0180 - val_loss: 4.8279 - val_accuracy: 0.0187\n",
      "Epoch 9/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 5.2230 - accuracy: 0.0309 - val_loss: 4.6403 - val_accuracy: 0.0187\n",
      "Epoch 10/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 4.9797 - accuracy: 0.0415 - val_loss: 4.4613 - val_accuracy: 0.0280\n",
      "Epoch 11/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 4.7831 - accuracy: 0.0603 - val_loss: 4.2842 - val_accuracy: 0.0374\n",
      "Epoch 12/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 4.5644 - accuracy: 0.0729 - val_loss: 4.1110 - val_accuracy: 0.0654\n",
      "Epoch 13/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 4.3718 - accuracy: 0.0991 - val_loss: 3.9462 - val_accuracy: 0.1121\n",
      "Epoch 14/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 4.1506 - accuracy: 0.1277 - val_loss: 3.7879 - val_accuracy: 0.1215\n",
      "Epoch 15/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 4.0312 - accuracy: 0.1614 - val_loss: 3.6354 - val_accuracy: 0.1589\n",
      "Epoch 16/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 3.8227 - accuracy: 0.2029 - val_loss: 3.4894 - val_accuracy: 0.2150\n",
      "Epoch 17/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 3.6516 - accuracy: 0.2342 - val_loss: 3.3510 - val_accuracy: 0.2430\n",
      "Epoch 18/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 3.5110 - accuracy: 0.2750 - val_loss: 3.2188 - val_accuracy: 0.2617\n",
      "Epoch 19/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 3.4362 - accuracy: 0.2797 - val_loss: 3.0937 - val_accuracy: 0.3084\n",
      "Epoch 20/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 3.3466 - accuracy: 0.3012 - val_loss: 2.9787 - val_accuracy: 0.3645\n",
      "Epoch 21/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 3.2134 - accuracy: 0.3243 - val_loss: 2.8689 - val_accuracy: 0.3645\n",
      "Epoch 22/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 3.1335 - accuracy: 0.3318 - val_loss: 2.7651 - val_accuracy: 0.3738\n",
      "Epoch 23/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 3.0317 - accuracy: 0.3510 - val_loss: 2.6648 - val_accuracy: 0.3832\n",
      "Epoch 24/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.9874 - accuracy: 0.3431 - val_loss: 2.5701 - val_accuracy: 0.3925\n",
      "Epoch 25/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.8296 - accuracy: 0.3729 - val_loss: 2.4775 - val_accuracy: 0.3925\n",
      "Epoch 26/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.7759 - accuracy: 0.3639 - val_loss: 2.3888 - val_accuracy: 0.4019\n",
      "Epoch 27/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.7862 - accuracy: 0.3639 - val_loss: 2.3101 - val_accuracy: 0.4206\n",
      "Epoch 28/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.6858 - accuracy: 0.3776 - val_loss: 2.2338 - val_accuracy: 0.4206\n",
      "Epoch 29/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.6756 - accuracy: 0.3733 - val_loss: 2.1623 - val_accuracy: 0.4206\n",
      "Epoch 30/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.5877 - accuracy: 0.3878 - val_loss: 2.0945 - val_accuracy: 0.3925\n",
      "Epoch 31/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.5285 - accuracy: 0.4003 - val_loss: 2.0311 - val_accuracy: 0.3832\n",
      "Epoch 32/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.4939 - accuracy: 0.3917 - val_loss: 1.9686 - val_accuracy: 0.3832\n",
      "Epoch 33/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 2.4708 - accuracy: 0.3933 - val_loss: 1.9128 - val_accuracy: 0.3832\n",
      "Epoch 34/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 2.4119 - accuracy: 0.3956 - val_loss: 1.8627 - val_accuracy: 0.3832\n",
      "Epoch 35/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 2.3822 - accuracy: 0.4011 - val_loss: 1.8149 - val_accuracy: 0.3925\n",
      "Epoch 36/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 2.4042 - accuracy: 0.3890 - val_loss: 1.7718 - val_accuracy: 0.3925\n",
      "Epoch 37/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.3116 - accuracy: 0.4007 - val_loss: 1.7299 - val_accuracy: 0.3832\n",
      "Epoch 38/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.2721 - accuracy: 0.4093 - val_loss: 1.6921 - val_accuracy: 0.4019\n",
      "Epoch 39/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.2402 - accuracy: 0.4089 - val_loss: 1.6583 - val_accuracy: 0.4112\n",
      "Epoch 40/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.2529 - accuracy: 0.4042 - val_loss: 1.6279 - val_accuracy: 0.4112\n",
      "Epoch 41/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.1487 - accuracy: 0.4211 - val_loss: 1.5972 - val_accuracy: 0.4112\n",
      "Epoch 42/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.2378 - accuracy: 0.4015 - val_loss: 1.5726 - val_accuracy: 0.4206\n",
      "Epoch 43/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.2208 - accuracy: 0.4034 - val_loss: 1.5484 - val_accuracy: 0.4206\n",
      "Epoch 44/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 2.2093 - accuracy: 0.4140 - val_loss: 1.5277 - val_accuracy: 0.4206\n",
      "Epoch 45/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 2.1719 - accuracy: 0.4262 - val_loss: 1.5073 - val_accuracy: 0.4299\n",
      "Epoch 46/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 2.2218 - accuracy: 0.4003 - val_loss: 1.4906 - val_accuracy: 0.4486\n",
      "Epoch 47/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.1566 - accuracy: 0.4148 - val_loss: 1.4728 - val_accuracy: 0.4486\n",
      "Epoch 48/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.2170 - accuracy: 0.4019 - val_loss: 1.4568 - val_accuracy: 0.4393\n",
      "Epoch 49/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.1577 - accuracy: 0.4187 - val_loss: 1.4428 - val_accuracy: 0.4393\n",
      "Epoch 50/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.1596 - accuracy: 0.4066 - val_loss: 1.4288 - val_accuracy: 0.4393\n",
      "Epoch 51/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.1071 - accuracy: 0.4230 - val_loss: 1.4148 - val_accuracy: 0.4299\n",
      "Epoch 52/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.1538 - accuracy: 0.4034 - val_loss: 1.4031 - val_accuracy: 0.4299\n",
      "Epoch 53/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0859 - accuracy: 0.4297 - val_loss: 1.3918 - val_accuracy: 0.4299\n",
      "Epoch 54/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.1755 - accuracy: 0.4117 - val_loss: 1.3824 - val_accuracy: 0.4393\n",
      "Epoch 55/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0729 - accuracy: 0.4395 - val_loss: 1.3689 - val_accuracy: 0.4393\n",
      "Epoch 56/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 2.1653 - accuracy: 0.4066 - val_loss: 1.3595 - val_accuracy: 0.4486\n",
      "Epoch 57/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 2.0843 - accuracy: 0.4121 - val_loss: 1.3474 - val_accuracy: 0.4486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.1366 - accuracy: 0.4238 - val_loss: 1.3384 - val_accuracy: 0.4673\n",
      "Epoch 59/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0775 - accuracy: 0.4211 - val_loss: 1.3278 - val_accuracy: 0.4673\n",
      "Epoch 60/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.1365 - accuracy: 0.4128 - val_loss: 1.3198 - val_accuracy: 0.4766\n",
      "Epoch 61/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.1187 - accuracy: 0.4199 - val_loss: 1.3115 - val_accuracy: 0.4766\n",
      "Epoch 62/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.1137 - accuracy: 0.4136 - val_loss: 1.3039 - val_accuracy: 0.4766\n",
      "Epoch 63/200\n",
      "160/160 [==============================] - 1s 4ms/step - loss: 2.0164 - accuracy: 0.4348 - val_loss: 1.2928 - val_accuracy: 0.4766\n",
      "Epoch 64/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9969 - accuracy: 0.4281 - val_loss: 1.2813 - val_accuracy: 0.4766\n",
      "Epoch 65/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0674 - accuracy: 0.4293 - val_loss: 1.2729 - val_accuracy: 0.4766\n",
      "Epoch 66/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0743 - accuracy: 0.4172 - val_loss: 1.2653 - val_accuracy: 0.4766\n",
      "Epoch 67/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0528 - accuracy: 0.4277 - val_loss: 1.2572 - val_accuracy: 0.4766\n",
      "Epoch 68/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0708 - accuracy: 0.4320 - val_loss: 1.2501 - val_accuracy: 0.4766\n",
      "Epoch 69/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0521 - accuracy: 0.4175 - val_loss: 1.2424 - val_accuracy: 0.4766\n",
      "Epoch 70/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0475 - accuracy: 0.4187 - val_loss: 1.2346 - val_accuracy: 0.4766\n",
      "Epoch 71/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9997 - accuracy: 0.4360 - val_loss: 1.2262 - val_accuracy: 0.4766\n",
      "Epoch 72/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0470 - accuracy: 0.4258 - val_loss: 1.2197 - val_accuracy: 0.4860\n",
      "Epoch 73/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0094 - accuracy: 0.4301 - val_loss: 1.2126 - val_accuracy: 0.4860\n",
      "Epoch 74/200\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 2.0559 - accuracy: 0.4258 - val_loss: 1.2066 - val_accuracy: 0.4860\n",
      "Epoch 75/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9901 - accuracy: 0.4360 - val_loss: 1.1998 - val_accuracy: 0.4860\n",
      "Epoch 76/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0260 - accuracy: 0.4324 - val_loss: 1.1932 - val_accuracy: 0.4860\n",
      "Epoch 77/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0172 - accuracy: 0.4266 - val_loss: 1.1871 - val_accuracy: 0.4860\n",
      "Epoch 78/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9945 - accuracy: 0.4328 - val_loss: 1.1808 - val_accuracy: 0.4860\n",
      "Epoch 79/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9740 - accuracy: 0.4371 - val_loss: 1.1744 - val_accuracy: 0.4860\n",
      "Epoch 80/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0316 - accuracy: 0.4281 - val_loss: 1.1691 - val_accuracy: 0.4860\n",
      "Epoch 81/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0147 - accuracy: 0.4277 - val_loss: 1.1640 - val_accuracy: 0.4860\n",
      "Epoch 82/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9959 - accuracy: 0.4297 - val_loss: 1.1588 - val_accuracy: 0.4860\n",
      "Epoch 83/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0217 - accuracy: 0.4273 - val_loss: 1.1542 - val_accuracy: 0.4860\n",
      "Epoch 84/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9863 - accuracy: 0.4289 - val_loss: 1.1490 - val_accuracy: 0.4860\n",
      "Epoch 85/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9768 - accuracy: 0.4328 - val_loss: 1.1431 - val_accuracy: 0.4860\n",
      "Epoch 86/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9922 - accuracy: 0.4316 - val_loss: 1.1380 - val_accuracy: 0.4860\n",
      "Epoch 87/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0628 - accuracy: 0.4160 - val_loss: 1.1349 - val_accuracy: 0.4766\n",
      "Epoch 88/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0008 - accuracy: 0.4269 - val_loss: 1.1303 - val_accuracy: 0.4766\n",
      "Epoch 89/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9641 - accuracy: 0.4395 - val_loss: 1.1257 - val_accuracy: 0.4766\n",
      "Epoch 90/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9946 - accuracy: 0.4262 - val_loss: 1.1215 - val_accuracy: 0.4766\n",
      "Epoch 91/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9861 - accuracy: 0.4414 - val_loss: 1.1173 - val_accuracy: 0.4673\n",
      "Epoch 92/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0256 - accuracy: 0.4250 - val_loss: 1.1141 - val_accuracy: 0.4673\n",
      "Epoch 93/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9786 - accuracy: 0.4313 - val_loss: 1.1103 - val_accuracy: 0.4766\n",
      "Epoch 94/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9406 - accuracy: 0.4512 - val_loss: 1.1065 - val_accuracy: 0.4766\n",
      "Epoch 95/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9967 - accuracy: 0.4328 - val_loss: 1.1021 - val_accuracy: 0.4766\n",
      "Epoch 96/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9845 - accuracy: 0.4348 - val_loss: 1.0990 - val_accuracy: 0.4766\n",
      "Epoch 97/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9665 - accuracy: 0.4461 - val_loss: 1.0954 - val_accuracy: 0.4766\n",
      "Epoch 98/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9717 - accuracy: 0.4348 - val_loss: 1.0918 - val_accuracy: 0.4766\n",
      "Epoch 99/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0089 - accuracy: 0.4258 - val_loss: 1.0894 - val_accuracy: 0.4766\n",
      "Epoch 100/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9669 - accuracy: 0.4328 - val_loss: 1.0859 - val_accuracy: 0.4766\n",
      "Epoch 101/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9506 - accuracy: 0.4352 - val_loss: 1.0826 - val_accuracy: 0.4766\n",
      "Epoch 102/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9449 - accuracy: 0.4430 - val_loss: 1.0791 - val_accuracy: 0.4766\n",
      "Epoch 103/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9442 - accuracy: 0.4340 - val_loss: 1.0761 - val_accuracy: 0.4766\n",
      "Epoch 104/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9851 - accuracy: 0.4230 - val_loss: 1.0739 - val_accuracy: 0.4766\n",
      "Epoch 105/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9648 - accuracy: 0.4438 - val_loss: 1.0711 - val_accuracy: 0.4766\n",
      "Epoch 106/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9690 - accuracy: 0.4352 - val_loss: 1.0684 - val_accuracy: 0.4766\n",
      "Epoch 107/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9770 - accuracy: 0.4379 - val_loss: 1.0661 - val_accuracy: 0.4766\n",
      "Epoch 108/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9723 - accuracy: 0.4375 - val_loss: 1.0635 - val_accuracy: 0.4766\n",
      "Epoch 109/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9338 - accuracy: 0.4367 - val_loss: 1.0607 - val_accuracy: 0.4766\n",
      "Epoch 110/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9231 - accuracy: 0.4508 - val_loss: 1.0581 - val_accuracy: 0.4766\n",
      "Epoch 111/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0253 - accuracy: 0.4309 - val_loss: 1.0569 - val_accuracy: 0.4860\n",
      "Epoch 112/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9698 - accuracy: 0.4371 - val_loss: 1.0546 - val_accuracy: 0.4860\n",
      "Epoch 113/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9763 - accuracy: 0.4242 - val_loss: 1.0530 - val_accuracy: 0.4673\n",
      "Epoch 114/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9769 - accuracy: 0.4403 - val_loss: 1.0507 - val_accuracy: 0.4673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0151 - accuracy: 0.4207 - val_loss: 1.0492 - val_accuracy: 0.4766\n",
      "Epoch 116/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9465 - accuracy: 0.4399 - val_loss: 1.0475 - val_accuracy: 0.4766\n",
      "Epoch 117/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9788 - accuracy: 0.4332 - val_loss: 1.0458 - val_accuracy: 0.4673\n",
      "Epoch 118/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9650 - accuracy: 0.4379 - val_loss: 1.0443 - val_accuracy: 0.4766\n",
      "Epoch 119/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0127 - accuracy: 0.4219 - val_loss: 1.0430 - val_accuracy: 0.4860\n",
      "Epoch 120/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9815 - accuracy: 0.4293 - val_loss: 1.0410 - val_accuracy: 0.4860\n",
      "Epoch 121/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0062 - accuracy: 0.4363 - val_loss: 1.0398 - val_accuracy: 0.4860\n",
      "Epoch 122/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9681 - accuracy: 0.4438 - val_loss: 1.0385 - val_accuracy: 0.4860\n",
      "Epoch 123/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9866 - accuracy: 0.4328 - val_loss: 1.0370 - val_accuracy: 0.4860\n",
      "Epoch 124/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9909 - accuracy: 0.4430 - val_loss: 1.0359 - val_accuracy: 0.4860\n",
      "Epoch 125/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9233 - accuracy: 0.4371 - val_loss: 1.0339 - val_accuracy: 0.4860\n",
      "Epoch 126/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9550 - accuracy: 0.4407 - val_loss: 1.0322 - val_accuracy: 0.4860\n",
      "Epoch 127/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9394 - accuracy: 0.4438 - val_loss: 1.0309 - val_accuracy: 0.4860\n",
      "Epoch 128/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9432 - accuracy: 0.4450 - val_loss: 1.0299 - val_accuracy: 0.4860\n",
      "Epoch 129/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9530 - accuracy: 0.4375 - val_loss: 1.0289 - val_accuracy: 0.4860\n",
      "Epoch 130/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9767 - accuracy: 0.4450 - val_loss: 1.0282 - val_accuracy: 0.4766\n",
      "Epoch 131/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9643 - accuracy: 0.4344 - val_loss: 1.0267 - val_accuracy: 0.4766\n",
      "Epoch 132/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9687 - accuracy: 0.4407 - val_loss: 1.0247 - val_accuracy: 0.4860\n",
      "Epoch 133/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9506 - accuracy: 0.4469 - val_loss: 1.0239 - val_accuracy: 0.4860\n",
      "Epoch 134/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9192 - accuracy: 0.4465 - val_loss: 1.0225 - val_accuracy: 0.4860\n",
      "Epoch 135/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9957 - accuracy: 0.4336 - val_loss: 1.0221 - val_accuracy: 0.4860\n",
      "Epoch 136/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9207 - accuracy: 0.4461 - val_loss: 1.0209 - val_accuracy: 0.4766\n",
      "Epoch 137/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9856 - accuracy: 0.4458 - val_loss: 1.0196 - val_accuracy: 0.4860\n",
      "Epoch 138/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9943 - accuracy: 0.4305 - val_loss: 1.0195 - val_accuracy: 0.4860\n",
      "Epoch 139/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9639 - accuracy: 0.4454 - val_loss: 1.0192 - val_accuracy: 0.4860\n",
      "Epoch 140/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9340 - accuracy: 0.4548 - val_loss: 1.0181 - val_accuracy: 0.4953\n",
      "Epoch 141/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9592 - accuracy: 0.4410 - val_loss: 1.0174 - val_accuracy: 0.4860\n",
      "Epoch 142/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9374 - accuracy: 0.4379 - val_loss: 1.0164 - val_accuracy: 0.4766\n",
      "Epoch 143/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9802 - accuracy: 0.4465 - val_loss: 1.0154 - val_accuracy: 0.4860\n",
      "Epoch 144/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0069 - accuracy: 0.4352 - val_loss: 1.0149 - val_accuracy: 0.4860\n",
      "Epoch 145/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9838 - accuracy: 0.4320 - val_loss: 1.0145 - val_accuracy: 0.4860\n",
      "Epoch 146/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9515 - accuracy: 0.4383 - val_loss: 1.0134 - val_accuracy: 0.4860\n",
      "Epoch 147/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9567 - accuracy: 0.4395 - val_loss: 1.0121 - val_accuracy: 0.4766\n",
      "Epoch 148/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9484 - accuracy: 0.4418 - val_loss: 1.0113 - val_accuracy: 0.4766\n",
      "Epoch 149/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9912 - accuracy: 0.4434 - val_loss: 1.0115 - val_accuracy: 0.4766\n",
      "Epoch 150/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9589 - accuracy: 0.4461 - val_loss: 1.0106 - val_accuracy: 0.4766\n",
      "Epoch 151/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9307 - accuracy: 0.4571 - val_loss: 1.0101 - val_accuracy: 0.4860\n",
      "Epoch 152/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9279 - accuracy: 0.4512 - val_loss: 1.0097 - val_accuracy: 0.4860\n",
      "Epoch 153/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9500 - accuracy: 0.4485 - val_loss: 1.0090 - val_accuracy: 0.4766\n",
      "Epoch 154/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9188 - accuracy: 0.4430 - val_loss: 1.0078 - val_accuracy: 0.4766\n",
      "Epoch 155/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9935 - accuracy: 0.4360 - val_loss: 1.0076 - val_accuracy: 0.4766\n",
      "Epoch 156/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.8806 - accuracy: 0.4567 - val_loss: 1.0073 - val_accuracy: 0.4860\n",
      "Epoch 157/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9756 - accuracy: 0.4383 - val_loss: 1.0073 - val_accuracy: 0.4860\n",
      "Epoch 158/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9448 - accuracy: 0.4422 - val_loss: 1.0068 - val_accuracy: 0.4860\n",
      "Epoch 159/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9240 - accuracy: 0.4465 - val_loss: 1.0062 - val_accuracy: 0.4766\n",
      "Epoch 160/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9627 - accuracy: 0.4414 - val_loss: 1.0058 - val_accuracy: 0.4860\n",
      "Epoch 161/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9394 - accuracy: 0.4563 - val_loss: 1.0047 - val_accuracy: 0.4860\n",
      "Epoch 162/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9181 - accuracy: 0.4461 - val_loss: 1.0043 - val_accuracy: 0.4766\n",
      "Epoch 163/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0106 - accuracy: 0.4446 - val_loss: 1.0042 - val_accuracy: 0.4766\n",
      "Epoch 164/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0135 - accuracy: 0.4399 - val_loss: 1.0040 - val_accuracy: 0.4860\n",
      "Epoch 165/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9644 - accuracy: 0.4450 - val_loss: 1.0040 - val_accuracy: 0.4860\n",
      "Epoch 166/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9560 - accuracy: 0.4387 - val_loss: 1.0030 - val_accuracy: 0.4766\n",
      "Epoch 167/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9792 - accuracy: 0.4367 - val_loss: 1.0031 - val_accuracy: 0.4860\n",
      "Epoch 168/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9421 - accuracy: 0.4505 - val_loss: 1.0025 - val_accuracy: 0.4766\n",
      "Epoch 169/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9642 - accuracy: 0.4403 - val_loss: 1.0020 - val_accuracy: 0.4860\n",
      "Epoch 170/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9852 - accuracy: 0.4316 - val_loss: 1.0020 - val_accuracy: 0.4860\n",
      "Epoch 171/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9072 - accuracy: 0.4520 - val_loss: 1.0011 - val_accuracy: 0.4766\n",
      "Epoch 172/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9042 - accuracy: 0.4532 - val_loss: 1.0007 - val_accuracy: 0.4860\n",
      "Epoch 173/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9920 - accuracy: 0.4360 - val_loss: 1.0002 - val_accuracy: 0.4860\n",
      "Epoch 174/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9028 - accuracy: 0.4516 - val_loss: 0.9999 - val_accuracy: 0.4766\n",
      "Epoch 175/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9400 - accuracy: 0.4505 - val_loss: 0.9996 - val_accuracy: 0.4766\n",
      "Epoch 176/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9500 - accuracy: 0.4450 - val_loss: 0.9998 - val_accuracy: 0.4953\n",
      "Epoch 177/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9323 - accuracy: 0.4489 - val_loss: 0.9998 - val_accuracy: 0.4953\n",
      "Epoch 178/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9524 - accuracy: 0.4477 - val_loss: 0.9994 - val_accuracy: 0.4860\n",
      "Epoch 179/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.8922 - accuracy: 0.4602 - val_loss: 0.9989 - val_accuracy: 0.4953\n",
      "Epoch 180/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9309 - accuracy: 0.4520 - val_loss: 0.9986 - val_accuracy: 0.4953\n",
      "Epoch 181/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9514 - accuracy: 0.4446 - val_loss: 0.9979 - val_accuracy: 0.4953\n",
      "Epoch 182/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9778 - accuracy: 0.4391 - val_loss: 0.9983 - val_accuracy: 0.4860\n",
      "Epoch 183/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.8913 - accuracy: 0.4591 - val_loss: 0.9980 - val_accuracy: 0.4953\n",
      "Epoch 184/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9671 - accuracy: 0.4461 - val_loss: 0.9979 - val_accuracy: 0.4953\n",
      "Epoch 185/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0055 - accuracy: 0.4356 - val_loss: 0.9974 - val_accuracy: 0.4860\n",
      "Epoch 186/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9382 - accuracy: 0.4540 - val_loss: 0.9975 - val_accuracy: 0.4953\n",
      "Epoch 187/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9763 - accuracy: 0.4434 - val_loss: 0.9975 - val_accuracy: 0.4953\n",
      "Epoch 188/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9353 - accuracy: 0.4602 - val_loss: 0.9976 - val_accuracy: 0.4953\n",
      "Epoch 189/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9134 - accuracy: 0.4555 - val_loss: 0.9973 - val_accuracy: 0.4953\n",
      "Epoch 190/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9848 - accuracy: 0.4328 - val_loss: 0.9967 - val_accuracy: 0.4953\n",
      "Epoch 191/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9394 - accuracy: 0.4379 - val_loss: 0.9967 - val_accuracy: 0.5047\n",
      "Epoch 192/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9366 - accuracy: 0.4512 - val_loss: 0.9964 - val_accuracy: 0.4953\n",
      "Epoch 193/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9085 - accuracy: 0.4465 - val_loss: 0.9960 - val_accuracy: 0.5047\n",
      "Epoch 194/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9779 - accuracy: 0.4508 - val_loss: 0.9962 - val_accuracy: 0.5047\n",
      "Epoch 195/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9188 - accuracy: 0.4501 - val_loss: 0.9961 - val_accuracy: 0.5140\n",
      "Epoch 196/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9453 - accuracy: 0.4450 - val_loss: 0.9957 - val_accuracy: 0.4953\n",
      "Epoch 197/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9669 - accuracy: 0.4348 - val_loss: 0.9955 - val_accuracy: 0.5047\n",
      "Epoch 198/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9382 - accuracy: 0.4465 - val_loss: 0.9950 - val_accuracy: 0.5047\n",
      "Epoch 199/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9954 - accuracy: 0.4301 - val_loss: 0.9954 - val_accuracy: 0.4953\n",
      "Epoch 200/200\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9059 - accuracy: 0.4540 - val_loss: 0.9957 - val_accuracy: 0.4953\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, batch_size=16, epochs=200, validation_split=0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'], label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.55\n"
     ]
    }
   ],
   "source": [
    "predictions = np.argmax(model.predict(x_test), axis=1)\n",
    "accuracy = (predictions == y_test.values).sum() / y_test.values.shape[0]\n",
    "print(\"Test set accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Betting time..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outcome when bet on home team\n",
    "Just a sanity check. Should not see high/any yields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelTester = ModelTester(model, x_train, y_train, x_test, y_test, bet_train, bet_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agencies = ['B365', 'BW', 'IW', 'LB', 'WH', 'SJ', 'VC', 'GB', 'BS']\n",
    "agencies = ['B365']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set size (380,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Test set size\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity checks by static betting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agency B365, \twin amount: -358.02\n",
      "Agency BW, \twin amount: -672.98\n",
      "Agency IW, \twin amount: -1066.66\n",
      "Agency LB, \twin amount: -809.56\n",
      "Agency WH, \twin amount: -522.30\n",
      "Agency SJ, \twin amount: 413.09\n",
      "Agency VC, \twin amount: -158.62\n",
      "Agency GB, \twin amount: 570.80\n",
      "Agency BS, \twin amount: 566.73\n"
     ]
    }
   ],
   "source": [
    "modelTester.predict_home_wins_only_profit(data=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agency B365, \twin amount: -24.49\n",
      "Agency BW, \twin amount: -109.16\n",
      "Agency IW, \twin amount: -207.18\n",
      "Agency LB, \twin amount: -137.50\n",
      "Agency WH, \twin amount: -70.90\n",
      "Agency SJ, \twin amount: 151.43\n",
      "Agency VC, \twin amount: 28.22\n",
      "Agency GB, \twin amount: 172.61\n",
      "Agency BS, \twin amount: 167.05\n"
     ]
    }
   ],
   "source": [
    "modelTester.predict_home_wins_only_profit(data=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agency B365, \twin amount: -1510.41\n",
      "Agency BW, \twin amount: -1904.15\n",
      "Agency IW, \twin amount: -2422.72\n",
      "Agency LB, \twin amount: -2037.35\n",
      "Agency WH, \twin amount: -2249.13\n",
      "Agency SJ, \twin amount: -1612.36\n",
      "Agency VC, \twin amount: -1259.90\n",
      "Agency GB, \twin amount: -1998.52\n",
      "Agency BS, \twin amount: -1916.66\n"
     ]
    }
   ],
   "source": [
    "modelTester.predict_draw_only_profit(data=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agency B365, \twin amount: -193.12\n",
      "Agency BW, \twin amount: -391.92\n",
      "Agency IW, \twin amount: -544.97\n",
      "Agency LB, \twin amount: -420.50\n",
      "Agency WH, \twin amount: -356.55\n",
      "Agency SJ, \twin amount: 283.93\n",
      "Agency VC, \twin amount: -106.94\n",
      "Agency GB, \twin amount: 304.80\n",
      "Agency BS, \twin amount: 324.83\n"
     ]
    }
   ],
   "source": [
    "modelTester.predict_away_wins_only_profit(data=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agency B365, \twin amount: -1255.44\n",
      "Agency BW, \twin amount: -2308.64\n",
      "Agency IW, \twin amount: -3345.15\n",
      "Agency LB, \twin amount: -2503.40\n",
      "Agency WH, \twin amount: -1956.18\n",
      "Agency SJ, \twin amount: 1448.62\n",
      "Agency VC, \twin amount: -678.29\n",
      "Agency GB, \twin amount: 1651.16\n",
      "Agency BS, \twin amount: 1845.30\n"
     ]
    }
   ],
   "source": [
    "modelTester.predict_bet_home_away_profit(data=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agency B365, \twin amount: -217.62\n",
      "Agency BW, \twin amount: -501.08\n",
      "Agency IW, \twin amount: -752.15\n",
      "Agency LB, \twin amount: -558.00\n",
      "Agency WH, \twin amount: -427.44\n",
      "Agency SJ, \twin amount: 435.37\n",
      "Agency VC, \twin amount: -78.72\n",
      "Agency GB, \twin amount: 477.41\n",
      "Agency BS, \twin amount: 491.88\n"
     ]
    }
   ],
   "source": [
    "modelTester.predict_bet_home_away_profit(data=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agency B365, \twin amount: -2765.85\n",
      "Agency BW, \twin amount: -4212.79\n",
      "Agency IW, \twin amount: -5767.86\n",
      "Agency LB, \twin amount: -4540.75\n",
      "Agency WH, \twin amount: -4205.31\n",
      "Agency SJ, \twin amount: -163.74\n",
      "Agency VC, \twin amount: -1938.20\n",
      "Agency GB, \twin amount: -347.36\n",
      "Agency BS, \twin amount: -71.36\n"
     ]
    }
   ],
   "source": [
    "modelTester.predict_bet_on_all_profit(data=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agency B365, \twin amount: -621.65\n",
      "Agency BW, \twin amount: -992.96\n",
      "Agency IW, \twin amount: -1379.64\n",
      "Agency LB, \twin amount: -1088.70\n",
      "Agency WH, \twin amount: -1013.40\n",
      "Agency SJ, \twin amount: 29.11\n",
      "Agency VC, \twin amount: -420.18\n",
      "Agency GB, \twin amount: -40.25\n",
      "Agency BS, \twin amount: -6.11\n"
     ]
    }
   ],
   "source": [
    "modelTester.predict_bet_on_all_profit(data=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bet on every match\n",
    "Always bet on the predicted winner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agency B365, \twin amount: 613.18\n",
      "Agency BW, \twin amount: 520.03\n",
      "Agency IW, \twin amount: 452.74\n",
      "Agency LB, \twin amount: 496.73\n",
      "Agency WH, \twin amount: 560.19\n",
      "Agency SJ, \twin amount: 1344.42\n",
      "Agency VC, \twin amount: 674.26\n",
      "Agency GB, \twin amount: 1570.76\n",
      "Agency BS, \twin amount: 1586.42\n"
     ]
    }
   ],
   "source": [
    "modelTester.always_bet_predicted_winner_profit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bet when expected return high enough\n",
    "First calculate the expected return of the team expected to win. If yield is high enough, then bet. \n",
    "* yield = prediction probability * odds. \n",
    "* Bet if yield > threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agency B365, \twin amount: 2271.63. Didn't bet on 70.95% of matches\n",
      "Agency BW, \twin amount: 2160.75. Didn't bet on 71.77% of matches\n",
      "Agency IW, \twin amount: 2061.74. Didn't bet on 71.78% of matches\n",
      "Agency LB, \twin amount: 2120.68. Didn't bet on 71.92% of matches\n",
      "Agency WH, \twin amount: 2210.53. Didn't bet on 71.02% of matches\n",
      "Agency SJ, \twin amount: 3146.63. Didn't bet on 65.43% of matches\n",
      "Agency VC, \twin amount: 2356.20. Didn't bet on 70.44% of matches\n",
      "Agency GB, \twin amount: 3408.79. Didn't bet on 63.04% of matches\n",
      "Agency BS, \twin amount: 3426.57. Didn't bet on 62.92% of matches\n"
     ]
    }
   ],
   "source": [
    "modelTester.bet_predicted_winner_with_threshold_profit(\"test\", threshold=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bet on highest return\n",
    "Multiplies the neural network match predictions with betting odds and from these multiplications chooses from home win, draw, away win the highest expected return value. A threshhold can be set to choose if the yield is high enough to bet.  [prediction probability * odds > threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agency B365, \twin amount: -66.00. Didn't bet on 97.92% of matches\n",
      "Agency BW, \twin amount: -40.00. Didn't bet on 98.67% of matches\n",
      "Agency IW, \twin amount: -18.00. Didn't bet on 99.20% of matches\n",
      "Agency LB, \twin amount: -41.00. Didn't bet on 98.62% of matches\n",
      "Agency WH, \twin amount: -53.00. Didn't bet on 98.30% of matches\n",
      "Agency SJ, \twin amount: -116.00. Didn't bet on 95.30% of matches\n",
      "Agency VC, \twin amount: -83.00. Didn't bet on 97.51% of matches\n",
      "Agency GB, \twin amount: -92.00. Didn't bet on 95.56% of matches\n",
      "Agency BS, \twin amount: -107.00. Didn't bet on 95.11% of matches\n"
     ]
    }
   ],
   "source": [
    "modelTester.predict_on_highest_return(threshold=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
